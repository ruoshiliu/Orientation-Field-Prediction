Script started on Tue 16 Apr 2019 11:49:20 PM EDT
krliu@dm:~/github/ansim\[rliu@dm ansim]$ exit[2PlsCUDA_VISIBLE_DEVICES=2,1,3 python3.6 train.py 1,2[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[K[K[Kls
[0m[01;35m1.jpg[0m              ConvLSTM.pyc         LICENSE      test.ipynb
ansim_dataset.py   convolution_lstm.py  [01;34mlog[0m          train.csv
ansim_dataset.pyc  dataset.ipynb        [01;34m__pycache__[0m  train.ipynb
ConvLSTM.ipynb     img_list.csv         README.md    train.py
ConvLSTM.py        indiecoder.ipynb     test.csv     Untitled1.ipynb
krliu@dm:~/github/ansim\[rliu@dm ansim]$ cd [K[K[Kcd ..
krliu@dm:~/github\[rliu@dm github]$ ls
[0m[01;34mansim[0m  [01;34mdefect_classifier[0m
krliu@dm:~/github\[rliu@dm github]$ cd ..
krliu@dm:~\[rliu@dm ~]$ ls
000510.weights  [0m[01;34mdefect_classifier[0m  [01;34mpytorch_tutorial[0m
activate.ini    defects_6300.m     [01;34mtensorflow1[0m
[01;34manaconda2[0m       [01;35mexample.png[0m        [01;31mtensorflow1.zip[0m
[01;34mansim[0m           [01;34mgithub[0m             [01;34mtorch[0m
[01;31mansim.zip[0m       [01;34mkaggle[0m             [01;31mv2_pytorch_yolo2_iclr.zip[0m
[01;34mbin[0m             [01;34m__MACOSX[0m           [01;34myolo2[0m
coord_list.npy  [01;34mmatlab[0m             [01;34myolo2-numbers[0m
[01;34mCOSI126a_HW3[0m    [01;34mmatlab_install[0m     [01;34myolov3[0m
[01;34mdata[0m            patents.txt
krliu@dm:~\[rliu@dm ~]$ cd ansim
krliu@dm:~/ansim\[rliu@dm ansim]$ ls
[0m[01;34mdata[0m  [01;34mmodels[0m  [01;34mresults[0m
krliu@dm:~/ansim\[rliu@dm ansim]$ m[Kcd md[Kodels
krliu@dm:~/ansim/models\[rliu@dm models]$ ls
0001.weights  0005.weights  [0m[01;34m4-16[0m  [01;34m4-16-2[0m
krliu@dm:~/ansim/models\[rliu@dm models]$ mkdir 4[K4-16-kernal55533333
krliu@dm:~/ansim/models\[rliu@dm models]$ cd ../..
krliu@dm:~\[rliu@dm ~]$ ls
000510.weights  [0m[01;34mdefect_classifier[0m  [01;34mpytorch_tutorial[0m
activate.ini    defects_6300.m     [01;34mtensorflow1[0m
[01;34manaconda2[0m       [01;35mexample.png[0m        [01;31mtensorflow1.zip[0m
[01;34mansim[0m           [01;34mgithub[0m             [01;34mtorch[0m
[01;31mansim.zip[0m       [01;34mkaggle[0m             [01;31mv2_pytorch_yolo2_iclr.zip[0m
[01;34mbin[0m             [01;34m__MACOSX[0m           [01;34myolo2[0m
coord_list.npy  [01;34mmatlab[0m             [01;34myolo2-numbers[0m
[01;34mCOSI126a_HW3[0m    [01;34mmatlab_install[0m     [01;34myolov3[0m
[01;34mdata[0m            patents.txt
krliu@dm:~\[rliu@dm ~]$ cd github/ansim/
krliu@dm:~/github/ansim\[rliu@dm ansim]$ ls
[0m[01;35m1.jpg[0m              ConvLSTM.pyc         LICENSE      test.ipynb
ansim_dataset.py   convolution_lstm.py  [01;34mlog[0m          train.csv
ansim_dataset.pyc  dataset.ipynb        [01;34m__pycache__[0m  train.ipynb
ConvLSTM.ipynb     img_list.csv         README.md    train.py
ConvLSTM.py        indiecoder.ipynb     test.csv     Untitled1.ipynb
krliu@dm:~/github/ansim\[rliu@dm ansim]$ p[Klscd github/ansim/ls[Kcd ../..mkdir 4-16-kernal55533333[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kmkdir 4-16-kernal55533333[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd modelsls[Kcd ansimls[Kcd ..[3Plscd ..[3Plsexit[2PlsCUDA_VISIBLE_DEVICES=2,1,3 python3.6 train.py 1,2[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C2,1[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
GPU in use
Epoch 0/199
----------
trainloader ready!
testloader ready!
train Loss: 13.4528 batch_loss: 10331.738281
train Loss: 30.7653 batch_loss: 13296.044922
train Loss: 43.9851 batch_loss: 10152.805664
train Loss: 53.3470 batch_loss: 7189.906250
train Loss: 66.4545 batch_loss: 10066.577148
train Loss: 77.2167 batch_loss: 8265.333008
train Loss: 88.1912 batch_loss: 8428.399414
train Loss: 96.9641 batch_loss: 6737.629395
train Loss: 108.2063 batch_loss: 8634.000000
train Loss: 121.6388 batch_loss: 10316.166016
train Loss: 132.3491 batch_loss: 8225.482422
train Loss: 146.2645 batch_loss: 10687.049805
train Loss: 156.5348 batch_loss: 7887.631348
train Loss: 169.6415 batch_loss: 10065.873047
train Loss: 180.4320 batch_loss: 8287.162109
train Loss: 193.4021 batch_loss: 9961.013672
train Loss: 201.7637 batch_loss: 6421.674316
train Loss: 208.3366 batch_loss: 5048.043945
train Loss: 220.6933 batch_loss: 9489.906250
train Loss: 233.8702 batch_loss: 10119.868164
train Loss: 243.2163 batch_loss: 7177.793457
train Loss: 253.1910 batch_loss: 7660.614746
train Loss: 263.5812 batch_loss: 7979.654785
train Loss: 270.5855 batch_loss: 5379.287598
train Loss: 282.2290 batch_loss: 8942.202148
train Loss: 290.9161 batch_loss: 6671.745117
train Loss: 300.4596 batch_loss: 7329.376465
train Loss: 309.9520 batch_loss: 7290.168945
train Loss: 317.5720 batch_loss: 5852.158691
train Loss: 328.0218 batch_loss: 8025.424805
train Loss: 337.1834 batch_loss: 7036.126465
train Loss: 349.5123 batch_loss: 9468.574219
train Loss: 359.3339 batch_loss: 7542.984375
train Loss: 365.9033 batch_loss: 5045.335938
train Loss: 371.7477 batch_loss: 4488.470215
train Loss: 378.4166 batch_loss: 5121.742676
train Loss: 384.0698 batch_loss: 4341.637207
train Loss: 392.6276 batch_loss: 6572.370605
train Loss: 400.0669 batch_loss: 5713.389648
train Loss: 405.4819 batch_loss: 4158.706543
train Loss: 414.0096 batch_loss: 6549.319336
train Loss: 427.0749 batch_loss: 10034.155273
train Loss: 433.1598 batch_loss: 4673.195801
train Loss: 443.2248 batch_loss: 7729.940430
train Loss: 449.7940 batch_loss: 5045.132324
train Loss: 459.1592 batch_loss: 7192.460449
train Loss: 466.7798 batch_loss: 5852.649414
train Loss: 472.8467 batch_loss: 4659.311035
train Loss: 482.8566 batch_loss: 7687.614258
train Loss: 487.9870 batch_loss: 3940.193115
train Loss: 494.2093 batch_loss: 4778.676758
train Loss: 502.3319 batch_loss: 6238.212891
train Loss: 513.6519 batch_loss: 8693.749023
train Loss: 523.7683 batch_loss: 7769.398926
train Loss: 532.4158 batch_loss: 6641.238770
train Loss: 542.1764 batch_loss: 7496.169922
train Loss: 548.3819 batch_loss: 4765.817383
train Loss: 558.5909 batch_loss: 7840.526367
train Loss: 562.2020 batch_loss: 2773.307373
train Loss: 570.8605 batch_loss: 6649.756836
train Loss: 579.8570 batch_loss: 6909.317383
train Loss: 584.5636 batch_loss: 3614.619385
train Loss: 591.6156 batch_loss: 5415.916504
train Loss: 600.0434 batch_loss: 6472.614258
train Loss: 606.7885 batch_loss: 5180.211914
train Loss: 615.8535 batch_loss: 6961.899414
train Loss: 621.9315 batch_loss: 4667.917480
train Loss: 630.7671 batch_loss: 6785.724121
train Loss: 635.0874 batch_loss: 3318.020508
train Loss: 639.3876 batch_loss: 3302.517822
train Loss: 644.1293 batch_loss: 3641.656738
train Loss: 650.2384 batch_loss: 4691.816895
train Loss: 653.9793 batch_loss: 2873.010498
train Loss: 658.5106 batch_loss: 3480.001221
train Loss: 664.6824 batch_loss: 4739.971680
train Loss: 670.7836 batch_loss: 4685.720703
train Loss: 678.9242 batch_loss: 6251.979980
train Loss: 683.2039 batch_loss: 3286.750000
train Loss: 688.4988 batch_loss: 4066.488525
train Loss: 693.9237 batch_loss: 4166.362793
train Loss: 701.7361 batch_loss: 5999.937500
train Loss: 705.3100 batch_loss: 2744.733398
train Loss: 710.5891 batch_loss: 4054.315674
train Loss: 714.4821 batch_loss: 2989.870850
train Loss: 719.7622 batch_loss: 4055.069824
train Loss: 724.5597 batch_loss: 3684.527100
train Loss: 728.2730 batch_loss: 2851.777344
train Loss: 734.2789 batch_loss: 4612.587402
train Loss: 738.7688 batch_loss: 3448.242188
train Loss: 741.7406 batch_loss: 2282.295166
train Loss: 744.9196 batch_loss: 2441.519775
train Loss: 750.3618 batch_loss: 4179.587891
train Loss: 755.8817 batch_loss: 4239.292480
train Loss: 760.8990 batch_loss: 3853.244873
train Loss: 766.0688 batch_loss: 3970.418213
train Loss: 771.9668 batch_loss: 4529.683105
train Loss: 774.8856 batch_loss: 2241.654785
train Loss: 780.5465 batch_loss: 4347.504883
train Loss: 786.4011 batch_loss: 4496.365234
train Loss: 789.0865 batch_loss: 2062.373291
train Loss: 793.8662 batch_loss: 3670.835938
train Loss: 797.8994 batch_loss: 3097.456055
train Loss: 801.3901 batch_loss: 2680.902344
train Loss: 803.8442 batch_loss: 1884.702881
train Loss: 807.1600 batch_loss: 2546.531494
train Loss: 811.8598 batch_loss: 3609.481934
train Loss: 816.9320 batch_loss: 3895.447754
train Loss: 820.3839 batch_loss: 2651.092285
train Loss: 826.8288 batch_loss: 4949.635254
train Loss: 829.4942 batch_loss: 2047.032593
train Loss: 833.9421 batch_loss: 3416.001709
train Loss: 837.8894 batch_loss: 3031.509521
train Loss: 841.7427 batch_loss: 2959.373291
train Loss: 847.3911 batch_loss: 4337.978027
train Loss: 851.8086 batch_loss: 3392.591064
train Loss: 857.1084 batch_loss: 4070.289307
train Loss: 859.9557 batch_loss: 2186.738037
train Loss: 863.8600 batch_loss: 2998.449951
train Loss: 868.7259 batch_loss: 3736.998535
train Loss: 873.9785 batch_loss: 4034.036621
train Loss: 876.0415 batch_loss: 1584.388794
train Loss: 879.8986 batch_loss: 2962.225586
train Loss: 884.9613 batch_loss: 3888.187012
train Loss: 889.1635 batch_loss: 3227.237305
train Loss: 893.3220 batch_loss: 3193.785400
train Loss: 897.1682 batch_loss: 2953.842773
train Loss: 901.4541 batch_loss: 3291.579346
train Loss: 903.8048 batch_loss: 1805.348267
train Loss: 909.1211 batch_loss: 4082.946777
train Loss: 912.9594 batch_loss: 2947.816650
train Loss: 917.1401 batch_loss: 3210.762207
train Loss: 919.3592 batch_loss: 1704.232788
train Loss: 923.8752 batch_loss: 3468.297363
train Loss: 928.6590 batch_loss: 3673.938477
train Loss: 930.0836 batch_loss: 1094.112915
train Loss: 934.5606 batch_loss: 3438.351318
train Loss: 939.3938 batch_loss: 3711.854980
train Loss: 941.6565 batch_loss: 1737.806519
train Loss: 945.0727 batch_loss: 2623.641846
train Loss: 947.9776 batch_loss: 2230.915527
train Loss: 952.0666 batch_loss: 3140.415039
train Loss: 955.1768 batch_loss: 2388.617676
train Loss: 957.7250 batch_loss: 1957.021729
train Loss: 961.1657 batch_loss: 2642.466064
train Loss: 964.1458 batch_loss: 2288.692627
train Loss: 965.8687 batch_loss: 1323.172729
train Loss: 969.1296 batch_loss: 2504.384277
train Loss: 971.9859 batch_loss: 2193.650146
train Loss: 974.0205 batch_loss: 1562.564697
train Loss: 977.0253 batch_loss: 2307.683105
train Loss: 978.5662 batch_loss: 1183.378906
train Loss: 982.4681 batch_loss: 2996.704590
train Loss: 985.5103 batch_loss: 2336.384277
train Loss: 989.2261 batch_loss: 2853.761719
train Loss: 991.5582 batch_loss: 1791.044434
train Loss: 994.6403 batch_loss: 2367.033936
train Loss: 997.4558 batch_loss: 2162.311035
train Loss: 999.9194 batch_loss: 1892.039673
train Loss: 1002.9297 batch_loss: 2311.944824
train Loss: 1004.4737 batch_loss: 1185.755859
train Loss: 1006.6854 batch_loss: 1698.584717
train Loss: 1008.4558 batch_loss: 1359.636719
train Loss: 1011.2672 batch_loss: 2159.211670
train Loss: 1014.6120 batch_loss: 2568.801025
train Loss: 1016.5666 batch_loss: 1501.117188
train Loss: 1017.7384 batch_loss: 899.920471
train Loss: 1020.1162 batch_loss: 1826.189575
train Loss: 1023.9384 batch_loss: 2935.448486
train Loss: 1026.2048 batch_loss: 1740.564087
train Loss: 1029.6752 batch_loss: 2665.250000
train Loss: 1033.3568 batch_loss: 2827.503174
train Loss: 1035.9645 batch_loss: 2002.701416
train Loss: 1037.8601 batch_loss: 1455.823730
train Loss: 1040.9455 batch_loss: 2369.629639
train Loss: 1041.7497 batch_loss: 617.622131
train Loss: 1043.8964 batch_loss: 1648.647705
train Loss: 1047.1401 batch_loss: 2491.139160
train Loss: 1049.3723 batch_loss: 1714.305054
train Loss: 1050.6610 batch_loss: 989.746826
train Loss: 1051.7661 batch_loss: 848.687805
train Loss: 1052.5342 batch_loss: 589.900085
train Loss: 1054.0170 batch_loss: 1138.817627
train Loss: 1056.7230 batch_loss: 2078.227539
train Loss: 1058.8162 batch_loss: 1607.561157
train Loss: 1062.0847 batch_loss: 2510.225098
train Loss: 1063.5071 batch_loss: 1092.406738
train Loss: 1066.8283 batch_loss: 2550.693115
train Loss: 1069.1211 batch_loss: 1760.863281
train Loss: 1071.3197 batch_loss: 1688.537964
train Loss: 1074.1530 batch_loss: 2175.927246
train Loss: 1075.2308 batch_loss: 827.740112
train Loss: 1078.1009 batch_loss: 2204.239014
train Loss: 1080.4910 batch_loss: 1835.606567
train Loss: 1082.1527 batch_loss: 1276.217529
train Loss: 1083.3248 batch_loss: 900.177734
train Loss: 1084.3319 batch_loss: 773.391479
train Loss: 1086.8792 batch_loss: 1956.363281
train Loss: 1088.4046 batch_loss: 1171.517456
train Loss: 1090.3050 batch_loss: 1459.521240
train Loss: 1092.8435 batch_loss: 1949.564453
train Loss: 1094.1399 batch_loss: 995.638855
train Loss: 1094.9162 batch_loss: 596.151184
train Loss: 1098.5443 batch_loss: 2786.370605
train Loss: 1100.1078 batch_loss: 1200.835938
train Loss: 1101.6075 batch_loss: 1151.753296
train Loss: 1104.3471 batch_loss: 2103.971924
train Loss: 1107.0311 batch_loss: 2061.373535
train Loss: 1107.8991 batch_loss: 666.561340
train Loss: 1108.7713 batch_loss: 669.848572
train Loss: 1109.4241 batch_loss: 501.369141
train Loss: 1110.6563 batch_loss: 946.370850
train Loss: 1112.5973 batch_loss: 1490.691650
train Loss: 1114.8206 batch_loss: 1707.433105
train Loss: 1116.6851 batch_loss: 1431.948975
train Loss: 1118.1598 batch_loss: 1132.578491
train Loss: 1119.7896 batch_loss: 1251.714478
train Loss: 1121.2413 batch_loss: 1114.871582
train Loss: 1122.7743 batch_loss: 1177.380127
train Loss: 1124.0458 batch_loss: 976.466125
train Loss: 1125.7428 batch_loss: 1303.276489
train Loss: 1126.8118 batch_loss: 821.066467
train Loss: 1127.7663 batch_loss: 733.025818
train Loss: 1129.5271 batch_loss: 1352.249634
train Loss: 1131.4581 batch_loss: 1483.022217
train Loss: 1133.3021 batch_loss: 1416.206909
train Loss: 1133.8453 batch_loss: 417.170532
train Loss: 1134.8543 batch_loss: 774.898376
train Loss: 1136.2212 batch_loss: 1049.783813
train Loss: 1138.0985 batch_loss: 1441.803589
train Loss: 1140.2550 batch_loss: 1656.191772
train Loss: 1141.9500 batch_loss: 1301.757324
train Loss: 1143.9951 batch_loss: 1570.611206
train Loss: 1145.2757 batch_loss: 983.482666
train Loss: 1146.9423 batch_loss: 1279.970215
train Loss: 1148.1515 batch_loss: 928.640808
train Loss: 1148.7184 batch_loss: 435.381927
train Loss: 1150.8652 batch_loss: 1648.754150
train Loss: 1153.0268 batch_loss: 1660.166748
train Loss: 1154.5289 batch_loss: 1153.553101
train Loss: 1156.1166 batch_loss: 1219.365234
train Loss: 1157.8379 batch_loss: 1321.981689
train Loss: 1158.3568 batch_loss: 398.535339
train Loss: 1159.1873 batch_loss: 637.755066
train Loss: 1161.9010 batch_loss: 2084.159424
train Loss: 1162.7000 batch_loss: 613.623840
train Loss: 1163.4359 batch_loss: 565.198853
train Loss: 1165.2903 batch_loss: 1424.187988
train Loss: 1166.9276 batch_loss: 1257.429932
train Loss: 1169.6773 batch_loss: 2111.780518
train Loss: 1171.9545 batch_loss: 1748.838013
train Loss: 1173.3467 batch_loss: 1069.256348
train Loss: 1173.9736 batch_loss: 481.407928
train Loss: 1175.7823 batch_loss: 1389.152954
train Loss: 1176.9514 batch_loss: 897.845459
train Loss: 1178.2712 batch_loss: 1013.559753
train Loss: 1180.1335 batch_loss: 1430.275879
Loss on the test images: 315.70338 
Epoch 1/199
----------
trainloader ready!
testloader ready!
train Loss: 2.7335 batch_loss: 2099.366211
train Loss: 3.5584 batch_loss: 633.491882
train Loss: 4.5224 batch_loss: 740.309631
train Loss: 5.6214 batch_loss: 844.055298
train Loss: 6.0927 batch_loss: 362.001099
train Loss: 7.9065 batch_loss: 1392.976807
train Loss: 10.0996 batch_loss: 1684.299683
train Loss: 11.1136 batch_loss: 778.721863
train Loss: 12.2251 batch_loss: 853.629150
train Loss: 14.0336 batch_loss: 1388.989868
train Loss: 15.0145 batch_loss: 753.260498
train Loss: 15.6982 batch_loss: 525.144470
train Loss: 16.4481 batch_loss: 575.918030
train Loss: 16.9760 batch_loss: 405.380585
train Loss: 17.9562 batch_loss: 752.832031
train Loss: 19.3542 batch_loss: 1073.650513
train Loss: 20.3648 batch_loss: 776.107239
train Loss: 22.6380 batch_loss: 1745.817261
train Loss: 23.6818 batch_loss: 801.643494
train Loss: 24.6469 batch_loss: 741.232117
train Loss: 25.8176 batch_loss: 899.052979
train Loss: 26.5794 batch_loss: 585.091064
train Loss: 27.3278 batch_loss: 574.760010
train Loss: 28.9753 batch_loss: 1265.298828
train Loss: 29.6408 batch_loss: 511.088867
train Loss: 30.8753 batch_loss: 948.087891
train Loss: 31.8533 batch_loss: 751.114319
train Loss: 33.4590 batch_loss: 1233.181519
train Loss: 34.4553 batch_loss: 765.192749
train Loss: 35.6402 batch_loss: 909.946716
train Loss: 36.9877 batch_loss: 1034.924561
train Loss: 37.7022 batch_loss: 548.688599
train Loss: 39.7073 batch_loss: 1539.912476
train Loss: 40.6668 batch_loss: 736.960327
train Loss: 41.6755 batch_loss: 774.666077
train Loss: 42.1209 batch_loss: 342.073883
train Loss: 42.6807 batch_loss: 429.906189
train Loss: 44.2036 batch_loss: 1169.616333
train Loss: 44.7099 batch_loss: 388.780396
train Loss: 45.1814 batch_loss: 362.161835
train Loss: 46.0749 batch_loss: 686.190735
train Loss: 47.7776 batch_loss: 1307.660522
train Loss: 48.6135 batch_loss: 641.984192
train Loss: 49.1205 batch_loss: 389.342468
train Loss: 50.1651 batch_loss: 802.309570
train Loss: 51.1054 batch_loss: 722.151550
train Loss: 51.5929 batch_loss: 374.348907
train Loss: 53.1888 batch_loss: 1225.659058
train Loss: 54.6197 batch_loss: 1098.984741
train Loss: 55.1787 batch_loss: 429.310059
train Loss: 56.1986 batch_loss: 783.239929
train Loss: 57.1672 batch_loss: 743.872803
train Loss: 58.1517 batch_loss: 756.140869
train Loss: 59.5643 batch_loss: 1084.818115
train Loss: 60.3846 batch_loss: 630.041626
train Loss: 61.2341 batch_loss: 652.401550
train Loss: 62.2708 batch_loss: 796.201843
train Loss: 63.0629 batch_loss: 608.287231
train Loss: 64.1934 batch_loss: 868.282410
train Loss: 65.4372 batch_loss: 955.239441
train Loss: 66.6198 batch_loss: 908.199463
train Loss: 67.5458 batch_loss: 711.159607
train Loss: 68.4028 batch_loss: 658.202698
train Loss: 70.6533 batch_loss: 1728.345703
train Loss: 71.3886 batch_loss: 564.739807
train Loss: 72.2815 batch_loss: 685.719971
train Loss: 72.8327 batch_loss: 423.331879
train Loss: 73.8578 batch_loss: 787.305969
train Loss: 74.8135 batch_loss: 733.943237
train Loss: 75.4841 batch_loss: 515.074097
train Loss: 77.0733 batch_loss: 1220.486450
train Loss: 77.4976 batch_loss: 325.815369
train Loss: 78.5708 batch_loss: 824.253479
train Loss: 79.4091 batch_loss: 643.819641
train Loss: 80.3573 batch_loss: 728.245422
train Loss: 81.3011 batch_loss: 724.830627
train Loss: 82.2252 batch_loss: 709.695557
train Loss: 82.9170 batch_loss: 531.283081
train Loss: 84.3009 batch_loss: 1062.866699
train Loss: 85.6414 batch_loss: 1029.451050
train Loss: 86.1794 batch_loss: 413.195251
train Loss: 86.8778 batch_loss: 536.388794
train Loss: 88.0367 batch_loss: 890.018921
train Loss: 88.8048 batch_loss: 589.945129
train Loss: 89.4726 batch_loss: 512.825439
train Loss: 89.9540 batch_loss: 369.754364
train Loss: 90.1455 batch_loss: 147.035828
train Loss: 90.7649 batch_loss: 475.735413
train Loss: 91.4415 batch_loss: 519.626038
train Loss: 92.3470 batch_loss: 695.370178
train Loss: 93.9973 batch_loss: 1267.457275
train Loss: 95.1198 batch_loss: 862.060547
train Loss: 95.9777 batch_loss: 658.900085
train Loss: 96.6301 batch_loss: 501.045349
train Loss: 97.8915 batch_loss: 968.727112
train Loss: 98.9558 batch_loss: 817.418091
train Loss: 100.2616 batch_loss: 1002.803101
train Loss: 101.1648 batch_loss: 693.679993
train Loss: 102.7495 batch_loss: 1217.039917
train Loss: 103.7429 batch_loss: 762.969910
train Loss: 104.1740 batch_loss: 331.046692
train Loss: 105.4235 batch_loss: 959.648987
train Loss: 106.2073 batch_loss: 601.915894
train Loss: 106.8303 batch_loss: 478.490875
train Loss: 107.3027 batch_loss: 362.810822
train Loss: 107.9472 batch_loss: 494.977264
train Loss: 109.5446 batch_loss: 1226.804443
train Loss: 110.2862 batch_loss: 569.506958
train Loss: 111.0180 batch_loss: 562.090271
train Loss: 111.7086 batch_loss: 530.331970
train Loss: 112.1239 batch_loss: 318.977875
train Loss: 112.8186 batch_loss: 533.551758
train Loss: 113.7305 batch_loss: 700.290100
train Loss: 114.6783 batch_loss: 727.912354
train Loss: 115.3599 batch_loss: 523.453064
train Loss: 116.5598 batch_loss: 921.530029
train Loss: 117.3885 batch_loss: 636.473328
train Loss: 117.8297 batch_loss: 338.813354
train Loss: 119.1184 batch_loss: 989.741272
train Loss: 120.3006 batch_loss: 907.901367
train Loss: 121.5185 batch_loss: 935.407349
train Loss: 122.7527 batch_loss: 947.809814
train Loss: 123.3082 batch_loss: 426.653778
train Loss: 124.0151 batch_loss: 542.917725
train Loss: 125.5715 batch_loss: 1195.257935
train Loss: 126.5031 batch_loss: 715.497375
train Loss: 126.9055 batch_loss: 309.022064
train Loss: 128.5760 batch_loss: 1282.951660
train Loss: 129.2598 batch_loss: 525.204712
train Loss: 130.2805 batch_loss: 783.876404
train Loss: 131.1082 batch_loss: 635.653137
train Loss: 131.7301 batch_loss: 477.665466
train Loss: 132.6204 batch_loss: 683.691528
train Loss: 133.9578 batch_loss: 1027.117554
train Loss: 135.1555 batch_loss: 919.823425
train Loss: 136.1444 batch_loss: 759.490967
train Loss: 136.8580 batch_loss: 548.087341
train Loss: 137.1238 batch_loss: 204.083328
train Loss: 137.9071 batch_loss: 601.630005
train Loss: 138.9949 batch_loss: 835.374634
train Loss: 139.9076 batch_loss: 700.995728
train Loss: 140.9666 batch_loss: 813.305603
train Loss: 141.6229 batch_loss: 504.011383
train Loss: 142.4130 batch_loss: 606.806274
train Loss: 143.2062 batch_loss: 609.165344
train Loss: 143.3984 batch_loss: 147.670227
train Loss: 144.1961 batch_loss: 612.596741
train Loss: 145.3539 batch_loss: 889.189575
train Loss: 146.0028 batch_loss: 498.350403
train Loss: 146.7305 batch_loss: 558.907471
train Loss: 147.3395 batch_loss: 467.683685
train Loss: 147.8833 batch_loss: 417.621307
train Loss: 148.6033 batch_loss: 552.979858
train Loss: 149.2996 batch_loss: 534.783447
train Loss: 150.1467 batch_loss: 650.553406
train Loss: 151.2722 batch_loss: 864.355408
train Loss: 151.7236 batch_loss: 346.661072
train Loss: 152.4836 batch_loss: 583.703003
train Loss: 153.2376 batch_loss: 579.092590
train Loss: 154.2079 batch_loss: 745.151367
train Loss: 154.6313 batch_loss: 325.190247
train Loss: 155.0644 batch_loss: 332.630768
train Loss: 155.5300 batch_loss: 357.570709
train Loss: 155.9667 batch_loss: 335.404297
train Loss: 156.6329 batch_loss: 511.636444
train Loss: 156.9301 batch_loss: 228.280792
train Loss: 157.9552 batch_loss: 787.232056
train Loss: 158.5549 batch_loss: 460.545837
train Loss: 159.0585 batch_loss: 386.822815
train Loss: 159.3674 batch_loss: 237.214691
train Loss: 160.2445 batch_loss: 673.588562
train Loss: 160.9119 batch_loss: 512.614380
train Loss: 161.4603 batch_loss: 421.171295
train Loss: 161.7738 batch_loss: 240.772995
train Loss: 162.9732 batch_loss: 921.116394
train Loss: 163.6583 batch_loss: 526.173645
train Loss: 164.5529 batch_loss: 686.995850
train Loss: 165.5125 batch_loss: 737.031067
train Loss: 166.5607 batch_loss: 804.957153
train Loss: 166.8370 batch_loss: 212.259933
train Loss: 167.4860 batch_loss: 498.390717
train Loss: 167.8149 batch_loss: 252.589417
train Loss: 168.3765 batch_loss: 431.334839
train Loss: 169.4870 batch_loss: 852.869446
train Loss: 169.8114 batch_loss: 249.146835
train Loss: 170.3468 batch_loss: 411.187561
train Loss: 171.1404 batch_loss: 609.458618
train Loss: 171.4578 batch_loss: 243.803650
train Loss: 171.7340 batch_loss: 212.111115
train Loss: 172.6188 batch_loss: 679.517334
train Loss: 173.5128 batch_loss: 686.578857
train Loss: 174.3500 batch_loss: 642.981323
train Loss: 174.9913 batch_loss: 492.474213
train Loss: 175.4949 batch_loss: 386.813751
train Loss: 176.3837 batch_loss: 682.555359
train Loss: 177.0921 batch_loss: 544.092957
train Loss: 177.7915 batch_loss: 537.090027
train Loss: 178.2431 batch_loss: 346.851898
train Loss: 179.1433 batch_loss: 691.396484
train Loss: 179.8082 batch_loss: 510.572571
train Loss: 180.2743 batch_loss: 357.992462
train Loss: 180.6807 batch_loss: 312.094727
train Loss: 181.3586 batch_loss: 520.668884
train Loss: 181.9004 batch_loss: 416.057068
train Loss: 182.3424 batch_loss: 339.466370
train Loss: 182.9440 batch_loss: 462.015320
train Loss: 183.7380 batch_loss: 609.793091
train Loss: 184.6676 batch_loss: 713.980530
train Loss: 185.0075 batch_loss: 260.990021
train Loss: 185.4116 batch_loss: 310.412994
train Loss: 185.7815 batch_loss: 284.094849
train Loss: 186.3326 batch_loss: 423.242554
train Loss: 186.8005 batch_loss: 359.275970
train Loss: 187.0527 batch_loss: 193.723770
train Loss: 187.7731 batch_loss: 553.291931
train Loss: 188.7523 batch_loss: 751.997742
train Loss: 189.2661 batch_loss: 394.636200
train Loss: 190.3038 batch_loss: 796.937805
train Loss: 190.5083 batch_loss: 157.060501
train Loss: 190.9259 batch_loss: 320.726685
train Loss: 191.7861 batch_loss: 660.639526
train Loss: 192.3688 batch_loss: 447.494141
train Loss: 193.1834 batch_loss: 625.619385
train Loss: 193.4772 batch_loss: 225.587845
train Loss: 193.8737 batch_loss: 304.569214
train Loss: 195.0316 batch_loss: 889.202820
train Loss: 195.9585 batch_loss: 711.911133
train Loss: 196.6250 batch_loss: 511.862732
train Loss: 197.4602 batch_loss: 641.458435
train Loss: 198.2038 batch_loss: 571.052002
train Loss: 198.6104 batch_loss: 312.272156
train Loss: 199.0026 batch_loss: 301.196808
train Loss: 199.7436 batch_loss: 569.107056
train Loss: 200.3893 batch_loss: 495.886993
train Loss: 201.0655 batch_loss: 519.328735
train Loss: 201.8145 batch_loss: 575.232422
train Loss: 202.3827 batch_loss: 436.399017
train Loss: 202.8866 batch_loss: 386.976990
train Loss: 203.4298 batch_loss: 417.188660
train Loss: 204.0813 batch_loss: 500.365173
train Loss: 204.8648 batch_loss: 601.663147
train Loss: 205.5485 batch_loss: 525.085876
train Loss: 206.1446 batch_loss: 457.847382
train Loss: 206.6288 batch_loss: 371.881439
train Loss: 206.9466 batch_loss: 244.035736
train Loss: 207.6638 batch_loss: 550.843628
train Loss: 208.4216 batch_loss: 581.919800
train Loss: 208.9785 batch_loss: 427.760284
train Loss: 209.6155 batch_loss: 489.207672
train Loss: 210.2727 batch_loss: 504.685822
train Loss: 210.9950 batch_loss: 554.745422
train Loss: 211.5423 batch_loss: 420.352020
train Loss: 212.0832 batch_loss: 415.365112
train Loss: 212.4781 batch_loss: 303.279114
train Loss: 212.7570 batch_loss: 214.229034
train Loss: 213.5884 batch_loss: 638.510071
Loss on the test images: 166.04172 
Epoch 2/199
----------
trainloader ready!
testloader ready!
train Loss: 0.7861 batch_loss: 603.720154
train Loss: 1.5708 batch_loss: 602.664856
train Loss: 2.1549 batch_loss: 448.556244
train Loss: 2.7085 batch_loss: 425.166138
train Loss: 3.2491 batch_loss: 415.177246
train Loss: 4.0454 batch_loss: 611.580994
train Loss: 4.5399 batch_loss: 379.753479
train Loss: 5.2734 batch_loss: 563.375732
train Loss: 6.1721 batch_loss: 690.157471
train Loss: 6.6010 batch_loss: 329.401367
train Loss: 7.1096 batch_loss: 390.625519
train Loss: 7.7543 batch_loss: 495.140625
train Loss: 8.0529 batch_loss: 229.334488
train Loss: 8.5420 batch_loss: 375.621948
train Loss: 9.0651 batch_loss: 401.749847
train Loss: 9.7581 batch_loss: 532.215698
train Loss: 10.2301 batch_loss: 362.457367
train Loss: 11.0053 batch_loss: 595.400208
train Loss: 11.7418 batch_loss: 565.630005
train Loss: 12.2910 batch_loss: 421.763092
train Loss: 13.3164 batch_loss: 787.503479
train Loss: 13.9461 batch_loss: 483.574097
train Loss: 14.3476 batch_loss: 308.401367
train Loss: 14.9876 batch_loss: 491.493164
train Loss: 15.5563 batch_loss: 436.739380
train Loss: 16.1563 batch_loss: 460.837311
train Loss: 16.8797 batch_loss: 555.565308
train Loss: 17.4475 batch_loss: 436.067841
train Loss: 18.0997 batch_loss: 500.871643
train Loss: 18.6286 batch_loss: 406.210052
train Loss: 18.9319 batch_loss: 232.940170
train Loss: 19.8227 batch_loss: 684.173035
train Loss: 20.4502 batch_loss: 481.880280
train Loss: 20.9425 batch_loss: 378.087433
train Loss: 21.2950 batch_loss: 270.689148
train Loss: 21.7171 batch_loss: 324.174744
train Loss: 22.4241 batch_loss: 542.971008
train Loss: 23.1867 batch_loss: 585.697449
train Loss: 23.8286 batch_loss: 492.961273
train Loss: 24.5155 batch_loss: 527.577271
train Loss: 25.0706 batch_loss: 426.285431
train Loss: 25.3829 batch_loss: 239.856415
train Loss: 25.9190 batch_loss: 411.730652
train Loss: 26.4092 batch_loss: 376.474670
train Loss: 27.3411 batch_loss: 715.712891
train Loss: 27.9647 batch_loss: 478.957703
train Loss: 28.4545 batch_loss: 376.140076
train Loss: 28.9582 batch_loss: 386.860870
train Loss: 29.3168 batch_loss: 275.382233
train Loss: 29.8025 batch_loss: 372.989655
train Loss: 30.2226 batch_loss: 322.681366
train Loss: 30.6612 batch_loss: 336.844543
train Loss: 30.8957 batch_loss: 180.037567
train Loss: 31.3801 batch_loss: 372.086639
train Loss: 32.1432 batch_loss: 586.012512
train Loss: 32.6047 batch_loss: 354.410828
train Loss: 33.0767 batch_loss: 362.514648
train Loss: 33.6665 batch_loss: 452.958008
train Loss: 34.1408 batch_loss: 364.280090
train Loss: 34.8002 batch_loss: 506.443298
train Loss: 35.2900 batch_loss: 376.120239
train Loss: 35.8205 batch_loss: 407.430115
train Loss: 36.0280 batch_loss: 159.363968
train Loss: 36.6422 batch_loss: 471.721619
train Loss: 37.1058 batch_loss: 356.072662
train Loss: 37.6511 batch_loss: 418.783630
train Loss: 38.1377 batch_loss: 373.688324
train Loss: 38.9641 batch_loss: 634.648621
train Loss: 39.4544 batch_loss: 376.592926
train Loss: 39.8199 batch_loss: 280.703339
train Loss: 40.2637 batch_loss: 340.809204
train Loss: 40.7021 batch_loss: 336.706665
train Loss: 41.0739 batch_loss: 285.511017
train Loss: 41.7779 batch_loss: 540.676453
train Loss: 42.6626 batch_loss: 679.489075
train Loss: 43.1727 batch_loss: 391.709686
train Loss: 43.8485 batch_loss: 519.079529
train Loss: 44.0293 batch_loss: 138.805435
train Loss: 44.5981 batch_loss: 436.872467
train Loss: 44.9757 batch_loss: 289.977631
train Loss: 45.4737 batch_loss: 382.454254
train Loss: 46.0902 batch_loss: 473.458069
train Loss: 46.2856 batch_loss: 150.132980
train Loss: 46.8505 batch_loss: 433.796814
train Loss: 47.4672 batch_loss: 473.648834
train Loss: 48.4659 batch_loss: 766.959229
train Loss: 48.9153 batch_loss: 345.191193
train Loss: 49.4025 batch_loss: 374.156586
train Loss: 49.9368 batch_loss: 410.345032
train Loss: 50.3608 batch_loss: 325.611267
train Loss: 50.9021 batch_loss: 415.704895
train Loss: 51.5428 batch_loss: 492.113678
train Loss: 52.0997 batch_loss: 427.691132
train Loss: 52.8066 batch_loss: 542.847534
train Loss: 53.1518 batch_loss: 265.133698
train Loss: 53.4903 batch_loss: 260.008911
train Loss: 53.8970 batch_loss: 312.347260
train Loss: 54.3810 batch_loss: 371.675781
train Loss: 54.8058 batch_loss: 326.219696
train Loss: 55.2291 batch_loss: 325.127594
train Loss: 55.8058 batch_loss: 442.899475
train Loss: 56.4733 batch_loss: 512.647583
train Loss: 57.0028 batch_loss: 406.673828
train Loss: 57.5458 batch_loss: 417.011627
train Loss: 57.9615 batch_loss: 319.238770
train Loss: 58.4919 batch_loss: 407.383606
train Loss: 58.9826 batch_loss: 376.839935
train Loss: 59.3887 batch_loss: 311.905273
train Loss: 59.7778 batch_loss: 298.771484
train Loss: 60.5025 batch_loss: 556.585571
train Loss: 60.6903 batch_loss: 144.256805
train Loss: 61.0263 batch_loss: 258.029968
train Loss: 61.5797 batch_loss: 424.995758
train Loss: 61.9802 batch_loss: 307.632141
train Loss: 62.5764 batch_loss: 457.881165
train Loss: 62.9755 batch_loss: 306.483337
train Loss: 63.4516 batch_loss: 365.614929
train Loss: 63.9860 batch_loss: 410.431213
train Loss: 64.4333 batch_loss: 343.510712
train Loss: 64.9684 batch_loss: 410.979034
train Loss: 65.3191 batch_loss: 269.365784
train Loss: 65.8536 batch_loss: 410.467133
train Loss: 66.1565 batch_loss: 232.658768
train Loss: 66.6613 batch_loss: 387.663544
train Loss: 66.9935 batch_loss: 255.110794
train Loss: 67.3887 batch_loss: 303.507996
train Loss: 68.0609 batch_loss: 516.276489
train Loss: 68.5535 batch_loss: 378.341339
train Loss: 69.1800 batch_loss: 481.087524
train Loss: 69.7502 batch_loss: 437.965729
train Loss: 70.2075 batch_loss: 351.154877
train Loss: 70.6577 batch_loss: 345.777069
train Loss: 71.1911 batch_loss: 409.671021
train Loss: 71.4207 batch_loss: 176.331329
train Loss: 71.8485 batch_loss: 328.539490
train Loss: 72.3991 batch_loss: 422.846069
train Loss: 72.8631 batch_loss: 356.353333
train Loss: 73.3140 batch_loss: 346.329529
train Loss: 73.7193 batch_loss: 311.282349
train Loss: 73.9142 batch_loss: 149.616669
train Loss: 74.3936 batch_loss: 368.225647
train Loss: 74.9540 batch_loss: 430.360352
train Loss: 75.4371 batch_loss: 371.061035
train Loss: 75.8695 batch_loss: 332.087250
train Loss: 76.4266 batch_loss: 427.846039
train Loss: 76.8282 batch_loss: 308.379395
train Loss: 77.2031 batch_loss: 287.972260
train Loss: 77.5171 batch_loss: 241.126740
train Loss: 77.9410 batch_loss: 325.581451
train Loss: 78.3741 batch_loss: 332.605499
train Loss: 78.8995 batch_loss: 403.462952
train Loss: 79.3432 batch_loss: 340.795837
train Loss: 79.5860 batch_loss: 186.432220
train Loss: 79.9582 batch_loss: 285.856903
train Loss: 80.4010 batch_loss: 340.075531
train Loss: 80.8236 batch_loss: 324.607758
train Loss: 81.2730 batch_loss: 345.116150
train Loss: 81.7238 batch_loss: 346.175476
train Loss: 82.2808 batch_loss: 427.824188
train Loss: 82.5976 batch_loss: 243.311096
train Loss: 83.1238 batch_loss: 404.084045
train Loss: 83.5232 batch_loss: 306.786194
train Loss: 83.8748 batch_loss: 269.960907
train Loss: 84.2308 batch_loss: 273.418060
train Loss: 84.6651 batch_loss: 333.594116
train Loss: 84.7956 batch_loss: 100.211754
train Loss: 85.3831 batch_loss: 451.176575
train Loss: 85.9036 batch_loss: 399.734558
train Loss: 86.3840 batch_loss: 368.985809
train Loss: 86.7657 batch_loss: 293.160614
train Loss: 87.2014 batch_loss: 334.598480
train Loss: 87.9355 batch_loss: 563.752686
train Loss: 88.3253 batch_loss: 299.384430
train Loss: 88.7880 batch_loss: 355.365234
train Loss: 89.0755 batch_loss: 220.807266
train Loss: 89.3625 batch_loss: 220.385712
train Loss: 89.7583 batch_loss: 303.974457
train Loss: 90.1226 batch_loss: 279.816833
train Loss: 90.4312 batch_loss: 236.978577
train Loss: 90.7878 batch_loss: 273.854675
train Loss: 91.3904 batch_loss: 462.785431
train Loss: 91.8567 batch_loss: 358.119720
train Loss: 92.4019 batch_loss: 418.722046
^CTraceback (most recent call last):
  File "train.py", line 214, in <module>
    model = train_model(model, criterion, optimizer_ft, 
  File "train.py", line 102, in train_model
    _, _, predicted = model(inputs)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 143, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 153, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 75, in parallel_apply
    thread.join()
  File "/usr/local/lib/python3.6/threading.py", line 1056, in join
    self._wait_for_tstate_lock()
  File "/usr/local/lib/python3.6/threading.py", line 1072, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
KeyboardInterrupt
krliu@dm:~/github/ansim\[rliu@dm ansim]$ CUDA_VISIBLE_DEVICES=2,1,3 python3.6 train.py 
GPU in use
Model parameter: 2150785
^CTraceback (most recent call last):
  File "train.py", line 200, in <module>
    model.to(device)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 381, in to
    return self._apply(convert)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 187, in _apply
    module._apply(fn)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 187, in _apply
    module._apply(fn)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 193, in _apply
    param.data = fn(param.data)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 379, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
KeyboardInterrupt
krliu@dm:~/github/ansim\[rliu@dm ansim]$ exit
exit

Script done on Wed 17 Apr 2019 12:11:57 AM EDT
