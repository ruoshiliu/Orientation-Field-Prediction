Script started on Wed 17 Apr 2019 10:49:52 PM EDT
krliu@dm:~/github/ansim\[rliu@dm ansim]$ jupyter notebook --no-browser --port=808[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd ..[3Plscd github/ls[Ktop[1Plsnvidia-smi[6Pexit[2PlsCUDA_VISIBLE_DEVICES=2,1,3 python3.6 train.py [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[17Pvim 4-17_4x32-3x64-2x128.log[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd log[4Plscd github/ansim/ls[Kcd log[4Plsvim 4-17_4x32-3x64-2x128.log [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[17@CUDA_VISIBLE_DEVICES=2,1,3 python3.6 train.py[C
  File "train.py", line 183
    kernel_size=[(5,5),(3,3),(3,3),(3,3),(5,5)]
              ^
SyntaxError: invalid syntax
krliu@dm:~/github/ansim\[rliu@dm ansim]$ CUDA_VISIBLE_DEVICES=2,1,3 python3.6 train.py [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[6Pjupyter notebook --no-browser --port=808[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cls[Kcd ..[3Plscd ..[3Plsjupyter notebook --no-browser --port=808[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[CCUDA_VISIBLE_DEVICES=2,1,3 python3.6 train.py [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Kvim train.py
[?1049h[?1h=[1;31r[34l[34h[?25h[23m[24m[0m[H[J[?25l[31;1H"train.py" 221L, 8993C[1;1H[35mimport[0m pandas [33mas[0m pd
[35mimport[0m numpy [33mas[0m np
[35mfrom[0m PIL [35mimport[0m Image
[35mfrom[0m PIL [35mimport[0m ImageOps
[35mimport[0m PIL
[35mimport[0m torch, torchvision
[35mfrom[0m torch.utils.data [35mimport[0m Dataset, DataLoader
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m ansim_dataset [35mimport[0m ansimDataset, create_circular_mask
[34m# from convolution_lstm import encoderConvLSTM, decoderConvLSTM[0m
[35mfrom[0m ConvLSTM [35mimport[0m ConvLSTM
[35mimport[0m random
[35mimport[0m math
[35mimport[0m torch.nn [33mas[0m nn
[35mimport[0m torch.optim [33mas[0m optim
[35mfrom[0m torch.optim [35mimport[0m lr_scheduler
[35mfrom[0m torch.autograd [35mimport[0m Variable
[35mfrom[0m torchvision [35mimport[0m datasets, models, transforms
[35mimport[0m time
[35mimport[0m os

img_path = [31m'/home/rliu/ansim/data/data/JPEGImages/'[0m
img_list_csv = [31m'/home/rliu/github/ansim/img_list.csv'[0m
train_csv = [31m'/home/rliu/github/ansim/train.csv'[0m
test_csv = [31m'/home/rliu/github/ansim/test.csv'[0m
output_path = [31m'/home/rliu/ansim/models/4-17_64-128-256-128-64/second_and_stronger.weights'[0m

mask = create_circular_mask([31m128[0m,[31m128[0m)
trainset = ansimDataset(img_list_csv = img_list_csv, seq_csv = train_csv, root_dir = img_path, step=[31m20[0m, random_rr[30;1Hotate = [36mTrue[0m, transform=[36mNone[0m)[31;95H6,1[11CTop[6;1H[34h[?25h[?25l[31;95H7[7;1H[34h[?25h[?25l[31;95H8[8;1H[34h[?25h[?25l[31;95H9[9;1H[34h[?25h[?25l[31;95H10,1[10;1H[34h[?25h[?25l[31;96H1[11;1H[34h[?25h[?25l[31;96H2[12;1H[34h[?25h[?25l[31;96H3[13;1H[34h[?25h[?25l[31;96H4[14;1H[34h[?25h[?25l[31;96H5[15;1H[34h[?25h[?25l[31;96H6[16;1H[34h[?25h[?25l[31;96H7[17;1H[34h[?25h[?25l[31;96H8[18;1H[34h[?25h[?25l[31;96H9[19;1H[34h[?25h[?25l[31;95H20[20;1H[34h[?25h[?25l[31;96H1,0-1[21;1H[34h[?25h[?25l[31;96H2,1  [22;1H[34h[?25h[?25l[31;96H3[23;1H[34h[?25h[?25l[31;96H4[24;1H[34h[?25h[?25l[31;96H5[25;1H[34h[?25h[?25l[31;96H6[26;1H[34h[?25h[?25l[31;96H7,0-1[27;1H[34h[?25h[?25l[31;96H8,1  [28;1H[34h[?25h[?25l[31;96H9[29;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1Htrainloader = torch.utils.data.DataLoader(trainset,[31;1H[K[31;95H30,1[11C0%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;46Hbatch_size=[31m8[0m, shuffle=[36mTrue[0m,[31;95H[K[31;95H31,1[11C1%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;46Hnum_workers=[31m2[0m)[31;95H[K[31;95H32,1[11C1%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H33,0-1[9C2%[30;1H[34h[?25h[?25l[1;30r[1;1H[2M[1;31r[29;1Htestset = ansimDataset(img_list_csv = img_list_csv, seq_csv = test_csv, root_dir = img_path, step=[31m20[0m, random_rott[30;1Hate = [36mTrue[0m, transform=[36mNone[0m)[31;95H[K[31;95H34,1[11C3%[29;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1Htestloader = torch.utils.data.DataLoader(testset,[31;95H[K[31;95H35,1[11C3%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;46Hbatch_size=[31m8[0m, shuffle=[36mTrue[0m,[31;95H[K[31;95H36,1[11C4%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;46Hnum_workers=[31m2[0m)[31;95H[K[31;95H37,1[11C4%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H38,0-1[9C5%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1Huse_gpu = torch.cuda.is_available()[31;95H[K[31;95H39,1[11C5%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[33mif[0m use_gpu:[31;95H[K[31;95H40,1[11C6%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;5H[36mprint[0m([31m"GPU in use"[0m)[31;95H[K[31;95H41,1[11C6%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H42,0-1[9C7%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1Hdevice = torch.device([31m"cuda:0"[0m [33mif[0m torch.cuda.is_available() [33melse[0m [31m"cpu"[0m)[31;95H[K[31;95H43,1[11C7%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H44,0-1[9C8%[30;1H[34h[?25h[?25l[1;30r[1;1H[2M[1;31r[29;1H[33mdef[0m [36mtrain_model[0m(model, [30m[43mcriterion[0m, optimizer, scheduler, num_workers = [31m2[0m,  num_epochs=[31m25[0m, batch_size = [31m4[0m, step_sii[30;1Hze = [31m20[0m, image_size = [31m100[0m):[31;95H[K[31;95H45,1[11C9%[29;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;5Hsince = time.time()[31;95H[K[31;95H46,1[11C9%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H47,0-1[8C10%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;5Hbest_model_wts = model.state_dict()[31;95H[K[31;95H48,1[10C10%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;5Hbest_acc = [31m0.0[0m[31;95H[K[31;95H49,1[10C11%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H50,1[10C11%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;5Hepoch_num = [31m0[0m[31;95H[K[31;95H51,1[10C12%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;5H[33mfor[0m epoch [33min[0m [36mrange[0m(num_epochs):[31;95H[K[31;95H52,1[10C12%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9Hepoch_num += [31m1[0m[31;95H[K[31;95H53,1[10C13%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9H[36mprint[0m([31m'Epoch {}/{}'[0m.[36mformat[0m(epoch, num_epochs - [31m1[0m))[31;95H[K[31;95H54,1[10C13%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9H[36mprint[0m([31m'-'[0m * [31m10[0m)[31;95H[K[31;95H55,1[10C14%[30;1H[34h[?25h[?25l[1;30r[1;1H[2M[1;31r[30;9H[34m# Each epoch has a training phase[0m[31;95H[K[31;95H56,0-1[8C15%[29;1H[34h[?25h[?25l[31;96H7,1  [30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9Hscheduler.step()[31;95H[K[31;95H58,1[10C15%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9Hmodel.train([36mTrue[0m)  [34m# Set model to training mode[0m[31;95H[K[31;95H59,1[10C16%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9Hrunning_loss = [31m0.0[0m[31;95H[K[31;95H60,1[10C16%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9Hrunning_corrects = [31m0[0m[31;95H[K[31;95H61,1[10C17%[30;1H[34h[?25h[?25l[1;30r[1;1H[2M[1;31r[31;95H[K[31;95H62,1[10C17%[29;1H[34h[?25h[?25l[31;96H3,0-1[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9H[34m# Iterate over data.[0m[31;95H[K[31;95H64,1[10C18%[30;1H[34h[?25h[?25l[1;30r[1;1H[2M[1;31r[29;9Htrainset = ansimDataset(img_list_csv = img_list_csv, seq_csv = train_csv, root_dir = img_path, step=stepp[30;1H_size, random_rotate = [36mTrue[0m, transform=[36mNone[0m, image_size = image_size)[31;95H[K[31;95H65,1[10C19%[29;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9Htrainloader = torch.utils.data.DataLoader(trainset,[31;95H[K[31;95H66,1[10C19%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;54Hbatch_size=batch_size, shuffle=[36mTrue[0m,[31;95H[K[31;95H67,1[10C20%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;54Hnum_workers=num_workers)[31;95H[K[31;95H68,1[10C20%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H69,0-1[8C21%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9H[36mprint[0m([31m"trainloader ready!"[0m)[31;95H[K[31;95H70,1[10C21%[30;1H[34h[?25h[?25l[1;30r[1;1H[2M[1;31r[29;9Htestset = ansimDataset(img_list_csv = img_list_csv, seq_csv = test_csv, root_dir = img_path, step=step_ss[30;1Hize, random_rotate = [36mFalse[0m, transform=[36mNone[0m, image_size = image_size)[31;95H[K[31;95H71,1[10C22%[29;1H[34h[?25h[?25l[1;30r[1;1H[2M[1;31r[29;9Htestloader = torch.utils.data.DataLoader(testset,[30;54Hbatch_size=batch_size, shuffle=[36mFalse[0m,[31;95H[K[31;95H72,1[10C23%[29;1H[34h[?25h[?25l[31;96H3[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;54Hnum_workers=num_workers)[31;95H[K[31;95H74,1[10C23%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9H[36mprint[0m([31m"testloader ready!"[0m)[31;95H[K[31;95H75,1[10C24%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H76,1[10C24%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9H[33mfor[0m data [33min[0m trainloader:[31;95H[K[31;95H77,1[10C25%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[34m# get the inputs[0m[31;95H[K[31;95H78,1[10C25%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Hdata_split = torch.split(data, [36mint[0m(data.shape[[31m1[0m]/[31m2[0m), dim=[31m1[0m)[31;95H[K[31;95H79,1[10C26%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Hinputs = data_split[[31m0[0m][31;95H[K[31;95H80,1[10C26%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Htarget = data_split[[31m1[0m][31;95H[K[31;95H81,1[10C27%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#            print(inputs)[0m[31;95H[K[31;95H82,1[10C27%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[34m# wrap them in Variable[0m[31;95H[K[31;95H83,1[10C28%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[33mif[0m use_gpu:[31;95H[K[31;95H84,1[10C29%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17Hinputs, target = inputs.to(device), target.to(device)[31;95H[K[31;95H85,1[10C29%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[33melse[0m:[31;95H[K[31;95H86,1[10C30%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17Hinputs, target = Variable(inputs), Variable(target)[31;95H[K[31;95H87,1[10C30%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H88,0-1[8C31%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[34m# zero the parameter gradients[0m[31;95H[K[31;95H89,1[10C31%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Hoptimizer.zero_grad()[31;95H[K[31;95H90,1[10C32%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H91,1[10C32%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#             output, h, c, states = encoder(inputs)[0m[31;95H[K[31;95H92,1[10C33%[30;1H[34h[?25h[?25l[1;30r[1;1H[2M[1;31r[29;1H[34m#             output_last = output[0][0].double()
#             h1,h2,h3,h4,h5 = states[0][0],states[1][0],states[2][0],states[3][0],states[4][0][0m[31;95H[K[31;95H93,1[10C33%[29;1H[34h[?25h[?25l[31;96H4[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#             c1,c2,c3,c4,c5 = states[0][1],states[1][1],states[2][1],states[3][1],states[4][1][0m[31;95H[K[31;95H95,1[10C34%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#             states_cat = torch.cat((h1,h2,h3,h4,h5,c1,c2,c3,c4,c5), dim=1, out=None)[0m[31;95H[K[31;95H96,1[10C34%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#             x = decoder.activateConv(states_cat)[0m[31;95H[K[31;95H97,1[10C35%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#             input_d = [x, states][0m[31;95H[K[31;95H98,1[10C35%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#             output_d, h_d, c_d, states_d = decoder(input_d)[0m[31;95H[K[31;95H99,1[10C36%[30;1H[34h[?25h[?25l[1;30r[1;1H[2M[1;31r[29;1H[34m#             predicted = torch.cat(output_d, dim=0, out=None).double()[0m[31;95H[K[31;95H100,1[9C37%[29;1H[34h[?25h[?25l[31;97H1[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H_, _, predicted = model(inputs)[31;95H[K[31;95H102,1[9C37%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[34m#loss = 0.0[0m[31;95H[K[31;95H103,1[9C38%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[34m#for t in range(predicted.shape[1]):[0m[31;95H[K[31;95H104,1-8[7C38%[30;8H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[34m#   loss += [0m[30m[43mcriterion[0m[34m(predicted[:,t,:,:,:].long(), target[:,t,:,:,:].long())[0m[31;95H[K[31;95H105,1-8[7C39%[30;8H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[34m#pred_dim = predicted.permute(0,2,1,3,4)[0m[31;95H[K[31;95H106,1[9C39%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#             m = nn.Sigmoid()[0m[31;95H[K[31;95H107,1[9C40%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Hloss = [30m[43mcriterion[0m(predicted, target)[31;95H[K[31;95H108,1[9C40%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[34m# forward[0m[31;95H[K[31;95H109,1[9C41%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#            outputs = model(inputs)[0m[31;95H[K[31;95H110,1[9C41%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#            _, preds = torch.max(outputs.data, 1)[0m[31;95H[K[31;95H111,1[9C42%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#            loss = [0m[30m[43mcriterion[0m[34m(outputs, labels)[0m[31;95H[K[31;95H112,1[9C42%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H113,0-1[7C43%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Hloss.backward()[31;95H[K[31;95H114,1[9C43%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Hoptimizer.step()[31;95H[K[31;95H115,1[9C44%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H116,0-1[7C45%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[34m# statistics[0m[31;95H[K[31;95H117,1[9C45%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Hiter_loss = loss.item()[31;95H[K[31;95H118,1[9C46%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Hrunning_loss += loss.item()[31;95H[K[31;95H119,1[9C46%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Hepoch_loss = running_loss / [36mlen[0m(trainset)[31;95H[K[31;95H120,1[9C47%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H121,1[9C47%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[36mprint[0m([31m'{} Loss: {:.4f} batch_loss: {:f}'[0m.[36mformat[0m([31;95H[K[31;95H122,1[9C48%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17H[31m"train"[0m, epoch_loss, iter_loss))[31;95H[K[31;95H123,1[9C48%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H124,1[9C49%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9H[33mwith[0m torch.no_grad():[31;95H[K[31;95H125,1[9C49%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Hrunning_loss_test = [31m0.0[0m[31;95H[K[31;95H126,1[9C50%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[33mfor[0m data [33min[0m testloader:[31;95H[K[31;95H127,1[9C50%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17Hdata_split = torch.split(data, [36mint[0m(data.shape[[31m1[0m]/[31m2[0m), dim=[31m1[0m)[31;95H[K[31;95H128,1[9C51%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17Hinputs = data_split[[31m0[0m][31;95H[K[31;95H129,1[9C51%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17Htarget = data_split[[31m1[0m][31;95H[K[31;95H130,1[9C52%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H131,1[9C52%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H132,1[9C53%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17H[33mif[0m use_gpu:[31;95H[K[31;95H133,1[9C53%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;21Hinputs, target = inputs.to(device), target.to(device)[31;95H[K[31;95H134,1[9C54%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17H[33melse[0m:[31;95H[K[31;95H135,1[9C54%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;21Hinputs, target = Variable(inputs), Variable(target)[31;95H[K[31;95H136,1[9C55%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H137,0-1[7C56%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H138,1[9C56%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#                 output, h, c, states = encoder(inputs)[0m[31;95H[K[31;95H139,1[9C57%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#                 output_last = output[0][0].double()[0m[31;95H[K[31;95H140,1[9C57%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#                 h1,h2,h3,h4,h5 = states[0][0],states[1][0],states[2][0],states[3][0],states[4][0][0m[31;95H[K[31;95H141,1[9C58%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#                 c1,c2,c3,c4,c5 = states[0][1],states[1][1],states[2][1],states[3][1],states[4][1][0m[31;95H[K[31;95H142,1[9C58%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#                 states_cat = torch.cat((h1,h2,h3,h4,h5,c1,c2,c3,c4,c5), dim=1, out=None)[0m[31;95H[K[31;95H143,1[9C59%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#                 x = decoder.activateConv(states_cat)[0m[31;95H[K[31;95H144,1[9C59%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#                 input_d = [x, states][0m[31;95H[K[31;95H145,1[9C60%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#                 output_d, h_d, c_d, states_d = decoder(input_d)[0m[31;95H[K[31;95H146,1[9C60%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#                 predicted = torch.cat(output_d, dim=0, out=None).double()[0m[31;95H[K[31;95H147,1[9C61%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H148,0-1[7C61%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17H_, _, predicted = model(inputs)[31;95H[K[31;95H149,1[9C62%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17H[34m#loss_test = 0.0[0m[31;95H[K[31;95H150,1[9C62%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17H[34m#for t in range(predicted.shape[1]):[0m[31;95H[K[31;95H151,1[9C63%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17H[34m#       loss_test += [0m[30m[43mcriterion[0m[34m(predicted[:,t,:,:,:].long(), target[:,t,:,:,:].long()) [0m[31;95H[K[31;95H152,1[9C63%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17H[34m#pred_dim = predicted.permute(0,2,1,3,4)[0m[31;95H[K[31;95H153,1[9C64%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H154,1[9C64%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#                 m = nn.Sigmoid()[0m[31;95H[K[31;95H155,1[9C65%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17Hloss_test = [30m[43mcriterion[0m(predicted, target)[31;95H[K[31;95H156,1[9C65%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17Hiter_loss_test = loss_test.item()[31;95H[K[31;95H157,1[9C66%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17Hrunning_loss_test += loss_test.item()[31;95H[K[31;95H158,1[9C67%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;17Hepoch_loss_test = running_loss_test / [36mlen[0m(testset)[31;95H[K[31;95H159,1[9C67%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H160,0-1[7C68%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9H[36mprint[0m([31m'Loss on the test images: %.5f '[0m % ([31;95H[K[31;95H161,1[9C68%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Hepoch_loss_test))[31;95H[K[31;95H162,1[9C69%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9H[33mif[0m epoch_num % [31m5[0m == [31m0[0m:[31;95H[K[31;95H163,1[9C69%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13H[36mprint[0m([31m'saving wiehgts...'[0m)[31;95H[K[31;95H164,1[9C70%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Houtput_path = [31m"/home/rliu/ansim/models/4-17_64-128-256-128-64/%0.4d.weights"[0m % (epoch_num)[31;95H[K[31;95H165,1[9C70%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;13Htorch.save(model, output_path)[31;95H[K[31;95H166,1[9C71%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H167,0-1[7C71%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;5Htime_elapsed = time.time() - since[31;95H[K[31;95H168,1[9C72%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;5H[36mprint[0m([31m'Training complete in {:.0f}m {:.0f}s'[0m.[36mformat[0m([31;95H[K[31;95H169,1[9C72%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;9Htime_elapsed // [31m60[0m, time_elapsed % [31m60[0m))[31;95H[K[31;95H170,1[9C73%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;5H[36mprint[0m([31m'Best val Acc: {:4f}'[0m.[36mformat[0m(best_acc))[31;95H[K[31;95H171,1[9C73%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H172,0-1[7C74%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;5H[34m# load best model weights[0m[31;95H[K[31;95H173,1[9C74%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;5Hmodel.load_state_dict(best_model_wts)[31;95H[K[31;95H174,1[9C75%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#       print('saving wiehgts.../n')[0m[31;95H[K[31;95H175,1[9C75%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m#       output_path = sprintf("/home/rliu/ansim/models/%0.4d.weights" % epoc[0m[31;95H[K[31;95H176,1[9C76%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;5H[33mreturn[0m model[31;95H[K[31;95H177,1[9C76%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[34m# transfer learning resnet18[0m[31;95H[K[31;95H178,1[9C77%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1Hstep_size = [31m20[0m[31;95H[K[31;95H179,1[9C78%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1Hmodel = ConvLSTM(input_size=([31m128[0m,[31m128[0m),[31;95H[K[31;95H180,1[9C78%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;18Hinput_dim=[31m1[0m,[31;95H[K[31;95H181,1[9C79%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;18Hhidden_dim=[[31m64[0m,[31m128[0m,[31m256[0m,[31m128[0m,[31m64[0m][31;95H[K[31;95H182,1[9C79%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;18Hkernel_size=[([31m5[0m,[31m5[0m),([31m3[0m,[31m3[0m),([31m3[0m,[31m3[0m),([31m3[0m,[31m3[0m),([31m5[0m,[31m5[0m)][31;95H[K[31;95H183,1[9C80%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;18Hnum_layers=[31m5[0m,[31;95H[K[31;95H184,1[9C80%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;18Hpredict_steps=[36mint[0m(step_size/[31m2[0m),[31;95H[K[31;95H185,1[9C81%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;18Hbatch_first=[36mTrue[0m,[31;95H[K[31;95H186,1[9C81%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;18Hbias=[36mTrue[0m,[31;95H[K[31;95H187,1[9C82%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;18Hreturn_all_layers=[36mTrue[0m)[31;95H[K[31;95H188,1[9C82%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H189,0-1[7C83%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H190,0-1[7C83%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1Hcount_param = [36msum[0m(p.numel() [33mfor[0m p [33min[0m model.parameters() [33mif[0m p.requires_grad)[31;95H[K[31;95H191,1[9C84%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[30;1H[36mprint[0m([31m"Model parameter: %d"[0m % count_param)[31;95H[K[31;95H192,1[9C84%[30;1H[34h[?25h[?25l[1;30r[30;1H
[1;31r[31;95H[K[31;95H193,0-1[7C85%[30;1H[34h[?25h[?25l[31;97H2,1  [29;1H[34h[?25h[?25l[31;97H1[28;1H[34h[?25h[?25l[31;97H0,0-1[27;1H[34h[?25h[?25l[31;96H89[26;1H[34h[?25h[?25l[31;97H8,1  [25;1H[34h[?25h[?25l[31;97H7[24;1H[34h[?25h[?25l[31;97H6[23;1H[34h[?25h[?25l[31;97H5[22;1H[34h[?25h[?25l[31;97H4[21;1H[34h[?25h[?25l[31;97H3[20;1H[34h[?25h[?25l[31;99H2[20;2H[34h[?25h[?25l[31;99H3[20;3H[34h[?25h[?25l[31;99H4[20;4H[34h[?25h[?25l[31;99H5[20;5H[34h[?25h[?25l[31;99H6[20;6H[34h[?25h[?25l[31;99H7[20;7H[34h[?25h[?25l[31;99H8[20;8H[34h[?25h[?25l[31;99H9[20;9H[34h[?25h[?25l[31;99H10[20;10H[34h[?25h[?25l[31;100H1[20;11H[34h[?25h[?25l[31;100H2[20;12H[34h[?25h[?25l[31;100H3[20;13H[34h[?25h[?25l[31;100H4[20;14H[34h[?25h[?25l[31;100H5[20;15H[34h[?25h[?25l[31;100H6[20;16H[34h[?25h[?25l[31;100H7[20;17H[34h[?25h[?25l[31;100H8[20;18H[34h[?25h[?25l[31;100H9[20;19H[34h[?25h[?25l[31;99H20[20;20H[34h[?25h[?25l[31;100H1[20;21H[34h[?25h[?25l[31;100H2[20;22H[34h[?25h[?25l[31;100H3[20;23H[34h[?25h[?25l[31;100H4[20;24H[34h[?25h[?25l[31;100H5[20;25H[34h[?25h[?25l[31;100H6[20;26H[34h[?25h[?25l[31;100H7[20;27H[34h[?25h[?25l[31;100H8[20;28H[34h[?25h[?25l[31;100H9[20;29H[34h[?25h[?25l=[46m[[29C][0m[31;99H30[20;30H[34h[?25h[?25l[[46m([3C)[0m[24C][31;100H1[20;31H[34h[?25h[?25l([3C)[31;100H2[20;32H[34h[?25h[?25l[31;100H3[20;33H[34h[?25h[?25l[31;100H4[20;34H[34h[?25h[?25l[46m([3C)[0m[31;100H5[20;35H[34h[?25h[?25l([3C)[31;100H6[20;36H[34h[?25h[?25l,[46m([3C)[0m[31;100H7[20;37H[34h[?25h[?25l([3C)[31;100H8[20;38H[34h[?25h[?25l[31;100H9[20;39H[34h[?25h[?25l[31;99H40[20;40H[34h[?25h[?25l[46m([3C)[0m[31;100H1[20;41H[34h[?25h[?25l([3C)[31;100H2[20;42H[34h[?25h[?25l,[46m([3C)[0m[31;100H3[20;43H[34h[?25h[?25l([3C)[31;100H4[20;44H[34h[?25h[?25l[31;100H5[20;45H[34h[?25h[?25l[31;100H6[20;46H[34h[?25h[?25l[46m([3C)[0m[31;100H7[20;47H[34h[?25h[?25l([3C)[31;100H8[20;48H[34h[?25h[?25l,[46m([3C)[0m[31;100H9[20;49H[34h[?25h[?25l([3C)[31;99H50[20;50H[34h[?25h[?25l[31;100H1[20;51H[34h[?25h[?25l[31;100H2[20;52H[34h[?25h[?25l[46m([3C)[0m[31;100H3[20;53H[34h[?25h[?25l([3C)[31;100H4[20;54H[34h[?25h[?25l,[46m([3C)[0m[31;100H5[20;55H[34h[?25h[?25l([3C)[31;100H6[20;56H[34h[?25h[?25l[31;100H7[20;57H[34h[?25h[?25l[31;100H8[20;58H[34h[?25h[?25l[46m([3C)[0m[31;100H9[20;59H[34h[?25h[?25l[20;30H[46m[[0m[24C([3C)[46m][0m[31;99H60[20;60H[34h[?25h[?25l[31;1H[1m-- INSERT --[0m[31;95H[K[31;95H183,60[8C85%[20;60H[34h[?25h[?25l[31;95H[K[31;95H183,61[8C85%[20;61H[34h[?25h[?25l,[20;30H[[29C][31;100H2[20;62H[34h[?25h[31;1H[K[20;61H[?25l[31;95H183,61[8C85%[20;61H[34h[?25h[?25l[31;1H"train.py"[31;95H[K[31;12H221L, 8994C written
[?1l>[34h[?25h[?1049lkrliu@dm:~/github/ansim\[rliu@dm ansim]$ vim train.pyCUDA_VISIBLE_DEVICES=2,1,3 python3.6 train.py [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvim train.py[K[Knvidia-smi
Wed Apr 17 22:50:52 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN Xp            Off  | 00000000:05:00.0 Off |                  N/A |
| 36%   59C    P2    63W / 250W |  11631MiB / 12195MiB |     36%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN Xp            Off  | 00000000:06:00.0 Off |                  N/A |
| 42%   57C    P0    64W / 250W |      0MiB / 12196MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  TITAN Xp            Off  | 00000000:09:00.0 Off |                  N/A |
| 30%   48C    P0    58W / 250W |      0MiB / 12196MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  TITAN Xp            Off  | 00000000:0A:00.0 Off |                  N/A |
| 24%   43C    P0    57W / 250W |      0MiB / 12196MiB |      6%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     24169      C   python                                     11621MiB |
+-----------------------------------------------------------------------------+
krliu@dm:~/github/ansim\[rliu@dm ansim]$ nvidia-smivim train.pyCUDA_VISIBLE_DEVICES=2,1,3 python3.6 train.py [1P[1P[1P[1P[1P[1@1[1@2[1P[1@,[1@2[1@,[1@3
  File "train.py", line 183
    kernel_size=[(5,5),(3,3),(3,3),(3,3),(5,5)],
              ^
SyntaxError: invalid syntax
krliu@dm:~/github/ansim\[rliu@dm ansim]$ CUDA_VISIBLE_DEVICES=1,2,3 python3.6 train.py [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cnvidia-smi[Kvim train.pyCUDA_VISIBLE_DEVICES=2,1,3 python3.6 train.py [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[6Pjupyter notebook --no-browser --port=808[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[CCUDA_VISIBLE_DEVICES=2,1,3 python3.6 train.py [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvim train.py[K[2Pnvidia-smivim train.py
[?1049h[?1h=[1;31r[34l[34h[?25h[23m[24m[0m[H[J[?25l[31;1H"train.py" 221L, 8994C[1;5H[36mprint[0m([31m'Training complete in {:.0f}m {:.0f}s'[0m.[36mformat[0m([2;9Htime_elapsed // [31m60[0m, time_elapsed % [31m60[0m))
    [36mprint[0m([31m'Best val Acc: {:4f}'[0m.[36mformat[0m(best_acc))[5;5H[34m# load best model weights[0m
    model.load_state_dict(best_model_wts)
[34m#       print('saving wiehgts.../n')
#       output_path = sprintf("/home/rliu/ansim/models/%0.4d.weights" % epoc[0m
    [33mreturn[0m model
[34m# transfer learning resnet18[0m
step_size = [31m20[0m
model = ConvLSTM(input_size=([31m128[0m,[31m128[0m),[13;18Hinput_dim=[31m1[0m,[14;18Hhidden_dim=[[31m64[0m,[31m128[0m,[31m256[0m,[31m128[0m,[31m64[0m][15;18Hkernel_size=[([31m5[0m,[31m5[0m),([31m3[0m,[31m3[0m),([31m3[0m,[31m3[0m),([31m3[0m,[31m3[0m),([31m5[0m,[31m5[0m)],[16;18Hnum_layers=[31m5[0m,[17;18Hpredict_steps=[36mint[0m(step_size/[31m2[0m),[18;18Hbatch_first=[36mTrue[0m,[19;18Hbias=[36mTrue[0m,[20;18Hreturn_all_layers=[36mTrue[0m)


count_param = [36msum[0m(p.numel() [33mfor[0m p [33min[0m model.parameters() [33mif[0m p.requires_grad)
[36mprint[0m([31m"Model parameter: %d"[0m % count_param)[28;1H[33mif[0m use_gpu:
[34m#     encoder = torch.nn.DataParallel(encoder)
#     decoder = torch.nn.DataParallel(decoder)[0m[31;95H183,18[8C87%[15;18H[34h[?25h[?25l[31;97H4[16;18H[34h[?25h[?25l[31;97H5[17;18H[34h[?25h[?25l[31;97H4[16;18H[34h[?25h[?25l[31;97H3[15;18H[34h[?25h[?25l[31;97H2[14;18H[34h[?25h[?25l[31;97H3[15;18H[34h[?25h[?25l[31;97H4[16;18H[34h[?25h[?25l[31;97H5[17;18H[34h[?25h[?25l[31;97H6[18;18H[34h[?25h[?25l[31;97H7[19;18H[34h[?25h[?25l[31;97H6[18;18H[34h[?25h[?25l[31;97H5[17;18H[34h[?25h[?25l[31;97H4[16;18H[34h[?25h[?25l[31;97H3[15;18H[34h[?25h[?25l[31;97H2[14;18H[34h[?25h[?25l[31;97H1[13;18H[34h[?25h[?25l[31;97H2[14;18H[34h[?25h[?25l[31;97H3[15;18H[34h[?25h[?25l[31;97H4[16;18H[34h[?25h[?25l[31;97H3[15;18H[34h[?25h[?25l[31;100H9[15;19H[34h[?25h[?25l[31;99H20[15;20H[34h[?25h[?25l[31;100H1[15;21H[34h[?25h[?25l[31;100H2[15;22H[34h[?25h[?25l[31;100H3[15;23H[34h[?25h[?25l[31;100H4[15;24H[34h[?25h[?25l[31;100H5[15;25H[34h[?25h[?25l[31;100H6[15;26H[34h[?25h[?25l[31;100H7[15;27H[34h[?25h[?25l[31;100H8[15;28H[34h[?25h[?25l[31;100H9[15;29H[34h[?25h[?25l=[46m[[29C][0m[31;99H30[15;30H[34h[?25h[?25l[[46m([3C)[0m[24C][31;100H1[15;31H[34h[?25h[?25l[46m[[0m([3C)[24C[46m][0m[31;100H0[15;30H[34h[?25h[?25l[[46m([3C)[0m[24C][31;100H1[15;31H[34h[?25h[?25l([3C)[31;97H2[14;31H[34h[?25h[?25l[31;100H2[14;32H[34h[?25h[?25l[31;100H3[14;33H[34h[?25h[?25l[31;100H4[14;34H[34h[?25h[?25l[31;100H5[14;35H[34h[?25h[?25l[31;100H6[14;36H[34h[?25h[?25l[31;100H7[14;37H[34h[?25h[?25l[31;100H8[14;38H[34h[?25h[?25l[31;100H9[14;39H[34h[?25h[?25l[31;99H40[14;40H[34h[?25h[?25l[31;100H1[14;41H[34h[?25h[?25l[31;100H2[14;42H[34h[?25h[?25l[31;100H3[14;43H[34h[?25h[?25l[31;100H4[14;44H[34h[?25h[?25l[31;100H5[14;45H[34h[?25h[?25l[31;100H6[14;46H[34h[?25h[?25l[14;29H[46m[[17C][0m[31;100H7[14;47H[34h[?25h[?25l[31;1H[1m-- INSERT --[0m[31;13H[K[31;95H182,47[8C87%[14;47H[34h[?25h[?25l[31;95H[K[31;95H182,48[8C87%[14;48H[34h[?25h[?25l,[14;29H[[17C][31;100H9[14;49H[34h[?25h[31;1H[K[14;48H[?25l[31;95H182,48[8C87%[14;48H[34h[?25h[?25l[31;1H"train.py"[31;95H[K[31;12H221L, 8995C written
[?1l>[34h[?25h[?1049lkrliu@dm:~/github/ansim\[rliu@dm ansim]$ vim train.pyCUDA_VISIBLE_DEVICES=1,2,3 python3.6 train.py 
GPU in use
Model parameter: 7841153
Epoch 0/199
----------
trainloader ready!
testloader ready!
Traceback (most recent call last):
  File "train.py", line 220, in <module>
    image_size = 128)
  File "train.py", line 102, in train_model
    _, _, predicted = model(inputs)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 144, in forward
    return self.gather(outputs, self.output_device)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 156, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py", line 67, in gather
    return gather_map(outputs)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py", line 62, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py", line 54, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/nn/parallel/_functions.py", line 68, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/rliu/.local/lib/python3.6/site-packages/torch/cuda/comm.py", line 166, in gather
    return torch._C._gather(tensors, dim, destination)
RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 11.91 GiB total capacity; 11.13 GiB already allocated; 1.06 MiB free; 153.11 MiB cached) (malloc at /pytorch/aten/src/THC/THCCachingAllocator.cpp:231)
frame #0: std::function<std::string ()>::operator()() const + 0x11 (0x7f9bc12a0fe1 in /home/rliu/.local/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7f9bc12a0dfa in /home/rliu/.local/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #2: <unknown function> + 0x13cf9c5 (0x7f9b6b45d9c5 in /home/rliu/.local/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so)
frame #3: <unknown function> + 0x13d077a (0x7f9b6b45e77a in /home/rliu/.local/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, at::TensorOptions const&) + 0x443 (0x7f9b6c5f0a43 in /home/rliu/.local/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so)
frame #5: at::CUDAFloatType::empty(c10::ArrayRef<long>, at::TensorOptions const&) const + 0x161 (0x7f9b6b377531 in /home/rliu/.local/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so)
frame #6: torch::autograd::VariableType::empty(c10::ArrayRef<long>, at::TensorOptions const&) const + 0x179 (0x7f9b600c9df9 in /home/rliu/.local/lib/python3.6/site-packages/torch/lib/libtorch.so.1)
frame #7: torch::cuda::gather(c10::ArrayRef<at::Tensor>, long, c10::optional<int>) + 0x28b (0x7f9b9ae3351b in /home/rliu/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x4f501c (0x7f9b9ae3601c in /home/rliu/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x116fac (0x7f9b9aa57fac in /home/rliu/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #10: _PyCFunction_FastCallKeywords + 0x307 (0x4aa207 in python3.6)
frame #11: python3.6() [0x541094]
frame #12: _PyEval_EvalFrameDefault + 0x3584 (0x545604 in python3.6)
frame #13: python3.6() [0x540cb1]
frame #14: python3.6() [0x540faf]
frame #15: _PyEval_EvalFrameDefault + 0x3584 (0x545604 in python3.6)
frame #16: python3.6() [0x540cb1]
frame #17: PyEval_EvalCodeEx + 0x6d (0x541bad in python3.6)
frame #18: python3.6() [0x481edc]
frame #19: PyObject_Call + 0x60 (0x451f00 in python3.6)
frame #20: THPFunction_apply(_object*, _object*) + 0x581 (0x7f9b9ac554d1 in /home/rliu/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #21: PyCFunction_Call + 0xc1 (0x4a9e41 in python3.6)
frame #22: _PyEval_EvalFrameDefault + 0x786c (0x5498ec in python3.6)
frame #23: python3.6() [0x540cb1]
frame #24: _PyFunction_FastCallDict + 0x156 (0x549df6 in python3.6)
frame #25: _PyObject_FastCallDict + 0x1ef (0x45222f in python3.6)
frame #26: python3.6() [0x53c001]
frame #27: python3.6() [0x484ea5]
frame #28: python3.6() [0x485405]
frame #29: python3.6() [0x4bf82a]
frame #30: _PyObject_FastCallDict + 0xa2 (0x4520e2 in python3.6)
frame #31: python3.6() [0x540e05]
frame #32: _PyEval_EvalFrameDefault + 0x3584 (0x545604 in python3.6)
frame #33: python3.6() [0x540cb1]
frame #34: _PyFunction_FastCallDict + 0x156 (0x549df6 in python3.6)
frame #35: _PyObject_FastCallDict + 0x1ef (0x45222f in python3.6)
frame #36: python3.6() [0x53c001]
frame #37: PySequence_Tuple + 0xc9 (0x454259 in python3.6)
frame #38: python3.6() [0x4ba9b9]
frame #39: python3.6() [0x4bf793]
frame #40: _PyObject_FastCallDict + 0xa2 (0x4520e2 in python3.6)
frame #41: python3.6() [0x540e05]
frame #42: _PyEval_EvalFrameDefault + 0x3584 (0x545604 in python3.6)
frame #43: python3.6() [0x540cb1]
frame #44: python3.6() [0x540faf]
frame #45: _PyEval_EvalFrameDefault + 0x3584 (0x545604 in python3.6)
frame #46: python3.6() [0x540cb1]
frame #47: python3.6() [0x540faf]
frame #48: _PyEval_EvalFrameDefault + 0x435b (0x5463db in python3.6)
frame #49: python3.6() [0x53ffa1]
frame #50: python3.6() [0x5411d7]
frame #51: _PyEval_EvalFrameDefault + 0x3584 (0x545604 in python3.6)
frame #52: python3.6() [0x540cb1]
frame #53: _PyFunction_FastCallDict + 0x156 (0x549df6 in python3.6)
frame #54: _PyObject_FastCallDict + 0x1ef (0x45222f in python3.6)
frame #55: _PyObject_Call_Prepend + 0xcb (0x45232b in python3.6)
frame #56: PyObject_Call + 0x60 (0x451f00 in python3.6)
frame #57: _PyEval_EvalFrameDefault + 0x4262 (0x5462e2 in python3.6)
frame #58: python3.6() [0x540cb1]
frame #59: _PyFunction_FastCallDict + 0x156 (0x549df6 in python3.6)
frame #60: _PyObject_FastCallDict + 0x1ef (0x45222f in python3.6)
frame #61: _PyObject_Call_Prepend + 0xcb (0x45232b in python3.6)
frame #62: PyObject_Call + 0x60 (0x451f00 in python3.6)
frame #63: python3.6() [0x4c5463]

krliu@dm:~/github/ansim\[rliu@dm ansim]$ s[Kls
[0m[01;35m1.jpg[0m              ConvLSTM.py          img_list.csv      [01;34m__pycache__[0m  train.csv
ansim_dataset.py   ConvLSTM.pyc         indiecoder.ipynb  README.md    train.ipynb
ansim_dataset.pyc  convolution_lstm.py  LICENSE           test.csv     train.py
ConvLSTM.ipynb     dataset.ipynb        [01;34mlog[0m               test.ipynb   Untitled1.ipynb
krliu@dm:~/github/ansim\[rliu@dm ansim]$ cd [K[K[Kvim train.py
[?1049h[?1h=[1;31r[34l[34h[?25h[23m[24m[0m[H[J[?25l[31;1H"train.py" 221L, 8995C[1;5Htime_elapsed = time.time() - since
    [36mprint[0m([31m'Training complete in {:.0f}m {:.0f}s'[0m.[36mformat[0m([3;9Htime_elapsed // [31m60[0m, time_elapsed % [31m60[0m))
    [36mprint[0m([31m'Best val Acc: {:4f}'[0m.[36mformat[0m(best_acc))[6;5H[34m# load best model weights[0m
    model.load_state_dict(best_model_wts)
[34m#       print('saving wiehgts.../n')
#       output_path = sprintf("/home/rliu/ansim/models/%0.4d.weights" % epoc[0m
    [33mreturn[0m model
[34m# transfer learning resnet18[0m
step_size = [31m20[0m
model = ConvLSTM(input_size=([31m128[0m,[31m128[0m),[14;18Hinput_dim=[31m1[0m,[15;18Hhidden_dim=[[31m64[0m,[31m128[0m,[31m256[0m,[31m128[0m,[31m64[0m],[16;18Hkernel_size=[([31m5[0m,[31m5[0m),([31m3[0m,[31m3[0m),([31m3[0m,[31m3[0m),([31m3[0m,[31m3[0m),([31m5[0m,[31m5[0m)],[17;18Hnum_layers=[31m5[0m,[18;18Hpredict_steps=[36mint[0m(step_size/[31m2[0m),[19;18Hbatch_first=[36mTrue[0m,[20;18Hbias=[36mTrue[0m,[21;18Hreturn_all_layers=[36mTrue[0m)


count_param = [36msum[0m(p.numel() [33mfor[0m p [33min[0m model.parameters() [33mif[0m p.requires_grad)
[36mprint[0m([31m"Model parameter: %d"[0m % count_param)[29;1H[33mif[0m use_gpu:
[34m#     encoder = torch.nn.DataParallel(encoder)[0m[31;95H182,18[8C87%[15;18H[34h[?25h[?25l[31;100H9[15;19H[34h[?25h[?25l[31;99H20[15;20H[34h[?25h[?25l[31;100H1[15;21H[34h[?25h[?25l[31;100H2[15;22H[34h[?25h[?25l[31;100H3[15;23H[34h[?25h[?25l[31;100H4[15;24H[34h[?25h[?25l[31;100H5[15;25H[34h[?25h[?25l[31;100H6[15;26H[34h[?25h[?25l[31;100H7[15;27H[34h[?25h[?25l[31;100H8[15;28H[34h[?25h[?25l=[46m[[17C][0m[31;100H9[15;29H[34h[?25h[?25l[[17C][31;99H30[15;30H[34h[?25h[?25l[31;100H1[15;31H[34h[?25h[?25l[31;100H2[15;32H[34h[?25h[?25l[31;100H3[15;33H[34h[?25h[?25l[31;1H[1m-- INSERT --[0m[31;13H[K[31;95H182,33[8C87%[15;33H[34h[?25h[?25l[31;97H3[16;33H[34h[?25h[?25l[31;97H2[15;33H[34h[?25h[?25l[31;97H3[16;33H[34h[?25h[?25l[31;97H2[15;33H[34h[?25h[?25l[31;100H4[15;34H[34h[?25h[?25l[31;100H5[15;35H[34h[?25h[?25l[31;100H6[15;36H[34h[?25h[?25l[31;100H7[15;37H[34h[?25h[?25l[31;100H6[15;36H[34h[?25h[?25l[31;100H7[15;37H[34h[?25h[?25l[31;100H8[15;38H[34h[?25h[?25l[31;100H9[15;39H[34h[?25h[?25l[31;99H40[15;40H[34h[?25h[?25l,[31m128[0m,[31m64[0m],[15;48H[K[31;99H39[15;39H[34h[?25h[?25l,[31m128[0m,[31m64[0m],[15;47H[K[31;100H8[15;38H[34h[?25h[?25l,[31m128[0m,[31m64[0m],[15;46H[K[31;100H7[15;37H[34h[?25h[?25l[31m128[0m,[31m64[0m],[15;45H[K[31;100H6[15;36H[34h[?25h[?25l[16;31H[46m([3C)[0m[31;95H[K[31;95H183,36[8C87%[16;36H[34h[?25h[?25l([3C),[46m([3C)[0m[31;100H7[16;37H[34h[?25h[?25l[31;95H[K[31;95H183,38[8C87%[16;38H[34h[?25h[?25l([3C)[31;100H9[16;39H[34h[?25h[?25l[31;99H40[16;40H[34h[?25h[?25l[46m([3C)[0m[31;100H1[16;41H[34h[?25h[?25l[31;95H[K[31;95H183,42[8C87%[16;42H[34h[?25h[?25l([3C),[46m([3C)[0m[31;100H3[16;43H[34h[?25h[?25l[31;95H[K[31;95H183,44[8C87%[16;44H[34h[?25h[?25l([3C)[31;100H5[16;45H[34h[?25h[?25l[31;100H6[16;46H[34h[?25h[?25l[46m([3C)[0m[31;100H7[16;47H[34h[?25h[?25l[31;95H[K[31;95H183,48[8C87%[16;48H[34h[?25h[?25l([3C),[46m([3C)[0m[31;100H9[16;49H[34h[?25h[?25l([31m[46m3[0m,[31m3[0m)[46m,[0m([31m5[0m,[31m5[0m)],[16;61H[K[16;48H[46m([0m[31m3[0m[2C[46m)[0m,[31;100H8[16;48H[34h[?25h[?25l([31m[46m3[0m,[31m3[0m)[46m,[0m([31m5[0m,[31m5[0m)],[16;60H[K[16;47H[46m([0m[31m3[0m[2C[46m)[0m,[31;100H7[16;47H[34h[?25h[?25l([31m[46m3[0m,[31m3[0m)[46m,[0m([31m5[0m,[31m5[0m)],[16;59H[K[16;46H[46m([0m[31m3[0m[2C[46m)[0m,[31;100H6[16;46H[34h[?25h[?25l([31m[46m3[0m,[31m3[0m)[46m,[0m([31m5[0m,[31m5[0m)],[16;58H[K[16;45H[46m([0m[31m3[0m[2C[46m)[0m,[31;100H5[16;45H[34h[?25h[?25l([31m[46m3[0m,[31m3[0m)[46m,[0m([31m5[0m,[31m5[0m)],[16;57H[K[16;44H[46m([0m[31m3[0m[2C[46m)[0m,[31;100H4[16;44H[34h[?25h[?25l[31m[46m3[0m,[31m3[0m)[46m,[0m([31m5[0m,[31m5[0m)],[16;56H[K[16;43H[46m([0m[31m3[0m[2C[46m)[0m,[31;100H3[16;43H[34h[?25h[?25l[46m([3C)[0m,([3C)[31;95H[K[31;95H183,42[8C87%[16;42H[34h[?25h[?25l[31;100H1[16;41H[34h[?25h[?25l[31;95H[K[31;95H183,42[8C87%[16;42H[34h[?25h[?25l[31;100H1[16;41H[34h[?25h[?25l)[46m,[0m([31m3[0m,[31m3[0m),([31m5[0m,[31m5[0m)],[16;55H[K[16;40H[46m)[0m,[31;100H0[16;40H[34h[?25h[?25l[31m[46m5[0m),([31m3[0m,[31m3[0m),([31m5[0m,[31m5[0m)],[16;40H[31m5[0m[46m)[0m[31;100H1[16;41H[34h[?25h[?25l([3C)[31;100H0[16;40H[34h[?25h[?25l[31;99H39[16;39H[34h[?25h[?25l,[31m5[0m),([31m3[0m,[31m3[0m),([31m5[0m,[31m5[0m)],[16;55H[K[16;37H[46m([2C)[0m[31;95H[K[31;95H183,38[8C87%[16;38H[34h[?25h[?25l[31m5[0m,[31m[46m5[0m),([31m3[0m,[31m3[0m),([31m5[0m,[31m5[0m)],[16;37H([2C[31m5[0m[31;100H9[16;39H[34h[?25h[?25l[31;99H40[16;40H[34h[?25h[?25l[46m([3C)[0m[31;100H1[16;41H[34h[?25h[?25l[31;95H[K[31;95H183,42[8C87%[16;42H[34h[?25h[?25l([3C),[46m([3C)[0m[31;100H3[16;43H[34h[?25h[?25l[31;95H[K[31;95H183,44[8C87%[16;44H[34h[?25h[?25l([3C)[31;100H5[16;45H[34h[?25h[?25l[31;100H6[16;46H[34h[?25h[?25l[46m([3C)[0m[31;100H7[16;47H[34h[?25h[?25l)[46m,[0m([31m5[0m,[31m5[0m)],[16;55H[K[16;46H[46m)[0m,[31;100H6[16;46H[34h[?25h[?25l([2C)[31;100H5[16;45H[34h[?25h[?25l[46m([2C)[0m[31;95H[K[31;95H183,44[8C87%[16;44H[34h[?25h[?25l([2C)[31;100H5[16;45H[34h[?25h[?25l,),([31m5[0m,[31m5[0m)],[16;54H[K[16;43H[46m([0m,[46m)[0m[31;95H[K[31;95H183,44[8C87%[16;44H[34h[?25h[?25l[31m5[0m[46m,[0m),([31m5[0m,[31m5[0m)],[16;43H([1C,[31;100H5[16;45H[34h[?25h[?25l[46m([2C)[0m[31;100H6[16;46H[34h[?25h[?25l[31m[46m5[0m),([31m5[0m,[31m5[0m)],[16;46H[31m5[0m[46m)[0m[31;100H7[16;47H[34h[?25h[?25l[31;95H[K[31;95H183,48[8C87%[16;48H[34h[?25h[?25l([3C),[46m([3C)[0m[31;100H9[16;49H[34h[?25h[?25l[31;95H[K[31;95H183,50[8C87%[16;50H[34h[?25h[?25l([3C)[31;100H1[16;51H[34h[?25h[?25l[31;100H2[16;52H[34h[?25h[?25l[46m([3C)[0m[31;100H3[16;53H[34h[?25h[?25l[16;30H[46m[[0m[18C([3C)[46m][0m[31;100H4[16;54H[34h[?25h[?25l[31;95H[K[31;95H183,55[8C87%[16;55H[34h[?25h[?25l[31;100H4[16;54H[34h[?25h[?25l][46m,[0m[16;55H[K[16;53H[46m][0m,[31;100H3[16;53H[34h[?25h[?25l][46m,[0m[16;54H[K[16;52H[46m][0m,[31;100H2[16;52H[34h[?25h[?25l][46m,[0m[16;53H[K[16;51H[46m][0m,[31;100H1[16;51H[34h[?25h[?25l][46m,[0m[16;52H[K[16;50H[46m][0m,[31;100H0[16;50H[34h[?25h[?25l][46m,[0m[16;51H[K[16;49H[46m][0m,[31;99H49[16;49H[34h[?25h[?25l][46m,[0m[16;50H[K[16;48H[46m][0m,[31;100H8[16;48H[34h[?25h[?25l[16;30H[[17C][31;97H2,45[15;45H[34h[?25h[?25l[15;29H[46m[[13C][0m[31;95H[K[31;95H182,44[8C87%[15;44H[34h[?25h[?25l[31;100H3[15;43H[34h[?25h[?25l][46m,[0m[15;44H[K[15;42H[46m][0m,[31;100H2[15;42H[34h[?25h[?25l][46m,[0m[15;43H[K[15;41H[46m][0m,[31;100H1[15;41H[34h[?25h[?25l][46m,[0m[15;42H[K[15;40H[46m][0m,[31;100H0[15;40H[34h[?25h[?25l][46m,[0m[15;41H[K[15;39H[46m][0m,[31;99H39[15;39H[34h[?25h[?25l][46m,[0m[15;40H[K[15;38H[46m][0m,[31;100H8[15;38H[34h[?25h[?25l][46m,[0m[15;39H[K[15;37H[46m][0m,[31;100H7[15;37H[34h[?25h[?25l][46m,[0m[15;38H[K[15;36H[46m][0m,[31;100H6[15;36H[34h[?25h[?25l][46m,[0m[15;37H[K[15;35H[46m][0m,[31;100H5[15;35H[34h[?25h[?25l][46m,[0m[15;36H[K[15;34H[46m][0m,[31;100H4[15;34H[34h[?25h[?25l][46m,[0m[15;35H[K[15;33H[46m][0m,[31;100H3[15;33H[34h[?25h[?25l][46m,[0m[15;34H[K[15;32H[46m][0m,[31;100H2[15;32H[34h[?25h[?25l][46m,[0m[15;33H[K[15;31H[46m][0m,[31;100H1[15;31H[34h[?25h[?25l][46m,[0m[15;32H[K[15;30H[46m][0m,[31;100H0[15;30H[34h[?25h[?25l[31m[46m1[0m],[31m1[0m[46m][0m[31;100H1[15;31H[34h[?25h[?25l[31m[46m2[0m],[31m2[0m[46m][0m[31;100H2[15;32H[34h[?25h[?25l[31m[46m8[0m],[31m8[0m[46m][0m[31;100H3[15;33H[34h[?25h[?25l[46m,[0m],,[46m][0m[31;100H4[15;34H[34h[?25h[?25l[31m[46m1[0m],[31m1[0m[46m][0m[31;100H5[15;35H[34h[?25h[?25l[31m[46m2[0m],[31m2[0m[46m][0m[31;100H6[15;36H[34h[?25h[?25l[31m[46m8[0m],[31m8[0m[46m][0m[31;100H7[15;37H[34h[?25h[?25l[46m,[0m],,[46m][0m[31;100H8[15;38H[34h[?25h[?25l[31m[46m2[0m],[31m2[0m[46m][0m[31;100H9[15;39H[34h[?25h[?25l[31m[46m5[0m],[31m5[0m[46m][0m[31;99H40[15;40H[34h[?25h[?25l[31m[46m6[0m],[31m6[0m[46m][0m[31;100H1[15;41H[34h[?25h[?25l[15;29H[[11C][16;37H[46m([3C)[0m[31;97H3[16;41H[34h[?25h[?25l([3C)[31;97H4,3[17;31H[34h[?25h[?25l[31;100H0[17;30H[34h[?25h[?25l,[17;30H[K[31;99H29[17;29H[34h[?25h[?25l[31m3[0m,[31;99H30[17;30H[34h[?25h[31;1H[K[17;29H[?25l[31;95H184,29[8C87%[17;29H[34h[?25h[?25l[31;1H"train.py"[31;95H[K[31;12H221L, 8977C written
[?1l>[34h[?25h[?1049lkrliu@dm:~/github/ansim\[rliu@dm ansim]$ ls
[0m[01;35m1.jpg[0m              ConvLSTM.py          img_list.csv      [01;34m__pycache__[0m  train.csv
ansim_dataset.py   ConvLSTM.pyc         indiecoder.ipynb  README.md    train.ipynb
ansim_dataset.pyc  convolution_lstm.py  LICENSE           test.csv     train.py
ConvLSTM.ipynb     dataset.ipynb        [01;34mlog[0m               test.ipynb   Untitled1.ipynb
krliu@dm:~/github/ansim\[rliu@dm ansim]$ lsvim train.pyls[KCUDA_VISIBLE_DEVICES=1,2,3 python3.6 train.py 
GPU in use
Model parameter: 14760961
Epoch 0/199
----------
trainloader ready!
testloader ready!
train Loss: 11.7510 batch_loss: 9024.805664
train Loss: 25.8128 batch_loss: 10799.410156
train Loss: 38.5220 batch_loss: 9760.683594
train Loss: 53.0034 batch_loss: 11121.684570
train Loss: 63.1655 batch_loss: 7804.503418
train Loss: 74.8451 batch_loss: 8969.967773
train Loss: 92.0571 batch_loss: 13218.817383
train Loss: 100.1416 batch_loss: 6208.859863
train Loss: 111.2205 batch_loss: 8508.577148
train Loss: 126.1889 batch_loss: 11495.739258
train Loss: 138.9736 batch_loss: 9818.682617
train Loss: 152.1223 batch_loss: 10098.189453
train Loss: 164.1795 batch_loss: 9259.910156
train Loss: 177.3448 batch_loss: 10110.954102
train Loss: 188.6109 batch_loss: 8652.374023
train Loss: 203.1334 batch_loss: 11153.290039
train Loss: 214.1722 batch_loss: 8477.784180
train Loss: 224.8903 batch_loss: 8231.525391
train Loss: 237.5492 batch_loss: 9721.994141
train Loss: 245.4452 batch_loss: 6064.157227
train Loss: 257.2240 batch_loss: 9046.116211
train Loss: 267.5702 batch_loss: 7945.916992
train Loss: 283.2716 batch_loss: 12058.653320
train Loss: 290.3088 batch_loss: 5404.599121
train Loss: 300.3726 batch_loss: 7728.962402
train Loss: 311.1220 batch_loss: 8255.533203
train Loss: 323.2264 batch_loss: 9296.206055
train Loss: 329.6827 batch_loss: 4958.405762
train Loss: 338.7185 batch_loss: 6939.539551
train Loss: 346.2485 batch_loss: 5783.037598
train Loss: 355.2911 batch_loss: 6944.700195
train Loss: 364.4035 batch_loss: 6998.286133
train Loss: 374.3633 batch_loss: 7649.176758
train Loss: 386.3958 batch_loss: 9240.942383
train Loss: 398.3463 batch_loss: 9177.939453
train Loss: 409.9327 batch_loss: 8898.395508
train Loss: 421.5844 batch_loss: 8948.527344
train Loss: 429.8105 batch_loss: 6317.584473
train Loss: 439.3945 batch_loss: 7360.559570
train Loss: 447.2458 batch_loss: 6029.751953
train Loss: 458.7068 batch_loss: 8802.083008
train Loss: 464.7359 batch_loss: 4630.376465
train Loss: 474.1683 batch_loss: 7244.061523
train Loss: 483.2906 batch_loss: 7005.914551
train Loss: 494.4239 batch_loss: 8550.339844
train Loss: 501.9283 batch_loss: 5763.442383
train Loss: 510.3258 batch_loss: 6449.255371
train Loss: 515.9150 batch_loss: 4292.504395
train Loss: 525.1598 batch_loss: 7099.981445
train Loss: 534.5924 batch_loss: 7244.255371
train Loss: 545.7086 batch_loss: 8537.276367
train Loss: 557.4605 batch_loss: 9025.426758
train Loss: 564.0772 batch_loss: 5081.632324
train Loss: 572.2564 batch_loss: 6281.612305
train Loss: 580.6548 batch_loss: 6449.953125
train Loss: 592.4443 batch_loss: 9054.357422
train Loss: 601.7480 batch_loss: 7145.273926
train Loss: 611.7725 batch_loss: 7698.780273
train Loss: 619.2728 batch_loss: 5760.234375
train Loss: 627.1727 batch_loss: 6067.147949
train Loss: 633.9083 batch_loss: 5172.942871
train Loss: 639.5641 batch_loss: 4343.664551
train Loss: 645.9879 batch_loss: 4933.433105
train Loss: 652.4256 batch_loss: 4944.194824
train Loss: 658.2094 batch_loss: 4441.893066
train Loss: 665.6297 batch_loss: 5698.815430
train Loss: 673.3163 batch_loss: 5903.290527
train Loss: 679.1722 batch_loss: 4497.351074
train Loss: 683.8679 batch_loss: 3606.304932
train Loss: 693.3368 batch_loss: 7272.142578
train Loss: 698.4602 batch_loss: 3934.717041
train Loss: 706.4843 batch_loss: 6162.556641
train Loss: 718.4733 batch_loss: 9207.556641
train Loss: 723.5077 batch_loss: 3866.421387
train Loss: 731.7324 batch_loss: 6316.509766
train Loss: 740.0880 batch_loss: 6417.111328
train Loss: 748.2979 batch_loss: 6305.217773
train Loss: 756.8760 batch_loss: 6587.957520
train Loss: 763.6858 batch_loss: 5229.960938
train Loss: 770.4732 batch_loss: 5212.735352
train Loss: 781.8096 batch_loss: 8706.314453
train Loss: 789.4389 batch_loss: 5859.358398
train Loss: 798.8722 batch_loss: 7244.736328
train Loss: 805.0099 batch_loss: 4713.754395
train Loss: 810.9016 batch_loss: 4524.827148
train Loss: 820.6294 batch_loss: 7470.933105
train Loss: 829.8291 batch_loss: 7065.358398
train Loss: 835.2691 batch_loss: 4177.991211
train Loss: 843.9107 batch_loss: 6636.737305
train Loss: 850.0446 batch_loss: 4710.843262
train Loss: 856.6171 batch_loss: 5047.668457
train Loss: 861.1536 batch_loss: 3483.993164
train Loss: 867.5809 batch_loss: 4936.169922
train Loss: 873.4868 batch_loss: 4535.709961
train Loss: 881.8959 batch_loss: 6458.241699
train Loss: 888.9539 batch_loss: 5420.510742
train Loss: 898.3874 batch_loss: 7244.919922
train Loss: 908.1252 batch_loss: 7478.634277
train Loss: 914.3614 batch_loss: 4789.407715
train Loss: 923.9138 batch_loss: 7336.249023
train Loss: 931.4392 batch_loss: 5779.534180
train Loss: 938.1697 batch_loss: 5169.035645
train Loss: 945.2125 batch_loss: 5408.860352
train Loss: 953.3122 batch_loss: 6220.554688
train Loss: 961.8763 batch_loss: 6577.216797
train Loss: 966.1443 batch_loss: 3277.814697
train Loss: 969.4920 batch_loss: 2571.084717
train Loss: 974.6021 batch_loss: 3924.496582
train Loss: 981.6262 batch_loss: 5394.500000
train Loss: 988.4894 batch_loss: 5270.959473
train Loss: 992.1306 batch_loss: 2796.466064
train Loss: 996.7293 batch_loss: 3531.753662
train Loss: 1001.3510 batch_loss: 3549.507812
train Loss: 1006.1702 batch_loss: 3701.101074
train Loss: 1011.4132 batch_loss: 4026.653564
train Loss: 1016.5450 batch_loss: 3941.208740
train Loss: 1020.3844 batch_loss: 2948.648682
train Loss: 1024.7185 batch_loss: 3328.636963
train Loss: 1030.1677 batch_loss: 4184.953125
train Loss: 1036.4459 batch_loss: 4821.661621
train Loss: 1040.0480 batch_loss: 2766.457520
train Loss: 1046.6094 batch_loss: 5039.091797
train Loss: 1051.8207 batch_loss: 4002.290771
train Loss: 1058.1926 batch_loss: 4893.604004
train Loss: 1061.5453 batch_loss: 2574.909180
train Loss: 1064.1315 batch_loss: 1986.183472
train Loss: 1070.6783 batch_loss: 5027.976074
train Loss: 1075.7154 batch_loss: 3868.486816
train Loss: 1078.7853 batch_loss: 2357.640625
train Loss: 1083.7421 batch_loss: 3806.881836
train Loss: 1090.8341 batch_loss: 5446.619141
train Loss: 1097.5531 batch_loss: 5160.215820
train Loss: 1103.5415 batch_loss: 4599.041992
train Loss: 1109.4959 batch_loss: 4572.992676
train Loss: 1113.1819 batch_loss: 2830.856689
train Loss: 1117.9438 batch_loss: 3657.160889
train Loss: 1120.9771 batch_loss: 2329.545166
train Loss: 1123.2037 batch_loss: 1710.060181
train Loss: 1127.4585 batch_loss: 3267.704590
train Loss: 1131.8017 batch_loss: 3335.515381
train Loss: 1134.6132 batch_loss: 2159.282715
train Loss: 1138.6766 batch_loss: 3120.690186
train Loss: 1142.1421 batch_loss: 2661.504150
train Loss: 1147.1681 batch_loss: 3859.982666
train Loss: 1149.8842 batch_loss: 2085.922607
train Loss: 1154.3573 batch_loss: 3435.347900
train Loss: 1157.1059 batch_loss: 2110.910645
train Loss: 1161.4860 batch_loss: 3363.900635
train Loss: 1166.1984 batch_loss: 3619.144287
train Loss: 1171.0759 batch_loss: 3745.970459
train Loss: 1175.4976 batch_loss: 3395.801025
train Loss: 1180.3202 batch_loss: 3703.781738
train Loss: 1182.6345 batch_loss: 1777.413818
train Loss: 1186.0448 batch_loss: 2619.063721
train Loss: 1189.2912 batch_loss: 2493.224365
train Loss: 1194.5435 batch_loss: 4033.812988
train Loss: 1199.8426 batch_loss: 4069.709473
train Loss: 1204.9224 batch_loss: 3901.243652
train Loss: 1209.1855 batch_loss: 3274.065674
train Loss: 1212.5516 batch_loss: 2585.181152
train Loss: 1217.9143 batch_loss: 4118.593262
train Loss: 1220.9031 batch_loss: 2295.393311
train Loss: 1225.1787 batch_loss: 3283.646973
train Loss: 1230.0187 batch_loss: 3717.136963
train Loss: 1232.1965 batch_loss: 1672.515869
train Loss: 1236.0956 batch_loss: 2994.514893
train Loss: 1240.3564 batch_loss: 3272.301514
train Loss: 1244.3039 batch_loss: 3031.642822
train Loss: 1246.8319 batch_loss: 1941.514893
train Loss: 1250.2009 batch_loss: 2587.417725
train Loss: 1254.5797 batch_loss: 3362.893799
train Loss: 1256.9019 batch_loss: 1783.465210
train Loss: 1261.1137 batch_loss: 3234.692383
train Loss: 1263.3739 batch_loss: 1735.832886
train Loss: 1265.3453 batch_loss: 1514.005737
train Loss: 1270.1451 batch_loss: 3686.251465
train Loss: 1273.2899 batch_loss: 2415.176025
train Loss: 1275.4807 batch_loss: 1682.572510
train Loss: 1278.6033 batch_loss: 2398.115479
train Loss: 1282.0954 batch_loss: 2681.976074
train Loss: 1283.7164 batch_loss: 1244.895264
train Loss: 1287.4431 batch_loss: 2862.144531
train Loss: 1292.0980 batch_loss: 3574.951904
train Loss: 1296.7932 batch_loss: 3605.918213
train Loss: 1299.5872 batch_loss: 2145.781250
train Loss: 1302.4300 batch_loss: 2183.228271
train Loss: 1306.5582 batch_loss: 3170.463623
train Loss: 1310.2338 batch_loss: 2822.927002
train Loss: 1312.5079 batch_loss: 1746.437744
train Loss: 1314.1447 batch_loss: 1257.082153
train Loss: 1315.9330 batch_loss: 1373.411255
train Loss: 1320.4501 batch_loss: 3469.158691
train Loss: 1323.8700 batch_loss: 2626.494385
train Loss: 1326.5416 batch_loss: 2051.741211
train Loss: 1328.1353 batch_loss: 1224.020752
train Loss: 1329.7370 batch_loss: 1230.071167
train Loss: 1333.5876 batch_loss: 2957.271973
train Loss: 1335.9750 batch_loss: 1833.515625
train Loss: 1341.6647 batch_loss: 4369.651367
train Loss: 1346.2385 batch_loss: 3512.727295
train Loss: 1348.3090 batch_loss: 1590.094849
train Loss: 1352.8132 batch_loss: 3459.282959
train Loss: 1357.4051 batch_loss: 3526.596436
train Loss: 1360.3010 batch_loss: 2224.002197
train Loss: 1362.0354 batch_loss: 1332.035889
train Loss: 1363.2638 batch_loss: 943.395203
train Loss: 1366.6968 batch_loss: 2636.552002
train Loss: 1369.7318 batch_loss: 2330.854736
train Loss: 1372.7773 batch_loss: 2338.937500
train Loss: 1374.1234 batch_loss: 1033.852051
train Loss: 1379.2919 batch_loss: 3969.425049
train Loss: 1383.3050 batch_loss: 3082.063477
train Loss: 1385.7622 batch_loss: 1887.087036
train Loss: 1388.1119 batch_loss: 1804.567139
train Loss: 1391.7319 batch_loss: 2780.198975
train Loss: 1394.9060 batch_loss: 2437.683838
train Loss: 1396.1398 batch_loss: 947.567322
train Loss: 1397.6977 batch_loss: 1196.418823
train Loss: 1399.7453 batch_loss: 1572.606079
train Loss: 1401.2640 batch_loss: 1166.373657
train Loss: 1404.7316 batch_loss: 2663.062012
train Loss: 1405.8301 batch_loss: 843.672913
train Loss: 1407.2977 batch_loss: 1127.116577
train Loss: 1411.1589 batch_loss: 2965.397949
train Loss: 1414.2066 batch_loss: 2340.668213
train Loss: 1415.3350 batch_loss: 866.564880
train Loss: 1417.3635 batch_loss: 1557.907593
train Loss: 1419.9352 batch_loss: 1975.036255
train Loss: 1422.9873 batch_loss: 2344.060303
train Loss: 1424.6118 batch_loss: 1247.616821
train Loss: 1425.7538 batch_loss: 877.057739
train Loss: 1428.2362 batch_loss: 1906.486084
train Loss: 1432.6730 batch_loss: 3407.428711
train Loss: 1435.0641 batch_loss: 1836.363770
train Loss: 1437.0844 batch_loss: 1551.582764
train Loss: 1439.3771 batch_loss: 1760.839600
train Loss: 1443.1992 batch_loss: 2935.374023
train Loss: 1446.0260 batch_loss: 2170.924561
train Loss: 1448.1915 batch_loss: 1663.124146
train Loss: 1451.8985 batch_loss: 2847.012939
train Loss: 1453.1450 batch_loss: 957.277771
train Loss: 1455.3386 batch_loss: 1684.731201
train Loss: 1457.6304 batch_loss: 1760.100098
train Loss: 1459.9109 batch_loss: 1751.355957
train Loss: 1461.8122 batch_loss: 1460.255127
train Loss: 1463.7973 batch_loss: 1524.499390
train Loss: 1467.2960 batch_loss: 2687.015625
train Loss: 1468.1375 batch_loss: 646.257935
train Loss: 1471.5034 batch_loss: 2585.044189
train Loss: 1472.9650 batch_loss: 1122.547363
train Loss: 1475.8953 batch_loss: 2250.399658
train Loss: 1477.7809 batch_loss: 1448.142334
train Loss: 1480.5796 batch_loss: 2149.403809
train Loss: 1482.3789 batch_loss: 1381.874268
train Loss: 1484.1787 batch_loss: 1382.241943
train Loss: 1486.1464 batch_loss: 1511.185669
Loss on the test images: 581.30465 
Epoch 1/199
----------
trainloader ready!
testloader ready!
train Loss: 3.0169 batch_loss: 2316.994385
train Loss: 4.1129 batch_loss: 841.702576
train Loss: 7.0225 batch_loss: 2234.562988
train Loss: 9.9675 batch_loss: 2261.764648
train Loss: 10.9203 batch_loss: 731.799988
train Loss: 12.1510 batch_loss: 945.106750
train Loss: 15.9886 batch_loss: 2947.343506
train Loss: 18.3691 batch_loss: 1828.205933
train Loss: 19.3970 batch_loss: 789.423401
train Loss: 21.1107 batch_loss: 1316.144409
train Loss: 24.2897 batch_loss: 2441.403564
train Loss: 27.3445 batch_loss: 2346.148438
train Loss: 28.1672 batch_loss: 631.823914
train Loss: 30.0194 batch_loss: 1422.477783
train Loss: 33.2891 batch_loss: 2511.108643
train Loss: 37.2792 batch_loss: 3064.390625
train Loss: 38.1751 batch_loss: 688.077881
train Loss: 39.0006 batch_loss: 633.975891
train Loss: 41.5932 batch_loss: 1991.108032
train Loss: 42.0759 batch_loss: 370.734375
train Loss: 43.6642 batch_loss: 1219.821533
train Loss: 45.3705 batch_loss: 1310.408936
train Loss: 46.9211 batch_loss: 1190.878784
train Loss: 48.1449 batch_loss: 939.856506
train Loss: 49.4917 batch_loss: 1034.392822
train Loss: 50.1312 batch_loss: 491.076050
train Loss: 52.6740 batch_loss: 1952.908447
train Loss: 54.3136 batch_loss: 1259.203735
train Loss: 55.7339 batch_loss: 1090.783203
train Loss: 57.9101 batch_loss: 1671.296509
train Loss: 59.5012 batch_loss: 1221.993286
train Loss: 61.2927 batch_loss: 1375.876099
train Loss: 62.1043 batch_loss: 623.309753
train Loss: 64.7044 batch_loss: 1996.845459
train Loss: 65.5251 batch_loss: 630.363220
train Loss: 67.9031 batch_loss: 1826.249878
train Loss: 69.7542 batch_loss: 1421.637329
train Loss: 71.8389 batch_loss: 1601.051880
train Loss: 73.5753 batch_loss: 1333.610474
train Loss: 74.6901 batch_loss: 856.158630
train Loss: 77.3710 batch_loss: 2058.914307
train Loss: 78.2910 batch_loss: 706.544800
train Loss: 80.0491 batch_loss: 1350.247314
train Loss: 82.0742 batch_loss: 1555.292236
train Loss: 82.8915 batch_loss: 627.690613
train Loss: 85.7387 batch_loss: 2186.577637
train Loss: 89.0447 batch_loss: 2539.070068
train Loss: 91.1910 batch_loss: 1648.351562
train Loss: 92.6940 batch_loss: 1154.272827
train Loss: 94.2269 batch_loss: 1177.299927
train Loss: 95.6127 batch_loss: 1064.307983
train Loss: 97.8245 batch_loss: 1698.626343
train Loss: 99.1723 batch_loss: 1035.085571
train Loss: 100.5431 batch_loss: 1052.828125
train Loss: 102.7150 batch_loss: 1667.957031
train Loss: 103.4744 batch_loss: 583.223328
train Loss: 104.7696 batch_loss: 994.742126
train Loss: 105.5115 batch_loss: 569.768799
train Loss: 106.2049 batch_loss: 532.553955
train Loss: 107.7195 batch_loss: 1163.222290
train Loss: 110.4812 batch_loss: 2120.950195
train Loss: 112.0507 batch_loss: 1205.380127
train Loss: 113.8254 batch_loss: 1362.959106
train Loss: 115.9952 batch_loss: 1666.410645
train Loss: 118.5204 batch_loss: 1939.401733
train Loss: 120.1887 batch_loss: 1281.208252
train Loss: 120.6892 batch_loss: 384.378632
train Loss: 121.5071 batch_loss: 628.168884
train Loss: 123.5731 batch_loss: 1586.663208
train Loss: 125.8284 batch_loss: 1732.089966
train Loss: 126.5543 batch_loss: 557.501831
train Loss: 128.1575 batch_loss: 1231.231201
train Loss: 128.9035 batch_loss: 572.940674
train Loss: 131.0847 batch_loss: 1675.142456
train Loss: 133.0929 batch_loss: 1542.328369
train Loss: 135.3258 batch_loss: 1714.898926
train Loss: 137.3797 batch_loss: 1577.345947
train Loss: 138.3023 batch_loss: 708.556091
train Loss: 138.7937 batch_loss: 377.391632
train Loss: 141.6859 batch_loss: 2221.214111
train Loss: 142.4064 batch_loss: 553.346924
train Loss: 143.9405 batch_loss: 1178.207764
train Loss: 144.5687 batch_loss: 482.445129
train Loss: 145.9493 batch_loss: 1060.297241
train Loss: 147.2898 batch_loss: 1029.526245
train Loss: 149.5217 batch_loss: 1714.074585
train Loss: 150.1387 batch_loss: 473.888489
train Loss: 151.6802 batch_loss: 1183.838623
train Loss: 152.9948 batch_loss: 1009.587891
train Loss: 154.4866 batch_loss: 1145.713379
train Loss: 154.9369 batch_loss: 345.832672
train Loss: 156.6731 batch_loss: 1333.416870
train Loss: 157.8964 batch_loss: 939.533508
train Loss: 159.3300 batch_loss: 1100.964600
train Loss: 160.4134 batch_loss: 832.070740
train Loss: 161.6578 batch_loss: 955.721558
train Loss: 162.6151 batch_loss: 735.134094
train Loss: 164.3975 batch_loss: 1368.920654
train Loss: 165.9830 batch_loss: 1217.656982
train Loss: 167.2543 batch_loss: 976.390930
train Loss: 167.9566 batch_loss: 539.377014
train Loss: 169.9339 batch_loss: 1518.511719
train Loss: 171.2915 batch_loss: 1042.627686
train Loss: 172.1205 batch_loss: 636.724854
train Loss: 172.5698 batch_loss: 345.074646
train Loss: 174.1187 batch_loss: 1189.505005
train Loss: 174.6622 batch_loss: 417.415680
train Loss: 174.9892 batch_loss: 251.134827
train Loss: 175.8374 batch_loss: 651.409180
train Loss: 177.0027 batch_loss: 894.950806
train Loss: 178.1625 batch_loss: 890.722900
train Loss: 178.6422 batch_loss: 368.435150
train Loss: 180.0035 batch_loss: 1045.494751
train Loss: 180.6593 batch_loss: 503.616913
train Loss: 182.9025 batch_loss: 1722.781738
train Loss: 184.4374 batch_loss: 1178.781982
train Loss: 185.5985 batch_loss: 891.769409
train Loss: 186.4391 batch_loss: 645.550781
train Loss: 188.1553 batch_loss: 1318.028687
train Loss: 189.9473 batch_loss: 1376.311768
train Loss: 190.5173 batch_loss: 437.743958
train Loss: 192.8517 batch_loss: 1792.783081
train Loss: 194.6006 batch_loss: 1343.160645
train Loss: 195.3399 batch_loss: 567.781372
train Loss: 196.6315 batch_loss: 991.944092
train Loss: 198.0273 batch_loss: 1072.027588
train Loss: 198.4792 batch_loss: 347.042511
train Loss: 200.3505 batch_loss: 1437.143433
train Loss: 201.7948 batch_loss: 1109.192871
train Loss: 203.0103 batch_loss: 933.510925
train Loss: 204.8482 batch_loss: 1411.562134
train Loss: 206.8618 batch_loss: 1546.390381
train Loss: 207.4744 batch_loss: 470.534973
train Loss: 208.7203 batch_loss: 956.818176
train Loss: 209.9817 batch_loss: 968.719666
train Loss: 210.7969 batch_loss: 626.108704
train Loss: 211.9383 batch_loss: 876.627502
train Loss: 213.0293 batch_loss: 837.822205
train Loss: 214.7739 batch_loss: 1339.857544
train Loss: 215.2257 batch_loss: 347.035583
train Loss: 216.3355 batch_loss: 852.325439
train Loss: 217.4826 batch_loss: 880.955688
train Loss: 218.3141 batch_loss: 638.612671
train Loss: 219.7312 batch_loss: 1088.321167
train Loss: 222.2494 batch_loss: 1933.974243
train Loss: 224.1450 batch_loss: 1455.838257
train Loss: 224.8058 batch_loss: 507.488312
train Loss: 226.7030 batch_loss: 1456.991089
train Loss: 227.2601 batch_loss: 427.910370
train Loss: 228.9107 batch_loss: 1267.614380
train Loss: 230.6841 batch_loss: 1362.034668
train Loss: 232.0024 batch_loss: 1012.379150
train Loss: 232.5422 batch_loss: 414.566345
train Loss: 233.4174 batch_loss: 672.186157
train Loss: 234.6005 batch_loss: 908.592529
train Loss: 236.3135 batch_loss: 1315.616699
train Loss: 238.4873 batch_loss: 1669.464111
train Loss: 239.1481 batch_loss: 507.475494
train Loss: 240.2125 batch_loss: 817.472229
train Loss: 240.6417 batch_loss: 329.620514
train Loss: 242.0784 batch_loss: 1103.416138
train Loss: 243.2348 batch_loss: 888.124817
train Loss: 243.6844 batch_loss: 345.283783
train Loss: 244.8124 batch_loss: 866.264526
train Loss: 245.2867 batch_loss: 364.286011
train Loss: 246.2926 batch_loss: 772.549438
train Loss: 247.0718 batch_loss: 598.380676
train Loss: 248.1816 batch_loss: 852.372498
train Loss: 250.6607 batch_loss: 1903.943726
train Loss: 251.3634 batch_loss: 539.638367
train Loss: 252.2821 batch_loss: 705.589478
train Loss: 252.9507 batch_loss: 513.525208
train Loss: 254.3966 batch_loss: 1110.378296
train Loss: 256.2009 batch_loss: 1385.727661
train Loss: 258.4415 batch_loss: 1720.777344
train Loss: 258.9084 batch_loss: 358.615082
train Loss: 259.8430 batch_loss: 717.774963
train Loss: 261.5082 batch_loss: 1278.887573
train Loss: 263.4489 batch_loss: 1490.448120
train Loss: 264.4509 batch_loss: 769.483459
train Loss: 264.9280 batch_loss: 366.403412
train Loss: 265.9623 batch_loss: 794.378296
train Loss: 267.2693 batch_loss: 1003.758118
train Loss: 268.9857 batch_loss: 1318.197510
train Loss: 270.0002 batch_loss: 779.123169
train Loss: 270.9863 batch_loss: 757.346252
train Loss: 271.9249 batch_loss: 720.832153
train Loss: 273.4124 batch_loss: 1142.429199
train Loss: 274.8744 batch_loss: 1122.819458
train Loss: 276.2580 batch_loss: 1062.606934
train Loss: 276.7366 batch_loss: 367.522064
train Loss: 277.3525 batch_loss: 473.016876
train Loss: 279.1170 batch_loss: 1355.181885
train Loss: 281.5076 batch_loss: 1835.968872
train Loss: 282.6828 batch_loss: 902.504028
train Loss: 283.8022 batch_loss: 859.733276
train Loss: 285.4639 batch_loss: 1276.156860
train Loss: 285.6403 batch_loss: 135.511246
train Loss: 286.6092 batch_loss: 744.106140
train Loss: 287.7551 batch_loss: 880.054932
train Loss: 289.3430 batch_loss: 1219.511475
train Loss: 290.5117 batch_loss: 897.517639
train Loss: 291.1814 batch_loss: 514.403015
train Loss: 291.5939 batch_loss: 316.761017
train Loss: 292.7112 batch_loss: 858.095764
train Loss: 294.1547 batch_loss: 1108.628662
train Loss: 295.7789 batch_loss: 1247.346069
train Loss: 296.4696 batch_loss: 530.446472
train Loss: 296.8464 batch_loss: 289.412018
train Loss: 297.9403 batch_loss: 840.137390
train Loss: 298.6571 batch_loss: 550.503540
train Loss: 299.7091 batch_loss: 807.896790
train Loss: 300.0496 batch_loss: 261.516205
train Loss: 300.8160 batch_loss: 588.563354
train Loss: 301.5624 batch_loss: 573.232422
train Loss: 302.8681 batch_loss: 1002.796875
train Loss: 304.4190 batch_loss: 1191.076782
train Loss: 305.9917 batch_loss: 1207.873901
train Loss: 307.3381 batch_loss: 1034.017090
train Loss: 308.3725 batch_loss: 794.459290
train Loss: 309.8232 batch_loss: 1114.085449
train Loss: 310.2133 batch_loss: 299.585388
train Loss: 311.1999 batch_loss: 757.733826
train Loss: 312.3284 batch_loss: 866.662354
train Loss: 313.9677 batch_loss: 1259.012207
train Loss: 315.3017 batch_loss: 1024.487671
train Loss: 315.8110 batch_loss: 391.173676
train Loss: 316.7139 batch_loss: 693.458618
train Loss: 318.2305 batch_loss: 1164.692139
train Loss: 318.6936 batch_loss: 355.687897
train Loss: 319.4814 batch_loss: 605.042419
train Loss: 319.9342 batch_loss: 347.717957
train Loss: 320.6709 batch_loss: 565.777466
train Loss: 321.6567 batch_loss: 757.129700
train Loss: 323.0046 batch_loss: 1035.147827
train Loss: 323.8970 batch_loss: 685.386047
train Loss: 324.3732 batch_loss: 365.737579
train Loss: 325.9512 batch_loss: 1211.877075
train Loss: 326.9356 batch_loss: 756.036499
train Loss: 327.8293 batch_loss: 686.343323
train Loss: 328.9125 batch_loss: 831.886292
train Loss: 330.1451 batch_loss: 946.679565
train Loss: 330.5674 batch_loss: 324.339508
train Loss: 331.4443 batch_loss: 673.412415
train Loss: 331.8023 batch_loss: 274.922302
train Loss: 332.1068 batch_loss: 233.911194
train Loss: 333.8482 batch_loss: 1337.367676
train Loss: 334.6090 batch_loss: 584.322510
train Loss: 335.5058 batch_loss: 688.706238
train Loss: 336.5504 batch_loss: 802.284790
train Loss: 337.6264 batch_loss: 826.383850
train Loss: 338.9428 batch_loss: 1010.927124
train Loss: 339.5209 batch_loss: 443.990234
train Loss: 340.4774 batch_loss: 734.607666
train Loss: 341.2075 batch_loss: 560.732239
train Loss: 342.0144 batch_loss: 619.703674
Loss on the test images: 226.09256 
Epoch 2/199
----------
trainloader ready!
testloader ready!
train Loss: 0.5693 batch_loss: 437.201477
train Loss: 1.5341 batch_loss: 740.976929
train Loss: 2.3974 batch_loss: 663.016174
train Loss: 3.5360 batch_loss: 874.451172
train Loss: 4.2345 batch_loss: 536.461487
train Loss: 4.9954 batch_loss: 584.382446
train Loss: 5.7804 batch_loss: 602.839722
train Loss: 6.8429 batch_loss: 816.052063
train Loss: 8.0725 batch_loss: 944.314453
train Loss: 8.7817 batch_loss: 544.686218
train Loss: 9.5042 batch_loss: 554.855896
train Loss: 10.3328 batch_loss: 636.352478
train Loss: 10.8977 batch_loss: 433.827423
train Loss: 11.6462 batch_loss: 574.855652
train Loss: 12.9708 batch_loss: 1017.280334
train Loss: 13.9839 batch_loss: 778.112915
train Loss: 14.3610 batch_loss: 289.612823
train Loss: 15.1164 batch_loss: 580.147278
train Loss: 16.3014 batch_loss: 910.069397
train Loss: 17.3554 batch_loss: 809.455872
train Loss: 17.8312 batch_loss: 365.442230
train Loss: 18.3826 batch_loss: 423.464905
train Loss: 18.8438 batch_loss: 354.174469
train Loss: 19.8140 batch_loss: 745.101257
train Loss: 21.3636 batch_loss: 1190.137573
train Loss: 21.8661 batch_loss: 385.903503
train Loss: 22.5502 batch_loss: 525.394958
train Loss: 23.6259 batch_loss: 826.099731
train Loss: 24.3315 batch_loss: 541.958191
train Loss: 24.9811 batch_loss: 498.861420
train Loss: 25.7123 batch_loss: 561.559509
train Loss: 26.7262 batch_loss: 778.688293
train Loss: 27.7252 batch_loss: 767.250244
train Loss: 28.2911 batch_loss: 434.588715
train Loss: 29.5604 batch_loss: 974.804443
train Loss: 30.2210 batch_loss: 507.338806
train Loss: 30.6256 batch_loss: 310.774292
train Loss: 31.0248 batch_loss: 306.579285
train Loss: 31.7277 batch_loss: 539.828491
train Loss: 32.3237 batch_loss: 457.706940
train Loss: 33.4078 batch_loss: 832.572998
train Loss: 34.4119 batch_loss: 771.180725
train Loss: 35.2265 batch_loss: 625.565735
train Loss: 36.1651 batch_loss: 720.842896
train Loss: 36.4463 batch_loss: 215.994705
train Loss: 37.1699 batch_loss: 555.689331
train Loss: 38.4609 batch_loss: 991.481750
train Loss: 38.8323 batch_loss: 285.256653
train Loss: 39.1215 batch_loss: 222.099945
train Loss: 39.3357 batch_loss: 164.547867
train Loss: 39.9145 batch_loss: 444.458557
train Loss: 40.6851 batch_loss: 591.835327
train Loss: 41.0199 batch_loss: 257.165283
train Loss: 41.3572 batch_loss: 259.051086
train Loss: 42.1110 batch_loss: 578.893250
train Loss: 42.9322 batch_loss: 630.673706
train Loss: 43.3688 batch_loss: 335.357056
train Loss: 44.1086 batch_loss: 568.130493
train Loss: 44.6490 batch_loss: 415.027069
train Loss: 45.5264 batch_loss: 673.816833
train Loss: 46.0160 batch_loss: 376.020325
train Loss: 47.3518 batch_loss: 1025.945801
train Loss: 48.5392 batch_loss: 911.870972
train Loss: 48.9881 batch_loss: 344.784241
train Loss: 49.4616 batch_loss: 363.674988
train Loss: 50.3252 batch_loss: 663.184937
train Loss: 51.1295 batch_loss: 617.694580
train Loss: 52.4962 batch_loss: 1049.667114
train Loss: 53.1697 batch_loss: 517.257751
train Loss: 53.7662 batch_loss: 458.125641
train Loss: 55.3097 batch_loss: 1185.341919
train Loss: 55.7744 batch_loss: 356.897491
train Loss: 56.5439 batch_loss: 591.028442
train Loss: 57.5190 batch_loss: 748.872437
train Loss: 57.9102 batch_loss: 300.384705
train Loss: 58.3263 batch_loss: 319.592377
train Loss: 59.6274 batch_loss: 999.269287
train Loss: 60.4925 batch_loss: 664.377136
train Loss: 61.1575 batch_loss: 510.718689
train Loss: 62.1760 batch_loss: 782.182312
train Loss: 62.9507 batch_loss: 594.972107
train Loss: 64.0589 batch_loss: 851.149719
train Loss: 64.4419 batch_loss: 294.106384
train Loss: 65.3049 batch_loss: 662.782837
train Loss: 65.7107 batch_loss: 311.636505
train Loss: 66.0905 batch_loss: 291.710114
train Loss: 67.2410 batch_loss: 883.570374
train Loss: 67.9270 batch_loss: 526.853149
train Loss: 69.0998 batch_loss: 900.755066
train Loss: 69.8210 batch_loss: 553.863159
train Loss: 70.4944 batch_loss: 517.154236
train Loss: 71.0580 batch_loss: 432.834442
train Loss: 71.6424 batch_loss: 448.820648
train Loss: 72.4570 batch_loss: 625.621338
train Loss: 73.0862 batch_loss: 483.241119
train Loss: 73.4147 batch_loss: 252.264816
train Loss: 74.4589 batch_loss: 801.942566
train Loss: 75.4944 batch_loss: 795.275085
train Loss: 76.0774 batch_loss: 447.775513
train Loss: 76.9146 batch_loss: 642.974182
train Loss: 77.2725 batch_loss: 274.832855
train Loss: 77.5024 batch_loss: 176.527603
train Loss: 77.9018 batch_loss: 306.790894
train Loss: 78.5570 batch_loss: 503.157867
train Loss: 78.7279 batch_loss: 131.270935
train Loss: 79.3799 batch_loss: 500.743500
train Loss: 79.8099 batch_loss: 330.263092
train Loss: 81.1071 batch_loss: 996.189819
train Loss: 81.6204 batch_loss: 394.273926
train Loss: 82.3620 batch_loss: 569.507446
train Loss: 83.1956 batch_loss: 640.204895
train Loss: 84.1005 batch_loss: 694.993835
train Loss: 84.7646 batch_loss: 510.009186
train Loss: 85.3445 batch_loss: 445.341095
train Loss: 86.0178 batch_loss: 517.132019
train Loss: 86.6593 batch_loss: 492.622437
train Loss: 87.2955 batch_loss: 488.602356
train Loss: 88.3638 batch_loss: 820.469543
train Loss: 89.0505 batch_loss: 527.426819
train Loss: 89.8660 batch_loss: 626.290100
train Loss: 90.3451 batch_loss: 367.937561
train Loss: 90.9305 batch_loss: 449.603699
train Loss: 91.8705 batch_loss: 721.870667
train Loss: 92.5972 batch_loss: 558.135376
train Loss: 93.0314 batch_loss: 333.457489
train Loss: 93.4869 batch_loss: 349.823883
train Loss: 94.1405 batch_loss: 501.979370
train Loss: 94.6541 batch_loss: 394.479706
train Loss: 95.4111 batch_loss: 581.341003
train Loss: 95.6633 batch_loss: 193.690887
train Loss: 96.1621 batch_loss: 383.104309
train Loss: 96.7267 batch_loss: 433.549805
train Loss: 97.5624 batch_loss: 641.883545
train Loss: 98.4908 batch_loss: 712.992981
train Loss: 98.9795 batch_loss: 375.331909
train Loss: 99.4107 batch_loss: 331.169403
train Loss: 99.7390 batch_loss: 252.096024
train Loss: 100.5777 batch_loss: 644.146423
train Loss: 101.0059 batch_loss: 328.836609
train Loss: 101.5627 batch_loss: 427.593079
train Loss: 102.3801 batch_loss: 627.812622
train Loss: 103.0451 batch_loss: 510.706116
train Loss: 103.8193 batch_loss: 594.586609
train Loss: 104.4239 batch_loss: 464.368469
train Loss: 105.3768 batch_loss: 731.810791
train Loss: 105.5380 batch_loss: 123.808380
train Loss: 105.8826 batch_loss: 264.587036
train Loss: 106.8478 batch_loss: 741.321655
train Loss: 107.7708 batch_loss: 708.846313
train Loss: 108.5218 batch_loss: 576.746399
train Loss: 109.0886 batch_loss: 435.344910
train Loss: 109.7639 batch_loss: 518.623596
train Loss: 110.2467 batch_loss: 370.783966
train Loss: 110.7948 batch_loss: 420.965881
train Loss: 111.3889 batch_loss: 456.237732
train Loss: 111.9177 batch_loss: 406.117371
train Loss: 112.5539 batch_loss: 488.595062
train Loss: 113.0619 batch_loss: 390.119263
train Loss: 113.4768 batch_loss: 318.688110
train Loss: 114.1489 batch_loss: 516.155151
train Loss: 115.1363 batch_loss: 758.353394
train Loss: 115.9904 batch_loss: 655.895081
train Loss: 116.6492 batch_loss: 506.027344
train Loss: 116.9202 batch_loss: 208.098801
train Loss: 117.2117 batch_loss: 223.903076
train Loss: 117.6456 batch_loss: 333.216644
train Loss: 118.5320 batch_loss: 680.766602
train Loss: 119.0536 batch_loss: 400.546417
train Loss: 119.3694 batch_loss: 242.529526
train Loss: 119.8016 batch_loss: 331.973633
train Loss: 120.1134 batch_loss: 239.444443
train Loss: 120.6196 batch_loss: 388.743652
train Loss: 121.4429 batch_loss: 632.327454
train Loss: 121.8712 batch_loss: 328.915375
train Loss: 122.3817 batch_loss: 392.061798
train Loss: 122.7894 batch_loss: 313.089783
train Loss: 123.2425 batch_loss: 347.998962
train Loss: 124.0780 batch_loss: 641.689209
train Loss: 124.8449 batch_loss: 588.938049
train Loss: 125.1991 batch_loss: 272.053528
train Loss: 125.6625 batch_loss: 355.871368
train Loss: 126.1864 batch_loss: 402.344788
train Loss: 126.9238 batch_loss: 566.311829
train Loss: 127.6719 batch_loss: 574.577759
train Loss: 128.4727 batch_loss: 614.993835
train Loss: 129.1761 batch_loss: 540.239197
train Loss: 129.7036 batch_loss: 405.126984
train Loss: 130.4849 batch_loss: 600.035217
train Loss: 130.9895 batch_loss: 387.514526
train Loss: 131.3543 batch_loss: 280.135925
train Loss: 132.0383 batch_loss: 525.358276
train Loss: 132.8050 batch_loss: 588.843933
train Loss: 133.0296 batch_loss: 172.487823
train Loss: 133.3164 batch_loss: 220.270996
train Loss: 133.8825 batch_loss: 434.717010
train Loss: 134.4228 batch_loss: 415.001129
train Loss: 135.2327 batch_loss: 621.999939
train Loss: 135.8915 batch_loss: 505.899628
train Loss: 136.1486 batch_loss: 197.463043
train Loss: 136.9776 batch_loss: 636.655151
train Loss: 137.7649 batch_loss: 604.691650
train Loss: 138.0759 batch_loss: 238.868515
train Loss: 138.7796 batch_loss: 540.433655
train Loss: 139.1933 batch_loss: 317.665436
train Loss: 139.7758 batch_loss: 447.423706
train Loss: 140.4610 batch_loss: 526.189392
train Loss: 140.9657 batch_loss: 387.610077
train Loss: 141.5476 batch_loss: 446.922882
train Loss: 142.1061 batch_loss: 428.910431
train Loss: 142.5268 batch_loss: 323.119293
train Loss: 143.4976 batch_loss: 745.537659
train Loss: 144.4414 batch_loss: 724.891418
train Loss: 144.8018 batch_loss: 276.737335
train Loss: 145.2039 batch_loss: 308.793427
train Loss: 145.5755 batch_loss: 285.421509
train Loss: 145.9559 batch_loss: 292.116669
train Loss: 146.5310 batch_loss: 441.676300
train Loss: 146.9671 batch_loss: 334.980438
train Loss: 147.4241 batch_loss: 350.985413
train Loss: 148.1970 batch_loss: 593.563599
train Loss: 148.7981 batch_loss: 461.672577
train Loss: 149.1996 batch_loss: 308.351135
train Loss: 149.7261 batch_loss: 404.299103
train Loss: 150.1613 batch_loss: 334.262390
train Loss: 150.4190 batch_loss: 197.878021
train Loss: 150.9901 batch_loss: 438.651550
train Loss: 151.6711 batch_loss: 522.998779
train Loss: 152.1829 batch_loss: 393.028351
train Loss: 152.7760 batch_loss: 455.496185
train Loss: 153.1038 batch_loss: 251.794235
train Loss: 153.7925 batch_loss: 528.922729
train Loss: 154.4195 batch_loss: 481.484497
train Loss: 155.0077 batch_loss: 451.795868
train Loss: 155.5849 batch_loss: 443.235657
train Loss: 156.1896 batch_loss: 464.455719
train Loss: 156.5698 batch_loss: 292.012634
train Loss: 157.1971 batch_loss: 481.752136
train Loss: 157.6319 batch_loss: 333.932617
train Loss: 158.0122 batc