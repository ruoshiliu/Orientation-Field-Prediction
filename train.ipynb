{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import PIL\n",
    "import torch, torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from ansim_dataset import ansimDataset, create_circular_mask\n",
    "# from convolution_lstm import encoderConvLSTM, decoderConvLSTM\n",
    "from ConvLSTM import ConvLSTM\n",
    "import random\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/rliu/ansim/data/data/JPEGImages/'\n",
    "img_list_csv = '/home/rliu/github/ansim/img_list.csv'\n",
    "train_csv = '/home/rliu/github/ansim/train.csv'\n",
    "test_csv = '/home/rliu/github/ansim/test.csv'\n",
    "output_path = '/home/rliu/ansim/models/very_first.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = create_circular_mask(128,128)\n",
    "trainset = ansimDataset(img_list_csv = img_list_csv, seq_csv = train_csv, root_dir = img_path, step=20, random_rotate = True, mask = mask, transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                             batch_size=8, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "\n",
    "testset = ansimDataset(img_list_csv = img_list_csv, seq_csv = test_csv, root_dir = img_path, step=20, random_rotate = True, mask = mask, transform=None)\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                             batch_size=8, shuffle=True,\n",
    "                                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU in use\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"GPU in use\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(encoder, decoder, criterion, optimizer, scheduler, num_epochs=25):\n",
    "#     since = time.time()\n",
    "\n",
    "#     best_encoder_wts = encoder.state_dict()\n",
    "#     best_decoder_wts = decoder.state_dict()\n",
    "#     best_acc = 0.0\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "#         print('-' * 10)\n",
    "\n",
    "#         # Each epoch has a training phase\n",
    "#         scheduler.step()\n",
    "#         encoder.train(True)  # Set model to training mode\n",
    "#         decoder.train(True)  # Set model to training mode\n",
    "#         running_loss = 0.0\n",
    "#         running_corrects = 0\n",
    "\n",
    "#         # Iterate over data.\n",
    "#         trainset = ansimDataset(img_list_csv = img_list_csv, seq_csv = train_csv, root_dir = img_path, step=20, random_rotate = True, mask = mask, transform=None)\n",
    "#         trainloader = torch.utils.data.DataLoader(trainset,\n",
    "#                                                      batch_size=1, shuffle=True,\n",
    "#                                                      num_workers=4)\n",
    "\n",
    "#         print(\"trainloader ready!\")\n",
    "#         testset = ansimDataset(img_list_csv = img_list_csv, seq_csv = test_csv, root_dir = img_path, step=20, random_rotate = True, mask = mask, transform=None)\n",
    "#         testloader = torch.utils.data.DataLoader(testset,\n",
    "#                                                      batch_size=1, shuffle=True,\n",
    "#                                                      num_workers=4)\n",
    "#         print(\"testloader ready!\")\n",
    "        \n",
    "#         for data in trainloader:\n",
    "#             # get the inputs\n",
    "#             data_split = torch.split(data, 10, dim=1)\n",
    "#             inputs = data_split[0]\n",
    "#             target = data_split[1]\n",
    "# #            print(inputs)\n",
    "#             # wrap them in Variable\n",
    "#             if use_gpu:\n",
    "# #                inputs = Variable(inputs.cuda())\n",
    "# #                labels = Variable(labels.cuda())\n",
    "# #                inputs = torch.nn.DataParallel(inputs, device_ids=[0, 1]).cuda()\n",
    "# #                labels = torch.nn.DataParallel(labels, device_ids=[0, 1]).cuda()\n",
    "#                 inputs, target = inputs.to(device), target.to(device)\n",
    "# #                print(inputs)\n",
    "#             else:\n",
    "#                 inputs, target = Variable(inputs), Variable(target)\n",
    "\n",
    "#             # zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             output, h, c, states = encoder(inputs)\n",
    "#             output_last = output[0][0].double()\n",
    "#             h1,h2,h3,h4,h5 = states[0][0],states[1][0],states[2][0],states[3][0],states[4][0]\n",
    "#             c1,c2,c3,c4,c5 = states[0][1],states[1][1],states[2][1],states[3][1],states[4][1]\n",
    "#             states_cat = torch.cat((h1,h2,h3,h4,h5,c1,c2,c3,c4,c5), dim=1, out=None)\n",
    "#             x = decoder.activateConv(states_cat)\n",
    "#             input_d = [x, states]\n",
    "#             output_d, h_d, c_d, states_d = decoder(input_d)\n",
    "#             predicted = torch.cat(output_d, dim=0, out=None).double()\n",
    "            \n",
    "            \n",
    "#             loss = criterion(predicted, target)\n",
    "#             # forward\n",
    "# #            outputs = model(inputs)\n",
    "# #            _, preds = torch.max(outputs.data, 1)\n",
    "# #            loss = criterion(outputs, labels)\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # statistics\n",
    "#             iter_loss = loss.item()\n",
    "#             running_loss += loss.item()    \n",
    "#             epoch_loss = running_loss / len(trainset)\n",
    "            \n",
    "#             print('{} Loss: {:.4f} batch_loss: {:d}'.format(\n",
    "#                 \"train\", epoch_loss, iter_loss))\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for data in testloader:\n",
    "#                 data_split = torch.split(data, 10, dim=1)\n",
    "#                 inputs = data_split[0]\n",
    "#                 target = data_split[1]\n",
    "                \n",
    "#                 if use_gpu:\n",
    "#                     inputs, target = inputs.to(device), target.to(device)\n",
    "#                 else:\n",
    "#                     inputs, target = Variable(inputs), Variable(target)\n",
    "\n",
    "                    \n",
    "#                 output, h, c, states = encoder(inputs)\n",
    "#                 output_last = output[0][0].double()\n",
    "#                 h1,h2,h3,h4,h5 = states[0][0],states[1][0],states[2][0],states[3][0],states[4][0]\n",
    "#                 c1,c2,c3,c4,c5 = states[0][1],states[1][1],states[2][1],states[3][1],states[4][1]\n",
    "#                 states_cat = torch.cat((h1,h2,h3,h4,h5,c1,c2,c3,c4,c5), dim=1, out=None)\n",
    "#                 x = decoder.activateConv(states_cat)\n",
    "#                 input_d = [x, states]\n",
    "#                 output_d, h_d, c_d, states_d = decoder(input_d)\n",
    "#                 predicted = torch.cat(output_d, dim=0, out=None).double()\n",
    "            \n",
    "#                 loss_test = criterion(predicted, target)\n",
    "#                 iter_loss_test = loss_test.item()\n",
    "#                 running_loss_test += loss_test.item()    \n",
    "#                 epoch_loss_test = running_loss_test / len(testset)\n",
    "\n",
    "#         print('Loss on the test images: %.5f %%' % (\n",
    "#             epoch_loss_test))\n",
    "        \n",
    "\n",
    "#     time_elapsed = time.time() - since\n",
    "#     print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "#         time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "#     # load best model weights\n",
    "#     encoder.load_state_dict(best_encoder_wts)\n",
    "#     decoder.load_state_dict(best_decoder_wts)\n",
    "#     return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, batch_size = 4, step_size = 20):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training phase\n",
    "        scheduler.step()\n",
    "        model.train(True)  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "\n",
    "        # Iterate over data.\n",
    "        trainset = ansimDataset(img_list_csv = img_list_csv, seq_csv = train_csv, root_dir = img_path, step=step_size, random_rotate = True, mask = mask, transform=None)\n",
    "        trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                                     batch_size=batch_size, shuffle=True,\n",
    "                                                     num_workers=4)\n",
    "\n",
    "        print(\"trainloader ready!\")\n",
    "        testset = ansimDataset(img_list_csv = img_list_csv, seq_csv = test_csv, root_dir = img_path, step=step_size, random_rotate = True, mask = mask, transform=None)\n",
    "        testloader = torch.utils.data.DataLoader(testset,\n",
    "                                                     batch_size=batch_size, shuffle=True,\n",
    "                                                     num_workers=4)\n",
    "        print(\"testloader ready!\")\n",
    "        \n",
    "        for data in trainloader:\n",
    "            # get the inputs\n",
    "            data_split = torch.split(data, int(data.shape[1]/2), dim=1)\n",
    "            inputs = data_split[0]\n",
    "            target = data_split[1]\n",
    "#            print(inputs)\n",
    "            # wrap them in Variable\n",
    "            if use_gpu:\n",
    "                inputs, target = inputs.to(device), target.to(device)\n",
    "            else:\n",
    "                inputs, target = Variable(inputs), Variable(target)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "#             output, h, c, states = encoder(inputs)\n",
    "#             output_last = output[0][0].double()\n",
    "#             h1,h2,h3,h4,h5 = states[0][0],states[1][0],states[2][0],states[3][0],states[4][0]\n",
    "#             c1,c2,c3,c4,c5 = states[0][1],states[1][1],states[2][1],states[3][1],states[4][1]\n",
    "#             states_cat = torch.cat((h1,h2,h3,h4,h5,c1,c2,c3,c4,c5), dim=1, out=None)\n",
    "#             x = decoder.activateConv(states_cat)\n",
    "#             input_d = [x, states]\n",
    "#             output_d, h_d, c_d, states_d = decoder(input_d)\n",
    "#             predicted = torch.cat(output_d, dim=0, out=None).double()\n",
    "            \n",
    "            _, _, predicted = model(inputs)\n",
    "            \n",
    "            m = nn.Sigmoid()\n",
    "            loss = criterion(m(predicted), m(target))\n",
    "            # forward\n",
    "#            outputs = model(inputs)\n",
    "#            _, preds = torch.max(outputs.data, 1)\n",
    "#            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            iter_loss = loss.item()\n",
    "            running_loss += loss.item()    \n",
    "            epoch_loss = running_loss / len(trainset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} batch_loss: {:f}'.format(\n",
    "                \"train\", epoch_loss, iter_loss))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_loss_test = 0.0\n",
    "            for data in testloader:\n",
    "                data_split = torch.split(data, int(data.shape[1]/2), dim=1)\n",
    "                inputs = data_split[0]\n",
    "                target = data_split[1]\n",
    "                \n",
    "                \n",
    "                if use_gpu:\n",
    "                    inputs, target = inputs.to(device), target.to(device)\n",
    "                else:\n",
    "                    inputs, target = Variable(inputs), Variable(target)\n",
    "\n",
    "                    \n",
    "#                 output, h, c, states = encoder(inputs)\n",
    "#                 output_last = output[0][0].double()\n",
    "#                 h1,h2,h3,h4,h5 = states[0][0],states[1][0],states[2][0],states[3][0],states[4][0]\n",
    "#                 c1,c2,c3,c4,c5 = states[0][1],states[1][1],states[2][1],states[3][1],states[4][1]\n",
    "#                 states_cat = torch.cat((h1,h2,h3,h4,h5,c1,c2,c3,c4,c5), dim=1, out=None)\n",
    "#                 x = decoder.activateConv(states_cat)\n",
    "#                 input_d = [x, states]\n",
    "#                 output_d, h_d, c_d, states_d = decoder(input_d)\n",
    "#                 predicted = torch.cat(output_d, dim=0, out=None).double()\n",
    "\n",
    "                _, _, predicted = model(inputs)\n",
    "                \n",
    "                m = nn.Sigmoid()\n",
    "                loss_test = criterion(m(predicted), m(target))\n",
    "                \n",
    "                iter_loss_test = loss_test.item()\n",
    "                running_loss_test += loss_test.item()    \n",
    "                epoch_loss_test = running_loss_test / len(testset)\n",
    "\n",
    "        print('Loss on the test images: %.5f ' % (\n",
    "            epoch_loss_test))\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "trainloader ready!\n",
      "testloader ready!\n",
      "train Loss: 0.0009 batch_loss: 0.696245\n",
      "train Loss: 0.0016 batch_loss: 0.552550\n",
      "train Loss: 0.0019 batch_loss: 0.247794\n",
      "train Loss: 0.0022 batch_loss: 0.212266\n",
      "train Loss: 0.0026 batch_loss: 0.259702\n",
      "train Loss: 0.0029 batch_loss: 0.253041\n",
      "train Loss: 0.0032 batch_loss: 0.230204\n",
      "train Loss: 0.0035 batch_loss: 0.216968\n",
      "train Loss: 0.0037 batch_loss: 0.189752\n",
      "train Loss: 0.0040 batch_loss: 0.188197\n",
      "train Loss: 0.0042 batch_loss: 0.189403\n",
      "train Loss: 0.0044 batch_loss: 0.179107\n",
      "train Loss: 0.0047 batch_loss: 0.206736\n",
      "train Loss: 0.0049 batch_loss: 0.173229\n",
      "train Loss: 0.0052 batch_loss: 0.170160\n",
      "train Loss: 0.0054 batch_loss: 0.164219\n",
      "train Loss: 0.0056 batch_loss: 0.162365\n",
      "train Loss: 0.0058 batch_loss: 0.162835\n",
      "train Loss: 0.0060 batch_loss: 0.174340\n",
      "train Loss: 0.0062 batch_loss: 0.162468\n",
      "train Loss: 0.0065 batch_loss: 0.163850\n",
      "train Loss: 0.0067 batch_loss: 0.158713\n",
      "train Loss: 0.0069 batch_loss: 0.163574\n",
      "train Loss: 0.0071 batch_loss: 0.160104\n",
      "train Loss: 0.0073 batch_loss: 0.163788\n",
      "train Loss: 0.0075 batch_loss: 0.173304\n",
      "train Loss: 0.0077 batch_loss: 0.169957\n",
      "train Loss: 0.0079 batch_loss: 0.157871\n",
      "train Loss: 0.0082 batch_loss: 0.159711\n",
      "train Loss: 0.0084 batch_loss: 0.157085\n",
      "train Loss: 0.0086 batch_loss: 0.158714\n",
      "train Loss: 0.0088 batch_loss: 0.161289\n",
      "train Loss: 0.0090 batch_loss: 0.162339\n",
      "train Loss: 0.0092 batch_loss: 0.158571\n",
      "train Loss: 0.0094 batch_loss: 0.158927\n",
      "train Loss: 0.0096 batch_loss: 0.154362\n",
      "train Loss: 0.0098 batch_loss: 0.160008\n",
      "train Loss: 0.0100 batch_loss: 0.161551\n",
      "train Loss: 0.0102 batch_loss: 0.155248\n",
      "train Loss: 0.0104 batch_loss: 0.155643\n",
      "train Loss: 0.0106 batch_loss: 0.156769\n",
      "train Loss: 0.0108 batch_loss: 0.155546\n",
      "train Loss: 0.0110 batch_loss: 0.154605\n",
      "train Loss: 0.0112 batch_loss: 0.154226\n",
      "train Loss: 0.0114 batch_loss: 0.153840\n",
      "train Loss: 0.0116 batch_loss: 0.154695\n",
      "train Loss: 0.0118 batch_loss: 0.153818\n",
      "train Loss: 0.0120 batch_loss: 0.159057\n",
      "train Loss: 0.0122 batch_loss: 0.154762\n",
      "train Loss: 0.0124 batch_loss: 0.154970\n",
      "train Loss: 0.0126 batch_loss: 0.153679\n",
      "train Loss: 0.0128 batch_loss: 0.153450\n",
      "train Loss: 0.0130 batch_loss: 0.153172\n",
      "train Loss: 0.0132 batch_loss: 0.153772\n",
      "train Loss: 0.0134 batch_loss: 0.153717\n",
      "train Loss: 0.0136 batch_loss: 0.153427\n",
      "train Loss: 0.0139 batch_loss: 0.157179\n",
      "train Loss: 0.0140 batch_loss: 0.153108\n",
      "train Loss: 0.0142 batch_loss: 0.152598\n",
      "train Loss: 0.0144 batch_loss: 0.153347\n",
      "train Loss: 0.0146 batch_loss: 0.152490\n",
      "train Loss: 0.0149 batch_loss: 0.156721\n",
      "train Loss: 0.0151 batch_loss: 0.154202\n",
      "train Loss: 0.0153 batch_loss: 0.152928\n",
      "train Loss: 0.0155 batch_loss: 0.153497\n",
      "train Loss: 0.0156 batch_loss: 0.152285\n",
      "train Loss: 0.0159 batch_loss: 0.155147\n",
      "train Loss: 0.0160 batch_loss: 0.152316\n",
      "train Loss: 0.0162 batch_loss: 0.152535\n",
      "train Loss: 0.0164 batch_loss: 0.152055\n",
      "train Loss: 0.0166 batch_loss: 0.151823\n",
      "train Loss: 0.0168 batch_loss: 0.151897\n",
      "train Loss: 0.0170 batch_loss: 0.155495\n",
      "train Loss: 0.0172 batch_loss: 0.151314\n",
      "train Loss: 0.0174 batch_loss: 0.155838\n",
      "train Loss: 0.0176 batch_loss: 0.151472\n",
      "train Loss: 0.0178 batch_loss: 0.152036\n",
      "train Loss: 0.0180 batch_loss: 0.152573\n",
      "train Loss: 0.0182 batch_loss: 0.151657\n",
      "train Loss: 0.0184 batch_loss: 0.151815\n",
      "train Loss: 0.0186 batch_loss: 0.151481\n",
      "train Loss: 0.0188 batch_loss: 0.151824\n",
      "train Loss: 0.0190 batch_loss: 0.151662\n",
      "train Loss: 0.0192 batch_loss: 0.151304\n",
      "train Loss: 0.0194 batch_loss: 0.152168\n",
      "train Loss: 0.0196 batch_loss: 0.151646\n",
      "train Loss: 0.0198 batch_loss: 0.151379\n",
      "train Loss: 0.0200 batch_loss: 0.151333\n",
      "train Loss: 0.0202 batch_loss: 0.151818\n",
      "train Loss: 0.0204 batch_loss: 0.151172\n",
      "train Loss: 0.0206 batch_loss: 0.152468\n",
      "train Loss: 0.0208 batch_loss: 0.152189\n",
      "train Loss: 0.0210 batch_loss: 0.150916\n",
      "train Loss: 0.0212 batch_loss: 0.150855\n",
      "train Loss: 0.0214 batch_loss: 0.150772\n",
      "train Loss: 0.0216 batch_loss: 0.156167\n",
      "train Loss: 0.0218 batch_loss: 0.151577\n",
      "train Loss: 0.0220 batch_loss: 0.150700\n",
      "train Loss: 0.0222 batch_loss: 0.151577\n",
      "train Loss: 0.0224 batch_loss: 0.150760\n",
      "train Loss: 0.0226 batch_loss: 0.151645\n",
      "train Loss: 0.0228 batch_loss: 0.150628\n",
      "train Loss: 0.0230 batch_loss: 0.150713\n",
      "train Loss: 0.0232 batch_loss: 0.150835\n",
      "train Loss: 0.0234 batch_loss: 0.150695\n",
      "train Loss: 0.0236 batch_loss: 0.158225\n",
      "train Loss: 0.0238 batch_loss: 0.151133\n",
      "train Loss: 0.0240 batch_loss: 0.150655\n",
      "train Loss: 0.0242 batch_loss: 0.150685\n",
      "train Loss: 0.0244 batch_loss: 0.150956\n",
      "train Loss: 0.0246 batch_loss: 0.150736\n",
      "train Loss: 0.0248 batch_loss: 0.151883\n",
      "train Loss: 0.0249 batch_loss: 0.150600\n",
      "train Loss: 0.0251 batch_loss: 0.150662\n",
      "train Loss: 0.0253 batch_loss: 0.150588\n",
      "train Loss: 0.0255 batch_loss: 0.150503\n",
      "train Loss: 0.0257 batch_loss: 0.150719\n",
      "train Loss: 0.0259 batch_loss: 0.150494\n",
      "train Loss: 0.0261 batch_loss: 0.151619\n",
      "train Loss: 0.0263 batch_loss: 0.157527\n",
      "train Loss: 0.0265 batch_loss: 0.150479\n",
      "train Loss: 0.0267 batch_loss: 0.150527\n",
      "train Loss: 0.0269 batch_loss: 0.156100\n",
      "train Loss: 0.0271 batch_loss: 0.150500\n",
      "train Loss: 0.0273 batch_loss: 0.150461\n",
      "train Loss: 0.0275 batch_loss: 0.151286\n",
      "train Loss: 0.0277 batch_loss: 0.152057\n",
      "train Loss: 0.0279 batch_loss: 0.154539\n",
      "train Loss: 0.0281 batch_loss: 0.150470\n",
      "train Loss: 0.0283 batch_loss: 0.151437\n",
      "train Loss: 0.0285 batch_loss: 0.150421\n",
      "train Loss: 0.0287 batch_loss: 0.150492\n",
      "train Loss: 0.0289 batch_loss: 0.150430\n",
      "train Loss: 0.0291 batch_loss: 0.152245\n",
      "train Loss: 0.0293 batch_loss: 0.150383\n",
      "train Loss: 0.0295 batch_loss: 0.150428\n",
      "train Loss: 0.0297 batch_loss: 0.151989\n",
      "train Loss: 0.0299 batch_loss: 0.151059\n",
      "train Loss: 0.0301 batch_loss: 0.150337\n",
      "train Loss: 0.0303 batch_loss: 0.150307\n",
      "train Loss: 0.0305 batch_loss: 0.150310\n",
      "train Loss: 0.0307 batch_loss: 0.150282\n",
      "train Loss: 0.0309 batch_loss: 0.151004\n",
      "train Loss: 0.0311 batch_loss: 0.151034\n",
      "train Loss: 0.0313 batch_loss: 0.155551\n",
      "train Loss: 0.0315 batch_loss: 0.150612\n",
      "train Loss: 0.0316 batch_loss: 0.150592\n",
      "train Loss: 0.0318 batch_loss: 0.150665\n",
      "train Loss: 0.0320 batch_loss: 0.149950\n",
      "train Loss: 0.0322 batch_loss: 0.149896\n",
      "train Loss: 0.0324 batch_loss: 0.149896\n",
      "train Loss: 0.0326 batch_loss: 0.157267\n",
      "train Loss: 0.0328 batch_loss: 0.150300\n",
      "train Loss: 0.0330 batch_loss: 0.154426\n",
      "train Loss: 0.0332 batch_loss: 0.157462\n",
      "train Loss: 0.0334 batch_loss: 0.152932\n",
      "train Loss: 0.0336 batch_loss: 0.150426\n",
      "train Loss: 0.0338 batch_loss: 0.150498\n",
      "train Loss: 0.0340 batch_loss: 0.150468\n",
      "train Loss: 0.0342 batch_loss: 0.150539\n",
      "train Loss: 0.0344 batch_loss: 0.151631\n",
      "train Loss: 0.0346 batch_loss: 0.150381\n",
      "train Loss: 0.0348 batch_loss: 0.155020\n",
      "train Loss: 0.0350 batch_loss: 0.150386\n",
      "train Loss: 0.0352 batch_loss: 0.150284\n",
      "train Loss: 0.0354 batch_loss: 0.150301\n",
      "train Loss: 0.0356 batch_loss: 0.151105\n",
      "train Loss: 0.0358 batch_loss: 0.150241\n",
      "train Loss: 0.0360 batch_loss: 0.151127\n",
      "train Loss: 0.0362 batch_loss: 0.155030\n",
      "train Loss: 0.0364 batch_loss: 0.150193\n",
      "train Loss: 0.0366 batch_loss: 0.155299\n",
      "train Loss: 0.0368 batch_loss: 0.151184\n",
      "train Loss: 0.0370 batch_loss: 0.150271\n",
      "train Loss: 0.0372 batch_loss: 0.155053\n",
      "train Loss: 0.0374 batch_loss: 0.155562\n",
      "train Loss: 0.0376 batch_loss: 0.152138\n",
      "train Loss: 0.0378 batch_loss: 0.150486\n",
      "train Loss: 0.0380 batch_loss: 0.150605\n",
      "train Loss: 0.0382 batch_loss: 0.150698\n",
      "train Loss: 0.0384 batch_loss: 0.150382\n",
      "train Loss: 0.0386 batch_loss: 0.150301\n",
      "train Loss: 0.0388 batch_loss: 0.150188\n",
      "train Loss: 0.0390 batch_loss: 0.150208\n",
      "train Loss: 0.0392 batch_loss: 0.151235\n",
      "train Loss: 0.0394 batch_loss: 0.158265\n",
      "train Loss: 0.0396 batch_loss: 0.150303\n",
      "train Loss: 0.0398 batch_loss: 0.150249\n",
      "train Loss: 0.0399 batch_loss: 0.150805\n",
      "train Loss: 0.0401 batch_loss: 0.150581\n",
      "train Loss: 0.0403 batch_loss: 0.151296\n",
      "train Loss: 0.0405 batch_loss: 0.150469\n",
      "train Loss: 0.0407 batch_loss: 0.153760\n",
      "train Loss: 0.0409 batch_loss: 0.151196\n",
      "train Loss: 0.0411 batch_loss: 0.150265\n",
      "train Loss: 0.0413 batch_loss: 0.150253\n",
      "train Loss: 0.0415 batch_loss: 0.150173\n",
      "train Loss: 0.0417 batch_loss: 0.150192\n",
      "train Loss: 0.0419 batch_loss: 0.150187\n",
      "train Loss: 0.0421 batch_loss: 0.153234\n",
      "train Loss: 0.0423 batch_loss: 0.150289\n",
      "train Loss: 0.0425 batch_loss: 0.151608\n",
      "train Loss: 0.0427 batch_loss: 0.150264\n",
      "train Loss: 0.0429 batch_loss: 0.151472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0431 batch_loss: 0.150875\n",
      "train Loss: 0.0433 batch_loss: 0.150282\n",
      "train Loss: 0.0435 batch_loss: 0.150977\n",
      "train Loss: 0.0437 batch_loss: 0.150298\n",
      "train Loss: 0.0439 batch_loss: 0.151443\n",
      "train Loss: 0.0441 batch_loss: 0.153189\n",
      "train Loss: 0.0443 batch_loss: 0.151163\n",
      "train Loss: 0.0445 batch_loss: 0.150218\n",
      "train Loss: 0.0447 batch_loss: 0.156702\n",
      "train Loss: 0.0449 batch_loss: 0.150374\n",
      "train Loss: 0.0451 batch_loss: 0.150142\n",
      "train Loss: 0.0453 batch_loss: 0.150222\n",
      "train Loss: 0.0455 batch_loss: 0.150464\n",
      "train Loss: 0.0457 batch_loss: 0.150627\n",
      "train Loss: 0.0458 batch_loss: 0.150109\n",
      "train Loss: 0.0460 batch_loss: 0.150363\n",
      "train Loss: 0.0462 batch_loss: 0.149866\n",
      "train Loss: 0.0464 batch_loss: 0.150321\n",
      "train Loss: 0.0466 batch_loss: 0.151026\n",
      "train Loss: 0.0468 batch_loss: 0.149754\n",
      "train Loss: 0.0470 batch_loss: 0.150032\n",
      "train Loss: 0.0472 batch_loss: 0.151566\n",
      "train Loss: 0.0474 batch_loss: 0.150951\n",
      "train Loss: 0.0476 batch_loss: 0.151447\n",
      "train Loss: 0.0478 batch_loss: 0.156310\n",
      "train Loss: 0.0480 batch_loss: 0.150252\n",
      "train Loss: 0.0482 batch_loss: 0.151220\n",
      "train Loss: 0.0484 batch_loss: 0.151018\n",
      "train Loss: 0.0486 batch_loss: 0.150275\n",
      "train Loss: 0.0488 batch_loss: 0.153883\n",
      "train Loss: 0.0490 batch_loss: 0.150206\n",
      "train Loss: 0.0492 batch_loss: 0.150374\n",
      "train Loss: 0.0494 batch_loss: 0.150657\n",
      "train Loss: 0.0496 batch_loss: 0.153935\n",
      "train Loss: 0.0498 batch_loss: 0.150154\n",
      "train Loss: 0.0500 batch_loss: 0.150223\n",
      "train Loss: 0.0502 batch_loss: 0.150463\n",
      "train Loss: 0.0504 batch_loss: 0.150276\n",
      "train Loss: 0.0506 batch_loss: 0.150226\n",
      "train Loss: 0.0508 batch_loss: 0.150307\n",
      "train Loss: 0.0510 batch_loss: 0.150231\n",
      "train Loss: 0.0512 batch_loss: 0.151754\n",
      "train Loss: 0.0514 batch_loss: 0.150812\n",
      "train Loss: 0.0516 batch_loss: 0.150144\n",
      "train Loss: 0.0517 batch_loss: 0.150225\n",
      "train Loss: 0.0519 batch_loss: 0.150201\n",
      "train Loss: 0.0521 batch_loss: 0.150429\n",
      "train Loss: 0.0523 batch_loss: 0.150139\n",
      "train Loss: 0.0525 batch_loss: 0.156236\n",
      "train Loss: 0.0527 batch_loss: 0.151032\n",
      "train Loss: 0.0529 batch_loss: 0.151329\n",
      "train Loss: 0.0531 batch_loss: 0.151378\n",
      "train Loss: 0.0533 batch_loss: 0.151576\n",
      "train Loss: 0.0535 batch_loss: 0.150990\n",
      "train Loss: 0.0537 batch_loss: 0.150170\n",
      "train Loss: 0.0539 batch_loss: 0.150185\n",
      "train Loss: 0.0541 batch_loss: 0.150357\n",
      "train Loss: 0.0543 batch_loss: 0.150717\n",
      "train Loss: 0.0545 batch_loss: 0.150090\n",
      "train Loss: 0.0547 batch_loss: 0.150095\n",
      "train Loss: 0.0549 batch_loss: 0.150155\n",
      "train Loss: 0.0551 batch_loss: 0.150369\n",
      "train Loss: 0.0553 batch_loss: 0.151302\n",
      "train Loss: 0.0555 batch_loss: 0.151226\n",
      "train Loss: 0.0557 batch_loss: 0.150729\n",
      "train Loss: 0.0559 batch_loss: 0.150913\n",
      "train Loss: 0.0561 batch_loss: 0.152769\n",
      "train Loss: 0.0563 batch_loss: 0.150436\n",
      "train Loss: 0.0565 batch_loss: 0.150160\n",
      "train Loss: 0.0567 batch_loss: 0.151631\n",
      "train Loss: 0.0569 batch_loss: 0.150160\n",
      "train Loss: 0.0571 batch_loss: 0.151130\n",
      "train Loss: 0.0573 batch_loss: 0.156536\n",
      "train Loss: 0.0575 batch_loss: 0.154008\n",
      "train Loss: 0.0577 batch_loss: 0.151038\n",
      "train Loss: 0.0579 batch_loss: 0.151680\n",
      "train Loss: 0.0580 batch_loss: 0.150533\n",
      "train Loss: 0.0582 batch_loss: 0.150437\n",
      "train Loss: 0.0584 batch_loss: 0.150508\n",
      "train Loss: 0.0586 batch_loss: 0.150234\n",
      "train Loss: 0.0588 batch_loss: 0.151818\n",
      "train Loss: 0.0590 batch_loss: 0.150155\n",
      "train Loss: 0.0592 batch_loss: 0.154486\n",
      "train Loss: 0.0594 batch_loss: 0.150133\n",
      "train Loss: 0.0596 batch_loss: 0.150510\n",
      "train Loss: 0.0598 batch_loss: 0.151200\n",
      "train Loss: 0.0600 batch_loss: 0.150142\n",
      "train Loss: 0.0602 batch_loss: 0.150085\n",
      "train Loss: 0.0604 batch_loss: 0.150049\n",
      "train Loss: 0.0606 batch_loss: 0.150028\n",
      "train Loss: 0.0608 batch_loss: 0.150160\n",
      "train Loss: 0.0610 batch_loss: 0.150014\n",
      "train Loss: 0.0612 batch_loss: 0.151157\n",
      "train Loss: 0.0614 batch_loss: 0.150176\n",
      "train Loss: 0.0616 batch_loss: 0.150146\n",
      "train Loss: 0.0618 batch_loss: 0.151414\n",
      "train Loss: 0.0620 batch_loss: 0.150149\n",
      "train Loss: 0.0622 batch_loss: 0.154306\n",
      "train Loss: 0.0624 batch_loss: 0.150104\n",
      "train Loss: 0.0626 batch_loss: 0.150237\n",
      "train Loss: 0.0628 batch_loss: 0.150204\n",
      "train Loss: 0.0630 batch_loss: 0.151954\n",
      "train Loss: 0.0632 batch_loss: 0.152712\n",
      "train Loss: 0.0634 batch_loss: 0.150088\n",
      "train Loss: 0.0635 batch_loss: 0.150083\n",
      "train Loss: 0.0637 batch_loss: 0.151091\n",
      "train Loss: 0.0639 batch_loss: 0.150085\n",
      "train Loss: 0.0641 batch_loss: 0.153869\n",
      "train Loss: 0.0643 batch_loss: 0.155217\n",
      "train Loss: 0.0645 batch_loss: 0.149968\n",
      "train Loss: 0.0647 batch_loss: 0.149986\n",
      "train Loss: 0.0649 batch_loss: 0.149984\n",
      "train Loss: 0.0651 batch_loss: 0.151126\n",
      "train Loss: 0.0653 batch_loss: 0.150345\n",
      "train Loss: 0.0655 batch_loss: 0.150902\n",
      "train Loss: 0.0657 batch_loss: 0.150449\n",
      "train Loss: 0.0659 batch_loss: 0.150720\n",
      "train Loss: 0.0661 batch_loss: 0.150252\n",
      "train Loss: 0.0663 batch_loss: 0.150154\n",
      "train Loss: 0.0665 batch_loss: 0.150200\n",
      "train Loss: 0.0667 batch_loss: 0.150194\n",
      "train Loss: 0.0669 batch_loss: 0.154643\n",
      "train Loss: 0.0671 batch_loss: 0.150243\n",
      "train Loss: 0.0673 batch_loss: 0.156162\n",
      "train Loss: 0.0675 batch_loss: 0.151004\n",
      "train Loss: 0.0677 batch_loss: 0.152954\n",
      "train Loss: 0.0679 batch_loss: 0.153932\n",
      "train Loss: 0.0681 batch_loss: 0.150066\n",
      "train Loss: 0.0683 batch_loss: 0.150034\n",
      "train Loss: 0.0685 batch_loss: 0.153356\n",
      "train Loss: 0.0687 batch_loss: 0.150034\n",
      "train Loss: 0.0689 batch_loss: 0.154086\n",
      "train Loss: 0.0691 batch_loss: 0.150076\n",
      "train Loss: 0.0693 batch_loss: 0.150167\n",
      "train Loss: 0.0695 batch_loss: 0.150392\n",
      "train Loss: 0.0697 batch_loss: 0.150256\n",
      "train Loss: 0.0699 batch_loss: 0.150707\n",
      "train Loss: 0.0700 batch_loss: 0.150417\n",
      "train Loss: 0.0702 batch_loss: 0.151121\n",
      "train Loss: 0.0704 batch_loss: 0.150766\n",
      "train Loss: 0.0706 batch_loss: 0.150125\n",
      "train Loss: 0.0708 batch_loss: 0.151430\n",
      "train Loss: 0.0710 batch_loss: 0.150092\n",
      "train Loss: 0.0712 batch_loss: 0.150148\n",
      "train Loss: 0.0714 batch_loss: 0.149949\n",
      "train Loss: 0.0716 batch_loss: 0.157055\n",
      "train Loss: 0.0718 batch_loss: 0.155160\n",
      "train Loss: 0.0720 batch_loss: 0.150639\n",
      "train Loss: 0.0722 batch_loss: 0.150390\n",
      "train Loss: 0.0724 batch_loss: 0.150877\n",
      "train Loss: 0.0726 batch_loss: 0.153713\n",
      "train Loss: 0.0728 batch_loss: 0.150189\n",
      "train Loss: 0.0730 batch_loss: 0.151300\n",
      "train Loss: 0.0732 batch_loss: 0.150316\n",
      "train Loss: 0.0734 batch_loss: 0.150147\n",
      "train Loss: 0.0736 batch_loss: 0.150125\n",
      "train Loss: 0.0738 batch_loss: 0.151180\n",
      "train Loss: 0.0740 batch_loss: 0.153757\n",
      "train Loss: 0.0742 batch_loss: 0.150201\n",
      "train Loss: 0.0744 batch_loss: 0.151657\n",
      "train Loss: 0.0746 batch_loss: 0.150135\n",
      "train Loss: 0.0748 batch_loss: 0.152676\n",
      "train Loss: 0.0750 batch_loss: 0.150320\n",
      "train Loss: 0.0752 batch_loss: 0.153455\n",
      "train Loss: 0.0754 batch_loss: 0.151640\n",
      "train Loss: 0.0756 batch_loss: 0.150146\n",
      "train Loss: 0.0758 batch_loss: 0.150150\n",
      "train Loss: 0.0760 batch_loss: 0.150098\n",
      "train Loss: 0.0762 batch_loss: 0.153443\n",
      "train Loss: 0.0764 batch_loss: 0.150077\n",
      "train Loss: 0.0765 batch_loss: 0.150043\n",
      "train Loss: 0.0767 batch_loss: 0.151650\n",
      "train Loss: 0.0769 batch_loss: 0.151243\n",
      "train Loss: 0.0771 batch_loss: 0.150198\n",
      "train Loss: 0.0773 batch_loss: 0.150097\n",
      "train Loss: 0.0775 batch_loss: 0.150274\n",
      "train Loss: 0.0777 batch_loss: 0.149710\n",
      "train Loss: 0.0779 batch_loss: 0.150576\n",
      "train Loss: 0.0781 batch_loss: 0.150255\n",
      "train Loss: 0.0783 batch_loss: 0.149707\n",
      "train Loss: 0.0785 batch_loss: 0.150003\n",
      "train Loss: 0.0787 batch_loss: 0.150199\n",
      "train Loss: 0.0789 batch_loss: 0.149989\n",
      "train Loss: 0.0791 batch_loss: 0.149981\n",
      "train Loss: 0.0793 batch_loss: 0.151764\n",
      "train Loss: 0.0795 batch_loss: 0.149928\n",
      "train Loss: 0.0797 batch_loss: 0.151972\n",
      "train Loss: 0.0799 batch_loss: 0.149927\n",
      "train Loss: 0.0801 batch_loss: 0.155956\n",
      "train Loss: 0.0803 batch_loss: 0.150165\n",
      "train Loss: 0.0805 batch_loss: 0.151001\n",
      "train Loss: 0.0807 batch_loss: 0.154651\n",
      "train Loss: 0.0809 batch_loss: 0.150292\n",
      "train Loss: 0.0811 batch_loss: 0.150123\n",
      "train Loss: 0.0813 batch_loss: 0.151484\n",
      "train Loss: 0.0815 batch_loss: 0.151499\n",
      "train Loss: 0.0817 batch_loss: 0.149848\n",
      "train Loss: 0.0819 batch_loss: 0.149693\n",
      "train Loss: 0.0821 batch_loss: 0.155059\n",
      "train Loss: 0.0822 batch_loss: 0.149793\n",
      "train Loss: 0.0824 batch_loss: 0.150359\n",
      "train Loss: 0.0826 batch_loss: 0.150143\n",
      "train Loss: 0.0828 batch_loss: 0.155451\n",
      "train Loss: 0.0830 batch_loss: 0.154849\n",
      "train Loss: 0.0832 batch_loss: 0.150123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0834 batch_loss: 0.150957\n",
      "train Loss: 0.0836 batch_loss: 0.150202\n",
      "train Loss: 0.0838 batch_loss: 0.151360\n",
      "train Loss: 0.0840 batch_loss: 0.154002\n",
      "train Loss: 0.0842 batch_loss: 0.150977\n",
      "train Loss: 0.0844 batch_loss: 0.150226\n",
      "train Loss: 0.0846 batch_loss: 0.154952\n",
      "train Loss: 0.0848 batch_loss: 0.150037\n",
      "train Loss: 0.0850 batch_loss: 0.150421\n",
      "train Loss: 0.0852 batch_loss: 0.150418\n",
      "train Loss: 0.0854 batch_loss: 0.150104\n",
      "train Loss: 0.0856 batch_loss: 0.150060\n",
      "train Loss: 0.0858 batch_loss: 0.151109\n",
      "train Loss: 0.0860 batch_loss: 0.150110\n",
      "train Loss: 0.0862 batch_loss: 0.149998\n",
      "train Loss: 0.0864 batch_loss: 0.155535\n",
      "train Loss: 0.0866 batch_loss: 0.149973\n",
      "train Loss: 0.0868 batch_loss: 0.150185\n",
      "train Loss: 0.0870 batch_loss: 0.150025\n",
      "train Loss: 0.0872 batch_loss: 0.149984\n",
      "train Loss: 0.0874 batch_loss: 0.150120\n",
      "train Loss: 0.0876 batch_loss: 0.150010\n",
      "train Loss: 0.0878 batch_loss: 0.150162\n",
      "train Loss: 0.0880 batch_loss: 0.150275\n",
      "train Loss: 0.0882 batch_loss: 0.151233\n",
      "train Loss: 0.0883 batch_loss: 0.151571\n",
      "train Loss: 0.0885 batch_loss: 0.154525\n",
      "train Loss: 0.0887 batch_loss: 0.149844\n",
      "train Loss: 0.0889 batch_loss: 0.154087\n",
      "train Loss: 0.0891 batch_loss: 0.150006\n",
      "train Loss: 0.0893 batch_loss: 0.150047\n",
      "train Loss: 0.0895 batch_loss: 0.150007\n",
      "train Loss: 0.0897 batch_loss: 0.153224\n",
      "train Loss: 0.0899 batch_loss: 0.149969\n",
      "train Loss: 0.0901 batch_loss: 0.151258\n",
      "train Loss: 0.0903 batch_loss: 0.150563\n",
      "train Loss: 0.0905 batch_loss: 0.149770\n",
      "train Loss: 0.0907 batch_loss: 0.150811\n",
      "train Loss: 0.0909 batch_loss: 0.151858\n",
      "train Loss: 0.0911 batch_loss: 0.150694\n",
      "train Loss: 0.0913 batch_loss: 0.154092\n",
      "train Loss: 0.0915 batch_loss: 0.150049\n",
      "train Loss: 0.0917 batch_loss: 0.150185\n",
      "train Loss: 0.0919 batch_loss: 0.150151\n",
      "train Loss: 0.0921 batch_loss: 0.149995\n",
      "train Loss: 0.0923 batch_loss: 0.149999\n",
      "train Loss: 0.0925 batch_loss: 0.151235\n",
      "train Loss: 0.0927 batch_loss: 0.150067\n",
      "train Loss: 0.0929 batch_loss: 0.149972\n",
      "train Loss: 0.0931 batch_loss: 0.149969\n",
      "train Loss: 0.0933 batch_loss: 0.154360\n",
      "train Loss: 0.0935 batch_loss: 0.150080\n",
      "train Loss: 0.0937 batch_loss: 0.153127\n",
      "train Loss: 0.0939 batch_loss: 0.150049\n",
      "train Loss: 0.0941 batch_loss: 0.150036\n",
      "train Loss: 0.0942 batch_loss: 0.150049\n",
      "train Loss: 0.0944 batch_loss: 0.153486\n",
      "train Loss: 0.0946 batch_loss: 0.154904\n",
      "train Loss: 0.0948 batch_loss: 0.150238\n",
      "train Loss: 0.0950 batch_loss: 0.150289\n",
      "train Loss: 0.0952 batch_loss: 0.150088\n",
      "train Loss: 0.0954 batch_loss: 0.150107\n",
      "train Loss: 0.0956 batch_loss: 0.150201\n",
      "train Loss: 0.0958 batch_loss: 0.150244\n",
      "train Loss: 0.0960 batch_loss: 0.150925\n",
      "train Loss: 0.0962 batch_loss: 0.150117\n",
      "train Loss: 0.0964 batch_loss: 0.150106\n",
      "train Loss: 0.0966 batch_loss: 0.151104\n",
      "train Loss: 0.0968 batch_loss: 0.151627\n",
      "train Loss: 0.0970 batch_loss: 0.156923\n",
      "train Loss: 0.0972 batch_loss: 0.150092\n",
      "train Loss: 0.0974 batch_loss: 0.150414\n",
      "train Loss: 0.0976 batch_loss: 0.155244\n",
      "train Loss: 0.0978 batch_loss: 0.150554\n",
      "train Loss: 0.0980 batch_loss: 0.153132\n",
      "train Loss: 0.0982 batch_loss: 0.149755\n",
      "train Loss: 0.0984 batch_loss: 0.155569\n",
      "train Loss: 0.0986 batch_loss: 0.151179\n",
      "train Loss: 0.0988 batch_loss: 0.150146\n",
      "train Loss: 0.0990 batch_loss: 0.155936\n",
      "train Loss: 0.0992 batch_loss: 0.152808\n",
      "train Loss: 0.0994 batch_loss: 0.150202\n",
      "train Loss: 0.0996 batch_loss: 0.150906\n",
      "train Loss: 0.0998 batch_loss: 0.151792\n",
      "train Loss: 0.1000 batch_loss: 0.150186\n",
      "train Loss: 0.1002 batch_loss: 0.150209\n",
      "train Loss: 0.1004 batch_loss: 0.150093\n",
      "train Loss: 0.1006 batch_loss: 0.150824\n",
      "train Loss: 0.1008 batch_loss: 0.150007\n",
      "train Loss: 0.1010 batch_loss: 0.150125\n",
      "train Loss: 0.1012 batch_loss: 0.156991\n",
      "train Loss: 0.1014 batch_loss: 0.150084\n",
      "train Loss: 0.1015 batch_loss: 0.150174\n",
      "train Loss: 0.1017 batch_loss: 0.150164\n",
      "train Loss: 0.1019 batch_loss: 0.150705\n",
      "train Loss: 0.1021 batch_loss: 0.151460\n",
      "train Loss: 0.1023 batch_loss: 0.150028\n",
      "train Loss: 0.1025 batch_loss: 0.150032\n",
      "train Loss: 0.1027 batch_loss: 0.150026\n",
      "train Loss: 0.1029 batch_loss: 0.150537\n",
      "train Loss: 0.1031 batch_loss: 0.151028\n",
      "train Loss: 0.1033 batch_loss: 0.150886\n",
      "train Loss: 0.1035 batch_loss: 0.154438\n",
      "train Loss: 0.1037 batch_loss: 0.149637\n",
      "train Loss: 0.1039 batch_loss: 0.149601\n",
      "train Loss: 0.1041 batch_loss: 0.157147\n",
      "train Loss: 0.1043 batch_loss: 0.154134\n",
      "train Loss: 0.1045 batch_loss: 0.151301\n",
      "train Loss: 0.1047 batch_loss: 0.150596\n",
      "train Loss: 0.1049 batch_loss: 0.150078\n",
      "train Loss: 0.1051 batch_loss: 0.154471\n",
      "train Loss: 0.1053 batch_loss: 0.152476\n",
      "train Loss: 0.1055 batch_loss: 0.150247\n",
      "train Loss: 0.1057 batch_loss: 0.154704\n",
      "train Loss: 0.1059 batch_loss: 0.150301\n",
      "train Loss: 0.1061 batch_loss: 0.150927\n",
      "train Loss: 0.1063 batch_loss: 0.153148\n",
      "train Loss: 0.1065 batch_loss: 0.154063\n",
      "train Loss: 0.1067 batch_loss: 0.150043\n",
      "train Loss: 0.1069 batch_loss: 0.150213\n",
      "train Loss: 0.1071 batch_loss: 0.150183\n",
      "train Loss: 0.1073 batch_loss: 0.150096\n",
      "train Loss: 0.1075 batch_loss: 0.150064\n",
      "train Loss: 0.1077 batch_loss: 0.153731\n",
      "train Loss: 0.1079 batch_loss: 0.149812\n",
      "train Loss: 0.1081 batch_loss: 0.149849\n",
      "train Loss: 0.1082 batch_loss: 0.149840\n",
      "train Loss: 0.1084 batch_loss: 0.151270\n",
      "train Loss: 0.1086 batch_loss: 0.150769\n",
      "train Loss: 0.1088 batch_loss: 0.150252\n",
      "train Loss: 0.1090 batch_loss: 0.150600\n",
      "train Loss: 0.1092 batch_loss: 0.150093\n",
      "train Loss: 0.1094 batch_loss: 0.149990\n",
      "train Loss: 0.1096 batch_loss: 0.152875\n",
      "train Loss: 0.1098 batch_loss: 0.150537\n",
      "train Loss: 0.1100 batch_loss: 0.149699\n",
      "train Loss: 0.1102 batch_loss: 0.149685\n",
      "train Loss: 0.1104 batch_loss: 0.149616\n",
      "train Loss: 0.1106 batch_loss: 0.149812\n",
      "train Loss: 0.1108 batch_loss: 0.149907\n",
      "train Loss: 0.1110 batch_loss: 0.154888\n",
      "train Loss: 0.1112 batch_loss: 0.149931\n",
      "train Loss: 0.1114 batch_loss: 0.151551\n",
      "train Loss: 0.1116 batch_loss: 0.151320\n",
      "train Loss: 0.1118 batch_loss: 0.150869\n",
      "train Loss: 0.1120 batch_loss: 0.150218\n",
      "train Loss: 0.1122 batch_loss: 0.151600\n",
      "train Loss: 0.1124 batch_loss: 0.151230\n",
      "train Loss: 0.1126 batch_loss: 0.150389\n",
      "train Loss: 0.1128 batch_loss: 0.151024\n",
      "train Loss: 0.1130 batch_loss: 0.153101\n",
      "train Loss: 0.1132 batch_loss: 0.150235\n",
      "train Loss: 0.1134 batch_loss: 0.151065\n",
      "train Loss: 0.1136 batch_loss: 0.150976\n",
      "train Loss: 0.1137 batch_loss: 0.150114\n",
      "train Loss: 0.1139 batch_loss: 0.151206\n",
      "train Loss: 0.1141 batch_loss: 0.153473\n",
      "train Loss: 0.1143 batch_loss: 0.150185\n",
      "train Loss: 0.1145 batch_loss: 0.150159\n",
      "train Loss: 0.1147 batch_loss: 0.151216\n",
      "train Loss: 0.1149 batch_loss: 0.149942\n",
      "train Loss: 0.1151 batch_loss: 0.150003\n",
      "train Loss: 0.1153 batch_loss: 0.151878\n",
      "train Loss: 0.1155 batch_loss: 0.151113\n",
      "train Loss: 0.1157 batch_loss: 0.150300\n",
      "train Loss: 0.1159 batch_loss: 0.150240\n",
      "train Loss: 0.1161 batch_loss: 0.156841\n",
      "train Loss: 0.1163 batch_loss: 0.150191\n",
      "train Loss: 0.1165 batch_loss: 0.153610\n",
      "train Loss: 0.1167 batch_loss: 0.151717\n",
      "train Loss: 0.1169 batch_loss: 0.150103\n",
      "train Loss: 0.1171 batch_loss: 0.151198\n",
      "train Loss: 0.1173 batch_loss: 0.150239\n",
      "train Loss: 0.1175 batch_loss: 0.149961\n",
      "train Loss: 0.1177 batch_loss: 0.150826\n",
      "train Loss: 0.1179 batch_loss: 0.150768\n",
      "train Loss: 0.1181 batch_loss: 0.154542\n",
      "train Loss: 0.1183 batch_loss: 0.150235\n",
      "train Loss: 0.1185 batch_loss: 0.151633\n",
      "train Loss: 0.1187 batch_loss: 0.150141\n",
      "train Loss: 0.1189 batch_loss: 0.150218\n",
      "train Loss: 0.1191 batch_loss: 0.150349\n",
      "train Loss: 0.1193 batch_loss: 0.151502\n",
      "train Loss: 0.1195 batch_loss: 0.150029\n",
      "train Loss: 0.1197 batch_loss: 0.150354\n",
      "train Loss: 0.1198 batch_loss: 0.150491\n",
      "train Loss: 0.1200 batch_loss: 0.151162\n",
      "train Loss: 0.1202 batch_loss: 0.150005\n",
      "train Loss: 0.1204 batch_loss: 0.150005\n",
      "train Loss: 0.1206 batch_loss: 0.150001\n",
      "train Loss: 0.1208 batch_loss: 0.150067\n",
      "train Loss: 0.1210 batch_loss: 0.151487\n",
      "train Loss: 0.1212 batch_loss: 0.150118\n",
      "train Loss: 0.1214 batch_loss: 0.157983\n",
      "train Loss: 0.1216 batch_loss: 0.150064\n",
      "train Loss: 0.1218 batch_loss: 0.150487\n",
      "train Loss: 0.1220 batch_loss: 0.150905\n",
      "train Loss: 0.1222 batch_loss: 0.150131\n",
      "train Loss: 0.1224 batch_loss: 0.151001\n",
      "train Loss: 0.1226 batch_loss: 0.150302\n",
      "train Loss: 0.1228 batch_loss: 0.151534\n",
      "train Loss: 0.1230 batch_loss: 0.150030\n",
      "train Loss: 0.1232 batch_loss: 0.150289\n",
      "train Loss: 0.1234 batch_loss: 0.149982\n",
      "train Loss: 0.1236 batch_loss: 0.150167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1238 batch_loss: 0.150051\n",
      "train Loss: 0.1240 batch_loss: 0.154558\n",
      "train Loss: 0.1242 batch_loss: 0.150024\n",
      "train Loss: 0.1244 batch_loss: 0.151297\n",
      "train Loss: 0.1246 batch_loss: 0.150761\n",
      "train Loss: 0.1248 batch_loss: 0.151039\n",
      "train Loss: 0.1250 batch_loss: 0.149918\n",
      "train Loss: 0.1252 batch_loss: 0.150186\n",
      "train Loss: 0.1254 batch_loss: 0.153952\n",
      "train Loss: 0.1255 batch_loss: 0.150044\n",
      "train Loss: 0.1257 batch_loss: 0.150384\n",
      "train Loss: 0.1259 batch_loss: 0.150096\n",
      "train Loss: 0.1261 batch_loss: 0.151371\n",
      "train Loss: 0.1263 batch_loss: 0.150248\n",
      "train Loss: 0.1265 batch_loss: 0.149977\n",
      "train Loss: 0.1267 batch_loss: 0.150041\n",
      "train Loss: 0.1269 batch_loss: 0.149983\n",
      "train Loss: 0.1271 batch_loss: 0.151206\n",
      "train Loss: 0.1273 batch_loss: 0.151093\n",
      "train Loss: 0.1275 batch_loss: 0.149951\n",
      "train Loss: 0.1277 batch_loss: 0.149959\n",
      "train Loss: 0.1279 batch_loss: 0.149970\n",
      "train Loss: 0.1281 batch_loss: 0.150042\n",
      "train Loss: 0.1283 batch_loss: 0.150255\n",
      "train Loss: 0.1285 batch_loss: 0.149889\n",
      "train Loss: 0.1287 batch_loss: 0.150283\n",
      "train Loss: 0.1289 batch_loss: 0.153614\n",
      "train Loss: 0.1291 batch_loss: 0.150202\n",
      "train Loss: 0.1293 batch_loss: 0.150043\n",
      "train Loss: 0.1295 batch_loss: 0.153874\n",
      "train Loss: 0.1297 batch_loss: 0.149980\n",
      "train Loss: 0.1299 batch_loss: 0.151498\n",
      "train Loss: 0.1301 batch_loss: 0.153691\n",
      "train Loss: 0.1303 batch_loss: 0.149918\n",
      "train Loss: 0.1305 batch_loss: 0.149687\n",
      "train Loss: 0.1306 batch_loss: 0.149615\n",
      "train Loss: 0.1308 batch_loss: 0.151232\n",
      "train Loss: 0.1310 batch_loss: 0.158127\n",
      "train Loss: 0.1312 batch_loss: 0.149839\n",
      "train Loss: 0.1314 batch_loss: 0.156153\n",
      "train Loss: 0.1316 batch_loss: 0.150192\n",
      "train Loss: 0.1318 batch_loss: 0.151361\n",
      "train Loss: 0.1320 batch_loss: 0.151246\n",
      "train Loss: 0.1322 batch_loss: 0.156299\n",
      "train Loss: 0.1324 batch_loss: 0.150059\n",
      "train Loss: 0.1326 batch_loss: 0.154312\n",
      "train Loss: 0.1328 batch_loss: 0.150326\n",
      "train Loss: 0.1330 batch_loss: 0.150205\n",
      "train Loss: 0.1332 batch_loss: 0.150155\n",
      "train Loss: 0.1334 batch_loss: 0.152158\n",
      "train Loss: 0.1336 batch_loss: 0.150159\n",
      "train Loss: 0.1338 batch_loss: 0.150505\n",
      "train Loss: 0.1340 batch_loss: 0.151065\n",
      "train Loss: 0.1342 batch_loss: 0.154138\n",
      "train Loss: 0.1344 batch_loss: 0.149921\n",
      "train Loss: 0.1346 batch_loss: 0.149776\n",
      "train Loss: 0.1348 batch_loss: 0.149839\n",
      "train Loss: 0.1350 batch_loss: 0.149911\n",
      "train Loss: 0.1352 batch_loss: 0.150252\n",
      "train Loss: 0.1354 batch_loss: 0.150047\n",
      "train Loss: 0.1356 batch_loss: 0.150431\n",
      "train Loss: 0.1358 batch_loss: 0.150298\n",
      "train Loss: 0.1360 batch_loss: 0.150416\n",
      "train Loss: 0.1362 batch_loss: 0.150511\n",
      "train Loss: 0.1364 batch_loss: 0.149779\n",
      "train Loss: 0.1366 batch_loss: 0.149828\n",
      "train Loss: 0.1368 batch_loss: 0.151291\n",
      "train Loss: 0.1370 batch_loss: 0.152678\n",
      "train Loss: 0.1371 batch_loss: 0.150128\n",
      "train Loss: 0.1373 batch_loss: 0.150017\n",
      "train Loss: 0.1375 batch_loss: 0.150048\n",
      "train Loss: 0.1377 batch_loss: 0.151202\n",
      "train Loss: 0.1379 batch_loss: 0.158131\n",
      "train Loss: 0.1381 batch_loss: 0.150025\n",
      "train Loss: 0.1383 batch_loss: 0.151582\n",
      "train Loss: 0.1385 batch_loss: 0.155131\n",
      "train Loss: 0.1387 batch_loss: 0.150085\n",
      "train Loss: 0.1389 batch_loss: 0.150614\n",
      "train Loss: 0.1391 batch_loss: 0.150159\n",
      "train Loss: 0.1393 batch_loss: 0.153870\n",
      "train Loss: 0.1395 batch_loss: 0.151316\n",
      "train Loss: 0.1397 batch_loss: 0.151048\n",
      "train Loss: 0.1399 batch_loss: 0.150552\n",
      "train Loss: 0.1401 batch_loss: 0.155347\n",
      "train Loss: 0.1403 batch_loss: 0.150795\n",
      "train Loss: 0.1405 batch_loss: 0.150928\n",
      "train Loss: 0.1407 batch_loss: 0.150165\n",
      "train Loss: 0.1409 batch_loss: 0.150138\n",
      "train Loss: 0.1411 batch_loss: 0.150011\n",
      "train Loss: 0.1413 batch_loss: 0.154273\n",
      "train Loss: 0.1415 batch_loss: 0.150193\n",
      "train Loss: 0.1417 batch_loss: 0.150084\n",
      "train Loss: 0.1419 batch_loss: 0.150080\n",
      "train Loss: 0.1421 batch_loss: 0.151208\n",
      "train Loss: 0.1423 batch_loss: 0.149979\n",
      "train Loss: 0.1425 batch_loss: 0.153577\n",
      "train Loss: 0.1427 batch_loss: 0.150178\n",
      "train Loss: 0.1429 batch_loss: 0.149937\n",
      "train Loss: 0.1431 batch_loss: 0.151339\n",
      "train Loss: 0.1433 batch_loss: 0.151448\n",
      "train Loss: 0.1435 batch_loss: 0.150074\n",
      "train Loss: 0.1436 batch_loss: 0.150366\n",
      "train Loss: 0.1438 batch_loss: 0.150941\n",
      "train Loss: 0.1440 batch_loss: 0.153549\n",
      "train Loss: 0.1442 batch_loss: 0.150037\n",
      "train Loss: 0.1444 batch_loss: 0.149942\n",
      "train Loss: 0.1446 batch_loss: 0.153542\n",
      "train Loss: 0.1448 batch_loss: 0.150067\n",
      "train Loss: 0.1450 batch_loss: 0.149989\n",
      "train Loss: 0.1452 batch_loss: 0.150045\n",
      "train Loss: 0.1454 batch_loss: 0.150065\n",
      "train Loss: 0.1456 batch_loss: 0.150051\n",
      "train Loss: 0.1458 batch_loss: 0.150043\n",
      "train Loss: 0.1460 batch_loss: 0.150034\n",
      "train Loss: 0.1462 batch_loss: 0.150172\n",
      "train Loss: 0.1464 batch_loss: 0.150175\n",
      "train Loss: 0.1466 batch_loss: 0.151522\n",
      "train Loss: 0.1468 batch_loss: 0.150172\n",
      "train Loss: 0.1470 batch_loss: 0.150072\n",
      "train Loss: 0.1472 batch_loss: 0.149901\n",
      "train Loss: 0.1474 batch_loss: 0.149895\n",
      "train Loss: 0.1476 batch_loss: 0.150285\n",
      "train Loss: 0.1478 batch_loss: 0.151200\n",
      "train Loss: 0.1480 batch_loss: 0.155176\n",
      "train Loss: 0.1482 batch_loss: 0.150128\n",
      "train Loss: 0.1484 batch_loss: 0.151536\n",
      "train Loss: 0.1486 batch_loss: 0.150004\n",
      "train Loss: 0.1488 batch_loss: 0.150003\n",
      "train Loss: 0.1489 batch_loss: 0.151408\n",
      "train Loss: 0.1491 batch_loss: 0.151197\n",
      "train Loss: 0.1493 batch_loss: 0.151166\n",
      "train Loss: 0.1495 batch_loss: 0.156261\n",
      "train Loss: 0.1497 batch_loss: 0.155280\n",
      "train Loss: 0.1499 batch_loss: 0.150047\n",
      "train Loss: 0.1501 batch_loss: 0.153523\n",
      "train Loss: 0.1503 batch_loss: 0.151339\n",
      "train Loss: 0.1505 batch_loss: 0.154983\n",
      "train Loss: 0.1507 batch_loss: 0.150097\n",
      "train Loss: 0.1509 batch_loss: 0.154033\n",
      "train Loss: 0.1511 batch_loss: 0.151720\n",
      "train Loss: 0.1513 batch_loss: 0.150587\n",
      "train Loss: 0.1515 batch_loss: 0.150210\n",
      "train Loss: 0.1517 batch_loss: 0.150905\n",
      "train Loss: 0.1519 batch_loss: 0.149803\n",
      "train Loss: 0.1521 batch_loss: 0.150896\n",
      "train Loss: 0.1523 batch_loss: 0.149793\n",
      "train Loss: 0.1525 batch_loss: 0.156007\n",
      "train Loss: 0.1527 batch_loss: 0.156620\n",
      "train Loss: 0.1529 batch_loss: 0.150748\n",
      "train Loss: 0.1531 batch_loss: 0.151420\n",
      "train Loss: 0.1533 batch_loss: 0.150761\n",
      "train Loss: 0.1535 batch_loss: 0.150160\n",
      "train Loss: 0.1537 batch_loss: 0.150234\n",
      "train Loss: 0.1539 batch_loss: 0.152876\n",
      "Loss on the test images: 0.15237 \n",
      "Epoch 1/9\n",
      "----------\n",
      "trainloader ready!\n",
      "testloader ready!\n",
      "train Loss: 0.0002 batch_loss: 0.150089\n",
      "train Loss: 0.0004 batch_loss: 0.150336\n",
      "train Loss: 0.0006 batch_loss: 0.150025\n",
      "train Loss: 0.0008 batch_loss: 0.149940\n",
      "train Loss: 0.0010 batch_loss: 0.154922\n",
      "train Loss: 0.0012 batch_loss: 0.154133\n",
      "train Loss: 0.0014 batch_loss: 0.150865\n",
      "train Loss: 0.0016 batch_loss: 0.150990\n",
      "train Loss: 0.0018 batch_loss: 0.154224\n",
      "train Loss: 0.0020 batch_loss: 0.150035\n",
      "train Loss: 0.0022 batch_loss: 0.150153\n",
      "train Loss: 0.0024 batch_loss: 0.150184\n",
      "train Loss: 0.0026 batch_loss: 0.150258\n",
      "train Loss: 0.0028 batch_loss: 0.150022\n",
      "train Loss: 0.0030 batch_loss: 0.150130\n",
      "train Loss: 0.0031 batch_loss: 0.149985\n",
      "train Loss: 0.0033 batch_loss: 0.150831\n",
      "train Loss: 0.0035 batch_loss: 0.150366\n",
      "train Loss: 0.0037 batch_loss: 0.150140\n",
      "train Loss: 0.0039 batch_loss: 0.150227\n",
      "train Loss: 0.0041 batch_loss: 0.150381\n",
      "train Loss: 0.0043 batch_loss: 0.155254\n",
      "train Loss: 0.0045 batch_loss: 0.154399\n",
      "train Loss: 0.0047 batch_loss: 0.150381\n",
      "train Loss: 0.0049 batch_loss: 0.151355\n",
      "train Loss: 0.0051 batch_loss: 0.149931\n",
      "train Loss: 0.0053 batch_loss: 0.151517\n",
      "train Loss: 0.0055 batch_loss: 0.150097\n",
      "train Loss: 0.0057 batch_loss: 0.150044\n",
      "train Loss: 0.0059 batch_loss: 0.150622\n",
      "train Loss: 0.0061 batch_loss: 0.150678\n",
      "train Loss: 0.0063 batch_loss: 0.150096\n",
      "train Loss: 0.0065 batch_loss: 0.150061\n",
      "train Loss: 0.0067 batch_loss: 0.150094\n",
      "train Loss: 0.0069 batch_loss: 0.153636\n",
      "train Loss: 0.0071 batch_loss: 0.153492\n",
      "train Loss: 0.0073 batch_loss: 0.151019\n",
      "train Loss: 0.0075 batch_loss: 0.149914\n",
      "train Loss: 0.0077 batch_loss: 0.154731\n",
      "train Loss: 0.0079 batch_loss: 0.150526\n",
      "train Loss: 0.0081 batch_loss: 0.150131\n",
      "train Loss: 0.0083 batch_loss: 0.155194\n",
      "train Loss: 0.0085 batch_loss: 0.150152\n",
      "train Loss: 0.0087 batch_loss: 0.150104\n",
      "train Loss: 0.0089 batch_loss: 0.149995\n",
      "train Loss: 0.0091 batch_loss: 0.150247\n",
      "train Loss: 0.0092 batch_loss: 0.150016\n",
      "train Loss: 0.0094 batch_loss: 0.150941\n",
      "train Loss: 0.0096 batch_loss: 0.149984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0098 batch_loss: 0.150306\n",
      "train Loss: 0.0100 batch_loss: 0.150802\n",
      "train Loss: 0.0102 batch_loss: 0.149968\n",
      "train Loss: 0.0104 batch_loss: 0.149927\n",
      "train Loss: 0.0106 batch_loss: 0.149935\n",
      "train Loss: 0.0108 batch_loss: 0.150010\n",
      "train Loss: 0.0110 batch_loss: 0.155453\n",
      "train Loss: 0.0112 batch_loss: 0.150482\n",
      "train Loss: 0.0114 batch_loss: 0.150090\n",
      "train Loss: 0.0116 batch_loss: 0.150017\n",
      "train Loss: 0.0118 batch_loss: 0.151357\n",
      "train Loss: 0.0120 batch_loss: 0.149960\n",
      "train Loss: 0.0122 batch_loss: 0.150991\n",
      "train Loss: 0.0124 batch_loss: 0.150015\n",
      "train Loss: 0.0126 batch_loss: 0.150010\n",
      "train Loss: 0.0128 batch_loss: 0.150825\n",
      "train Loss: 0.0130 batch_loss: 0.150031\n",
      "train Loss: 0.0132 batch_loss: 0.150001\n",
      "train Loss: 0.0134 batch_loss: 0.155754\n",
      "train Loss: 0.0136 batch_loss: 0.150080\n",
      "train Loss: 0.0138 batch_loss: 0.151737\n",
      "train Loss: 0.0140 batch_loss: 0.155264\n",
      "train Loss: 0.0142 batch_loss: 0.150803\n",
      "train Loss: 0.0144 batch_loss: 0.149681\n",
      "train Loss: 0.0146 batch_loss: 0.149580\n",
      "train Loss: 0.0147 batch_loss: 0.149622\n",
      "train Loss: 0.0149 batch_loss: 0.149655\n",
      "train Loss: 0.0151 batch_loss: 0.154949\n",
      "train Loss: 0.0153 batch_loss: 0.150118\n",
      "train Loss: 0.0155 batch_loss: 0.154638\n",
      "train Loss: 0.0157 batch_loss: 0.149995\n",
      "train Loss: 0.0159 batch_loss: 0.150926\n",
      "train Loss: 0.0161 batch_loss: 0.150043\n",
      "train Loss: 0.0163 batch_loss: 0.150059\n",
      "train Loss: 0.0165 batch_loss: 0.151132\n",
      "train Loss: 0.0167 batch_loss: 0.150123\n",
      "train Loss: 0.0169 batch_loss: 0.150204\n",
      "train Loss: 0.0171 batch_loss: 0.150102\n",
      "train Loss: 0.0173 batch_loss: 0.150140\n",
      "train Loss: 0.0175 batch_loss: 0.150103\n",
      "train Loss: 0.0177 batch_loss: 0.150680\n",
      "train Loss: 0.0179 batch_loss: 0.150071\n",
      "train Loss: 0.0181 batch_loss: 0.156702\n",
      "train Loss: 0.0183 batch_loss: 0.150038\n",
      "train Loss: 0.0185 batch_loss: 0.154094\n",
      "train Loss: 0.0187 batch_loss: 0.149941\n",
      "train Loss: 0.0189 batch_loss: 0.149936\n",
      "train Loss: 0.0191 batch_loss: 0.152478\n",
      "train Loss: 0.0193 batch_loss: 0.151567\n",
      "train Loss: 0.0195 batch_loss: 0.150143\n",
      "train Loss: 0.0197 batch_loss: 0.149983\n",
      "train Loss: 0.0199 batch_loss: 0.149984\n",
      "train Loss: 0.0201 batch_loss: 0.151588\n",
      "train Loss: 0.0203 batch_loss: 0.149935\n",
      "train Loss: 0.0204 batch_loss: 0.149969\n",
      "train Loss: 0.0206 batch_loss: 0.150189\n",
      "train Loss: 0.0208 batch_loss: 0.150162\n",
      "train Loss: 0.0210 batch_loss: 0.150057\n",
      "train Loss: 0.0212 batch_loss: 0.154071\n",
      "train Loss: 0.0214 batch_loss: 0.150000\n",
      "train Loss: 0.0216 batch_loss: 0.149989\n",
      "train Loss: 0.0218 batch_loss: 0.151424\n",
      "train Loss: 0.0220 batch_loss: 0.154469\n",
      "train Loss: 0.0222 batch_loss: 0.150953\n",
      "train Loss: 0.0224 batch_loss: 0.149998\n",
      "train Loss: 0.0226 batch_loss: 0.150962\n",
      "train Loss: 0.0228 batch_loss: 0.150024\n",
      "train Loss: 0.0230 batch_loss: 0.150782\n",
      "train Loss: 0.0232 batch_loss: 0.151149\n",
      "train Loss: 0.0234 batch_loss: 0.149955\n",
      "train Loss: 0.0236 batch_loss: 0.155458\n",
      "train Loss: 0.0238 batch_loss: 0.149924\n",
      "train Loss: 0.0240 batch_loss: 0.155635\n",
      "train Loss: 0.0242 batch_loss: 0.149930\n",
      "train Loss: 0.0244 batch_loss: 0.150260\n",
      "train Loss: 0.0246 batch_loss: 0.149971\n",
      "train Loss: 0.0248 batch_loss: 0.154539\n",
      "train Loss: 0.0250 batch_loss: 0.150118\n",
      "train Loss: 0.0252 batch_loss: 0.154673\n",
      "train Loss: 0.0254 batch_loss: 0.150010\n",
      "train Loss: 0.0256 batch_loss: 0.151034\n",
      "train Loss: 0.0258 batch_loss: 0.150227\n",
      "train Loss: 0.0260 batch_loss: 0.151069\n",
      "train Loss: 0.0262 batch_loss: 0.150239\n",
      "train Loss: 0.0264 batch_loss: 0.151145\n",
      "train Loss: 0.0266 batch_loss: 0.150104\n",
      "train Loss: 0.0268 batch_loss: 0.150969\n",
      "train Loss: 0.0269 batch_loss: 0.150070\n",
      "train Loss: 0.0271 batch_loss: 0.149947\n",
      "train Loss: 0.0273 batch_loss: 0.150020\n",
      "train Loss: 0.0275 batch_loss: 0.149970\n",
      "train Loss: 0.0277 batch_loss: 0.150136\n",
      "train Loss: 0.0279 batch_loss: 0.150088\n",
      "train Loss: 0.0281 batch_loss: 0.155569\n",
      "train Loss: 0.0283 batch_loss: 0.150130\n",
      "train Loss: 0.0285 batch_loss: 0.150036\n",
      "train Loss: 0.0287 batch_loss: 0.150048\n",
      "train Loss: 0.0289 batch_loss: 0.153654\n",
      "train Loss: 0.0291 batch_loss: 0.153070\n",
      "train Loss: 0.0293 batch_loss: 0.150945\n",
      "train Loss: 0.0295 batch_loss: 0.149692\n",
      "train Loss: 0.0297 batch_loss: 0.149597\n",
      "train Loss: 0.0299 batch_loss: 0.150846\n",
      "train Loss: 0.0301 batch_loss: 0.150099\n",
      "train Loss: 0.0303 batch_loss: 0.152209\n",
      "train Loss: 0.0305 batch_loss: 0.151034\n",
      "train Loss: 0.0307 batch_loss: 0.150081\n",
      "train Loss: 0.0309 batch_loss: 0.150050\n",
      "train Loss: 0.0311 batch_loss: 0.149984\n",
      "train Loss: 0.0313 batch_loss: 0.150010\n",
      "train Loss: 0.0315 batch_loss: 0.150001\n",
      "train Loss: 0.0317 batch_loss: 0.150017\n",
      "train Loss: 0.0319 batch_loss: 0.150007\n",
      "train Loss: 0.0320 batch_loss: 0.150184\n",
      "train Loss: 0.0322 batch_loss: 0.152503\n",
      "train Loss: 0.0324 batch_loss: 0.151031\n",
      "train Loss: 0.0326 batch_loss: 0.149955\n",
      "train Loss: 0.0328 batch_loss: 0.149929\n",
      "train Loss: 0.0330 batch_loss: 0.149913\n",
      "train Loss: 0.0332 batch_loss: 0.149963\n",
      "train Loss: 0.0334 batch_loss: 0.149902\n",
      "train Loss: 0.0336 batch_loss: 0.150195\n",
      "train Loss: 0.0338 batch_loss: 0.151379\n",
      "train Loss: 0.0340 batch_loss: 0.149935\n",
      "train Loss: 0.0342 batch_loss: 0.149953\n",
      "train Loss: 0.0344 batch_loss: 0.150695\n",
      "train Loss: 0.0346 batch_loss: 0.149943\n",
      "train Loss: 0.0348 batch_loss: 0.153592\n",
      "train Loss: 0.0350 batch_loss: 0.150760\n",
      "train Loss: 0.0352 batch_loss: 0.149934\n",
      "train Loss: 0.0354 batch_loss: 0.151355\n",
      "train Loss: 0.0356 batch_loss: 0.150519\n",
      "train Loss: 0.0358 batch_loss: 0.149909\n",
      "train Loss: 0.0360 batch_loss: 0.149911\n",
      "train Loss: 0.0362 batch_loss: 0.149929\n",
      "train Loss: 0.0364 batch_loss: 0.150111\n",
      "train Loss: 0.0366 batch_loss: 0.151478\n",
      "train Loss: 0.0368 batch_loss: 0.150007\n",
      "train Loss: 0.0369 batch_loss: 0.151750\n",
      "train Loss: 0.0371 batch_loss: 0.150217\n",
      "train Loss: 0.0373 batch_loss: 0.149890\n",
      "train Loss: 0.0375 batch_loss: 0.149928\n",
      "train Loss: 0.0377 batch_loss: 0.151007\n",
      "train Loss: 0.0379 batch_loss: 0.150028\n",
      "train Loss: 0.0381 batch_loss: 0.149983\n",
      "train Loss: 0.0383 batch_loss: 0.153239\n",
      "train Loss: 0.0385 batch_loss: 0.150149\n",
      "train Loss: 0.0387 batch_loss: 0.149898\n",
      "train Loss: 0.0389 batch_loss: 0.149911\n",
      "train Loss: 0.0391 batch_loss: 0.150645\n",
      "train Loss: 0.0393 batch_loss: 0.149887\n",
      "train Loss: 0.0395 batch_loss: 0.152627\n",
      "train Loss: 0.0397 batch_loss: 0.150314\n",
      "train Loss: 0.0399 batch_loss: 0.150081\n",
      "train Loss: 0.0401 batch_loss: 0.150127\n",
      "train Loss: 0.0403 batch_loss: 0.150905\n",
      "train Loss: 0.0405 batch_loss: 0.149969\n",
      "train Loss: 0.0407 batch_loss: 0.150979\n",
      "train Loss: 0.0409 batch_loss: 0.150028\n",
      "train Loss: 0.0411 batch_loss: 0.157310\n",
      "train Loss: 0.0413 batch_loss: 0.150117\n",
      "train Loss: 0.0415 batch_loss: 0.150010\n",
      "train Loss: 0.0417 batch_loss: 0.155954\n",
      "train Loss: 0.0419 batch_loss: 0.150053\n",
      "train Loss: 0.0421 batch_loss: 0.149903\n",
      "train Loss: 0.0423 batch_loss: 0.150927\n",
      "train Loss: 0.0424 batch_loss: 0.149970\n",
      "train Loss: 0.0426 batch_loss: 0.150731\n",
      "train Loss: 0.0428 batch_loss: 0.149871\n",
      "train Loss: 0.0430 batch_loss: 0.155287\n",
      "train Loss: 0.0432 batch_loss: 0.150752\n",
      "train Loss: 0.0434 batch_loss: 0.149658\n",
      "train Loss: 0.0436 batch_loss: 0.149569\n",
      "train Loss: 0.0438 batch_loss: 0.150359\n",
      "train Loss: 0.0440 batch_loss: 0.150045\n",
      "train Loss: 0.0442 batch_loss: 0.150823\n",
      "train Loss: 0.0444 batch_loss: 0.149993\n",
      "train Loss: 0.0446 batch_loss: 0.151020\n",
      "train Loss: 0.0448 batch_loss: 0.149962\n",
      "train Loss: 0.0450 batch_loss: 0.151183\n",
      "train Loss: 0.0452 batch_loss: 0.149958\n",
      "train Loss: 0.0454 batch_loss: 0.149912\n",
      "train Loss: 0.0456 batch_loss: 0.151511\n",
      "train Loss: 0.0458 batch_loss: 0.149984\n",
      "train Loss: 0.0460 batch_loss: 0.150106\n",
      "train Loss: 0.0462 batch_loss: 0.149988\n",
      "train Loss: 0.0464 batch_loss: 0.149987\n",
      "train Loss: 0.0466 batch_loss: 0.149994\n",
      "train Loss: 0.0468 batch_loss: 0.153210\n",
      "train Loss: 0.0470 batch_loss: 0.149988\n",
      "train Loss: 0.0472 batch_loss: 0.156072\n",
      "train Loss: 0.0474 batch_loss: 0.150847\n",
      "train Loss: 0.0476 batch_loss: 0.153323\n",
      "train Loss: 0.0478 batch_loss: 0.152363\n",
      "train Loss: 0.0480 batch_loss: 0.150627\n",
      "train Loss: 0.0481 batch_loss: 0.150623\n",
      "train Loss: 0.0483 batch_loss: 0.149957\n",
      "train Loss: 0.0485 batch_loss: 0.150137\n",
      "train Loss: 0.0487 batch_loss: 0.149967\n",
      "train Loss: 0.0489 batch_loss: 0.151701\n",
      "train Loss: 0.0491 batch_loss: 0.155581\n",
      "train Loss: 0.0493 batch_loss: 0.149982\n",
      "train Loss: 0.0495 batch_loss: 0.151142\n",
      "train Loss: 0.0497 batch_loss: 0.150149\n",
      "train Loss: 0.0499 batch_loss: 0.151300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0501 batch_loss: 0.150098\n",
      "train Loss: 0.0503 batch_loss: 0.151010\n",
      "train Loss: 0.0505 batch_loss: 0.152922\n",
      "train Loss: 0.0507 batch_loss: 0.150073\n",
      "train Loss: 0.0509 batch_loss: 0.150026\n",
      "train Loss: 0.0511 batch_loss: 0.150379\n",
      "train Loss: 0.0513 batch_loss: 0.151250\n",
      "train Loss: 0.0515 batch_loss: 0.150747\n",
      "train Loss: 0.0517 batch_loss: 0.150179\n",
      "train Loss: 0.0519 batch_loss: 0.150166\n",
      "train Loss: 0.0521 batch_loss: 0.151769\n",
      "train Loss: 0.0523 batch_loss: 0.150041\n",
      "train Loss: 0.0525 batch_loss: 0.150087\n",
      "train Loss: 0.0527 batch_loss: 0.150092\n",
      "train Loss: 0.0529 batch_loss: 0.150294\n",
      "train Loss: 0.0531 batch_loss: 0.150923\n",
      "train Loss: 0.0533 batch_loss: 0.151753\n",
      "train Loss: 0.0535 batch_loss: 0.150026\n",
      "train Loss: 0.0536 batch_loss: 0.149899\n",
      "train Loss: 0.0538 batch_loss: 0.149900\n",
      "train Loss: 0.0540 batch_loss: 0.154273\n",
      "train Loss: 0.0542 batch_loss: 0.149977\n",
      "train Loss: 0.0544 batch_loss: 0.151634\n",
      "train Loss: 0.0546 batch_loss: 0.149956\n",
      "train Loss: 0.0548 batch_loss: 0.155854\n",
      "train Loss: 0.0550 batch_loss: 0.153958\n",
      "train Loss: 0.0552 batch_loss: 0.150495\n",
      "train Loss: 0.0554 batch_loss: 0.156223\n",
      "train Loss: 0.0556 batch_loss: 0.150135\n",
      "train Loss: 0.0558 batch_loss: 0.149955\n",
      "train Loss: 0.0560 batch_loss: 0.150952\n",
      "train Loss: 0.0562 batch_loss: 0.150979\n",
      "train Loss: 0.0564 batch_loss: 0.149971\n",
      "train Loss: 0.0566 batch_loss: 0.150084\n",
      "train Loss: 0.0568 batch_loss: 0.149929\n",
      "train Loss: 0.0570 batch_loss: 0.151197\n",
      "train Loss: 0.0572 batch_loss: 0.149949\n",
      "train Loss: 0.0574 batch_loss: 0.150078\n",
      "train Loss: 0.0576 batch_loss: 0.150822\n",
      "train Loss: 0.0578 batch_loss: 0.153854\n",
      "train Loss: 0.0580 batch_loss: 0.149918\n",
      "train Loss: 0.0582 batch_loss: 0.150727\n",
      "train Loss: 0.0584 batch_loss: 0.149950\n",
      "train Loss: 0.0586 batch_loss: 0.151016\n",
      "train Loss: 0.0588 batch_loss: 0.150717\n",
      "train Loss: 0.0590 batch_loss: 0.150028\n",
      "train Loss: 0.0592 batch_loss: 0.150349\n",
      "train Loss: 0.0594 batch_loss: 0.150056\n",
      "train Loss: 0.0595 batch_loss: 0.151133\n",
      "train Loss: 0.0597 batch_loss: 0.153679\n",
      "train Loss: 0.0599 batch_loss: 0.149980\n",
      "train Loss: 0.0601 batch_loss: 0.150071\n",
      "train Loss: 0.0603 batch_loss: 0.150182\n",
      "train Loss: 0.0605 batch_loss: 0.150796\n",
      "train Loss: 0.0607 batch_loss: 0.150866\n",
      "train Loss: 0.0609 batch_loss: 0.152187\n",
      "train Loss: 0.0611 batch_loss: 0.150200\n",
      "train Loss: 0.0613 batch_loss: 0.150308\n",
      "train Loss: 0.0615 batch_loss: 0.150819\n",
      "train Loss: 0.0617 batch_loss: 0.149892\n",
      "train Loss: 0.0619 batch_loss: 0.149969\n",
      "train Loss: 0.0621 batch_loss: 0.149975\n",
      "train Loss: 0.0623 batch_loss: 0.155012\n",
      "train Loss: 0.0625 batch_loss: 0.150138\n",
      "train Loss: 0.0627 batch_loss: 0.150121\n",
      "train Loss: 0.0629 batch_loss: 0.150897\n",
      "train Loss: 0.0631 batch_loss: 0.150015\n",
      "train Loss: 0.0633 batch_loss: 0.153660\n",
      "train Loss: 0.0635 batch_loss: 0.151027\n",
      "train Loss: 0.0637 batch_loss: 0.154935\n",
      "train Loss: 0.0639 batch_loss: 0.150029\n",
      "train Loss: 0.0641 batch_loss: 0.151221\n",
      "train Loss: 0.0643 batch_loss: 0.154272\n",
      "train Loss: 0.0645 batch_loss: 0.151394\n",
      "train Loss: 0.0647 batch_loss: 0.151482\n",
      "train Loss: 0.0649 batch_loss: 0.151614\n",
      "train Loss: 0.0651 batch_loss: 0.153765\n",
      "train Loss: 0.0653 batch_loss: 0.150822\n",
      "train Loss: 0.0655 batch_loss: 0.149898\n",
      "train Loss: 0.0657 batch_loss: 0.149900\n",
      "train Loss: 0.0659 batch_loss: 0.149898\n",
      "train Loss: 0.0660 batch_loss: 0.151569\n",
      "train Loss: 0.0662 batch_loss: 0.154518\n",
      "train Loss: 0.0664 batch_loss: 0.150151\n",
      "train Loss: 0.0666 batch_loss: 0.149948\n",
      "train Loss: 0.0668 batch_loss: 0.149899\n",
      "train Loss: 0.0670 batch_loss: 0.150322\n",
      "train Loss: 0.0672 batch_loss: 0.150393\n",
      "train Loss: 0.0674 batch_loss: 0.150081\n",
      "train Loss: 0.0676 batch_loss: 0.150183\n",
      "train Loss: 0.0678 batch_loss: 0.153703\n",
      "train Loss: 0.0680 batch_loss: 0.152371\n",
      "train Loss: 0.0682 batch_loss: 0.154830\n",
      "train Loss: 0.0684 batch_loss: 0.150090\n",
      "train Loss: 0.0686 batch_loss: 0.150509\n",
      "train Loss: 0.0688 batch_loss: 0.153796\n",
      "train Loss: 0.0690 batch_loss: 0.150269\n",
      "train Loss: 0.0692 batch_loss: 0.151695\n",
      "train Loss: 0.0694 batch_loss: 0.150058\n",
      "train Loss: 0.0696 batch_loss: 0.150565\n",
      "train Loss: 0.0698 batch_loss: 0.152744\n",
      "train Loss: 0.0700 batch_loss: 0.150422\n",
      "train Loss: 0.0702 batch_loss: 0.150690\n",
      "train Loss: 0.0704 batch_loss: 0.153852\n",
      "train Loss: 0.0706 batch_loss: 0.150196\n",
      "train Loss: 0.0708 batch_loss: 0.155080\n",
      "train Loss: 0.0710 batch_loss: 0.150020\n",
      "train Loss: 0.0712 batch_loss: 0.150104\n",
      "train Loss: 0.0714 batch_loss: 0.150271\n",
      "train Loss: 0.0716 batch_loss: 0.151110\n",
      "train Loss: 0.0718 batch_loss: 0.150190\n",
      "train Loss: 0.0720 batch_loss: 0.152820\n",
      "train Loss: 0.0722 batch_loss: 0.153417\n",
      "train Loss: 0.0724 batch_loss: 0.151466\n",
      "train Loss: 0.0726 batch_loss: 0.151128\n",
      "train Loss: 0.0727 batch_loss: 0.150210\n",
      "train Loss: 0.0729 batch_loss: 0.150136\n",
      "train Loss: 0.0731 batch_loss: 0.149939\n",
      "train Loss: 0.0733 batch_loss: 0.150241\n",
      "train Loss: 0.0735 batch_loss: 0.150873\n",
      "train Loss: 0.0737 batch_loss: 0.156206\n",
      "train Loss: 0.0739 batch_loss: 0.149994\n",
      "train Loss: 0.0741 batch_loss: 0.150199\n",
      "train Loss: 0.0743 batch_loss: 0.150145\n",
      "train Loss: 0.0745 batch_loss: 0.151213\n",
      "train Loss: 0.0747 batch_loss: 0.154720\n",
      "train Loss: 0.0749 batch_loss: 0.149937\n",
      "train Loss: 0.0751 batch_loss: 0.149706\n",
      "train Loss: 0.0753 batch_loss: 0.149692\n",
      "train Loss: 0.0755 batch_loss: 0.152969\n",
      "train Loss: 0.0757 batch_loss: 0.149890\n",
      "train Loss: 0.0759 batch_loss: 0.150041\n",
      "train Loss: 0.0761 batch_loss: 0.149902\n",
      "train Loss: 0.0763 batch_loss: 0.150038\n",
      "train Loss: 0.0765 batch_loss: 0.149890\n",
      "train Loss: 0.0767 batch_loss: 0.150030\n",
      "train Loss: 0.0769 batch_loss: 0.149939\n",
      "train Loss: 0.0771 batch_loss: 0.150015\n",
      "train Loss: 0.0773 batch_loss: 0.156042\n",
      "train Loss: 0.0775 batch_loss: 0.149994\n",
      "train Loss: 0.0777 batch_loss: 0.149993\n",
      "train Loss: 0.0779 batch_loss: 0.150109\n",
      "train Loss: 0.0781 batch_loss: 0.149980\n",
      "train Loss: 0.0782 batch_loss: 0.150003\n",
      "train Loss: 0.0784 batch_loss: 0.149997\n",
      "train Loss: 0.0786 batch_loss: 0.150160\n",
      "train Loss: 0.0788 batch_loss: 0.151903\n",
      "train Loss: 0.0790 batch_loss: 0.149657\n",
      "train Loss: 0.0792 batch_loss: 0.151858\n",
      "train Loss: 0.0794 batch_loss: 0.149801\n",
      "train Loss: 0.0796 batch_loss: 0.150126\n",
      "train Loss: 0.0798 batch_loss: 0.150136\n",
      "train Loss: 0.0800 batch_loss: 0.150032\n",
      "train Loss: 0.0802 batch_loss: 0.150191\n",
      "train Loss: 0.0804 batch_loss: 0.149979\n",
      "train Loss: 0.0806 batch_loss: 0.153190\n",
      "train Loss: 0.0808 batch_loss: 0.149989\n",
      "train Loss: 0.0810 batch_loss: 0.151583\n",
      "train Loss: 0.0812 batch_loss: 0.152714\n",
      "train Loss: 0.0814 batch_loss: 0.150603\n",
      "train Loss: 0.0816 batch_loss: 0.150013\n",
      "train Loss: 0.0818 batch_loss: 0.151516\n",
      "train Loss: 0.0820 batch_loss: 0.151274\n",
      "train Loss: 0.0822 batch_loss: 0.152109\n",
      "train Loss: 0.0824 batch_loss: 0.149917\n",
      "train Loss: 0.0826 batch_loss: 0.150085\n",
      "train Loss: 0.0828 batch_loss: 0.149950\n",
      "train Loss: 0.0830 batch_loss: 0.149915\n",
      "train Loss: 0.0832 batch_loss: 0.150248\n",
      "train Loss: 0.0834 batch_loss: 0.153806\n",
      "train Loss: 0.0835 batch_loss: 0.149984\n",
      "train Loss: 0.0837 batch_loss: 0.149901\n",
      "train Loss: 0.0839 batch_loss: 0.149879\n",
      "train Loss: 0.0841 batch_loss: 0.150748\n",
      "train Loss: 0.0843 batch_loss: 0.150034\n",
      "train Loss: 0.0845 batch_loss: 0.151195\n",
      "train Loss: 0.0847 batch_loss: 0.149959\n",
      "train Loss: 0.0849 batch_loss: 0.150165\n",
      "train Loss: 0.0851 batch_loss: 0.150054\n",
      "train Loss: 0.0853 batch_loss: 0.149967\n",
      "train Loss: 0.0855 batch_loss: 0.150747\n",
      "train Loss: 0.0857 batch_loss: 0.153534\n",
      "train Loss: 0.0859 batch_loss: 0.149866\n",
      "train Loss: 0.0861 batch_loss: 0.149749\n",
      "train Loss: 0.0863 batch_loss: 0.149761\n",
      "train Loss: 0.0865 batch_loss: 0.150755\n",
      "train Loss: 0.0867 batch_loss: 0.149939\n",
      "train Loss: 0.0869 batch_loss: 0.151006\n",
      "train Loss: 0.0871 batch_loss: 0.150692\n",
      "train Loss: 0.0873 batch_loss: 0.153447\n",
      "train Loss: 0.0875 batch_loss: 0.149754\n",
      "train Loss: 0.0877 batch_loss: 0.149745\n",
      "train Loss: 0.0879 batch_loss: 0.151208\n",
      "train Loss: 0.0881 batch_loss: 0.153099\n",
      "train Loss: 0.0883 batch_loss: 0.150250\n",
      "train Loss: 0.0884 batch_loss: 0.149937\n",
      "train Loss: 0.0886 batch_loss: 0.150015\n",
      "train Loss: 0.0888 batch_loss: 0.150579\n",
      "train Loss: 0.0890 batch_loss: 0.154436\n",
      "train Loss: 0.0892 batch_loss: 0.151274\n",
      "train Loss: 0.0894 batch_loss: 0.149899\n",
      "train Loss: 0.0896 batch_loss: 0.149897\n",
      "train Loss: 0.0898 batch_loss: 0.149917\n",
      "train Loss: 0.0900 batch_loss: 0.149997\n",
      "train Loss: 0.0902 batch_loss: 0.149950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0904 batch_loss: 0.150912\n",
      "train Loss: 0.0906 batch_loss: 0.153149\n",
      "train Loss: 0.0908 batch_loss: 0.150849\n",
      "train Loss: 0.0910 batch_loss: 0.150335\n",
      "train Loss: 0.0912 batch_loss: 0.155538\n",
      "train Loss: 0.0914 batch_loss: 0.150034\n",
      "train Loss: 0.0916 batch_loss: 0.151068\n",
      "train Loss: 0.0918 batch_loss: 0.150058\n",
      "train Loss: 0.0920 batch_loss: 0.150357\n",
      "train Loss: 0.0922 batch_loss: 0.150018\n",
      "train Loss: 0.0924 batch_loss: 0.151357\n",
      "train Loss: 0.0926 batch_loss: 0.150050\n",
      "train Loss: 0.0928 batch_loss: 0.150040\n",
      "train Loss: 0.0930 batch_loss: 0.150127\n",
      "train Loss: 0.0932 batch_loss: 0.150783\n",
      "train Loss: 0.0934 batch_loss: 0.150604\n",
      "train Loss: 0.0936 batch_loss: 0.150012\n",
      "train Loss: 0.0938 batch_loss: 0.151662\n",
      "train Loss: 0.0939 batch_loss: 0.149883\n",
      "train Loss: 0.0941 batch_loss: 0.149929\n",
      "train Loss: 0.0943 batch_loss: 0.149893\n",
      "train Loss: 0.0945 batch_loss: 0.152996\n",
      "train Loss: 0.0947 batch_loss: 0.150100\n",
      "train Loss: 0.0949 batch_loss: 0.149968\n",
      "train Loss: 0.0951 batch_loss: 0.150005\n",
      "train Loss: 0.0953 batch_loss: 0.152427\n",
      "train Loss: 0.0955 batch_loss: 0.149796\n",
      "train Loss: 0.0957 batch_loss: 0.151028\n",
      "train Loss: 0.0959 batch_loss: 0.151005\n",
      "train Loss: 0.0961 batch_loss: 0.156487\n",
      "train Loss: 0.0963 batch_loss: 0.151049\n",
      "train Loss: 0.0965 batch_loss: 0.150130\n",
      "train Loss: 0.0967 batch_loss: 0.151126\n",
      "train Loss: 0.0969 batch_loss: 0.150126\n",
      "train Loss: 0.0971 batch_loss: 0.150068\n",
      "train Loss: 0.0973 batch_loss: 0.150241\n",
      "train Loss: 0.0975 batch_loss: 0.151109\n",
      "train Loss: 0.0977 batch_loss: 0.153626\n",
      "train Loss: 0.0979 batch_loss: 0.149921\n",
      "train Loss: 0.0981 batch_loss: 0.150051\n",
      "train Loss: 0.0983 batch_loss: 0.150323\n",
      "train Loss: 0.0985 batch_loss: 0.150672\n",
      "train Loss: 0.0987 batch_loss: 0.151192\n",
      "train Loss: 0.0989 batch_loss: 0.154190\n",
      "train Loss: 0.0991 batch_loss: 0.151057\n",
      "train Loss: 0.0993 batch_loss: 0.153092\n",
      "train Loss: 0.0995 batch_loss: 0.149951\n",
      "train Loss: 0.0997 batch_loss: 0.153816\n",
      "train Loss: 0.0999 batch_loss: 0.149979\n",
      "train Loss: 0.1000 batch_loss: 0.150168\n",
      "train Loss: 0.1002 batch_loss: 0.149983\n",
      "train Loss: 0.1004 batch_loss: 0.149988\n",
      "train Loss: 0.1006 batch_loss: 0.153815\n",
      "train Loss: 0.1008 batch_loss: 0.149550\n",
      "train Loss: 0.1010 batch_loss: 0.150664\n",
      "train Loss: 0.1012 batch_loss: 0.149636\n",
      "train Loss: 0.1014 batch_loss: 0.149735\n",
      "train Loss: 0.1016 batch_loss: 0.149985\n",
      "train Loss: 0.1018 batch_loss: 0.150053\n",
      "train Loss: 0.1020 batch_loss: 0.151200\n",
      "train Loss: 0.1022 batch_loss: 0.151140\n",
      "train Loss: 0.1024 batch_loss: 0.150066\n",
      "train Loss: 0.1026 batch_loss: 0.150028\n",
      "train Loss: 0.1028 batch_loss: 0.150050\n",
      "train Loss: 0.1030 batch_loss: 0.151666\n",
      "train Loss: 0.1032 batch_loss: 0.150222\n",
      "train Loss: 0.1034 batch_loss: 0.150251\n",
      "train Loss: 0.1036 batch_loss: 0.149991\n",
      "train Loss: 0.1038 batch_loss: 0.150852\n",
      "train Loss: 0.1040 batch_loss: 0.150320\n",
      "train Loss: 0.1042 batch_loss: 0.150010\n",
      "train Loss: 0.1044 batch_loss: 0.150833\n",
      "train Loss: 0.1046 batch_loss: 0.149993\n",
      "train Loss: 0.1048 batch_loss: 0.150866\n",
      "train Loss: 0.1049 batch_loss: 0.151848\n",
      "train Loss: 0.1051 batch_loss: 0.149756\n",
      "train Loss: 0.1053 batch_loss: 0.150102\n",
      "train Loss: 0.1055 batch_loss: 0.151112\n",
      "train Loss: 0.1057 batch_loss: 0.153317\n",
      "train Loss: 0.1059 batch_loss: 0.150134\n",
      "train Loss: 0.1061 batch_loss: 0.150057\n",
      "train Loss: 0.1063 batch_loss: 0.149934\n",
      "train Loss: 0.1065 batch_loss: 0.149848\n",
      "train Loss: 0.1067 batch_loss: 0.149846\n",
      "train Loss: 0.1069 batch_loss: 0.150856\n",
      "train Loss: 0.1071 batch_loss: 0.149560\n",
      "train Loss: 0.1073 batch_loss: 0.151042\n",
      "train Loss: 0.1075 batch_loss: 0.149783\n",
      "train Loss: 0.1077 batch_loss: 0.149539\n",
      "train Loss: 0.1079 batch_loss: 0.153328\n",
      "train Loss: 0.1081 batch_loss: 0.149946\n",
      "train Loss: 0.1083 batch_loss: 0.150055\n",
      "train Loss: 0.1085 batch_loss: 0.150812\n",
      "train Loss: 0.1087 batch_loss: 0.151487\n",
      "train Loss: 0.1089 batch_loss: 0.153602\n",
      "train Loss: 0.1091 batch_loss: 0.150916\n",
      "train Loss: 0.1093 batch_loss: 0.150081\n",
      "train Loss: 0.1095 batch_loss: 0.150000\n",
      "train Loss: 0.1097 batch_loss: 0.150254\n",
      "train Loss: 0.1099 batch_loss: 0.150230\n",
      "train Loss: 0.1100 batch_loss: 0.150430\n",
      "train Loss: 0.1102 batch_loss: 0.150761\n",
      "train Loss: 0.1104 batch_loss: 0.150259\n",
      "train Loss: 0.1106 batch_loss: 0.151135\n",
      "train Loss: 0.1108 batch_loss: 0.150020\n",
      "train Loss: 0.1110 batch_loss: 0.150001\n",
      "train Loss: 0.1112 batch_loss: 0.154828\n",
      "train Loss: 0.1114 batch_loss: 0.154719\n",
      "train Loss: 0.1116 batch_loss: 0.150250\n",
      "train Loss: 0.1118 batch_loss: 0.150035\n",
      "train Loss: 0.1120 batch_loss: 0.150589\n",
      "train Loss: 0.1122 batch_loss: 0.149939\n",
      "train Loss: 0.1124 batch_loss: 0.155182\n",
      "train Loss: 0.1126 batch_loss: 0.150208\n",
      "train Loss: 0.1128 batch_loss: 0.149993\n",
      "train Loss: 0.1130 batch_loss: 0.149970\n",
      "train Loss: 0.1132 batch_loss: 0.152897\n",
      "train Loss: 0.1134 batch_loss: 0.150002\n",
      "train Loss: 0.1136 batch_loss: 0.150020\n",
      "train Loss: 0.1138 batch_loss: 0.152699\n",
      "train Loss: 0.1140 batch_loss: 0.150930\n",
      "train Loss: 0.1142 batch_loss: 0.150372\n",
      "train Loss: 0.1144 batch_loss: 0.150065\n",
      "train Loss: 0.1146 batch_loss: 0.149906\n",
      "train Loss: 0.1148 batch_loss: 0.153378\n",
      "train Loss: 0.1150 batch_loss: 0.150042\n",
      "train Loss: 0.1152 batch_loss: 0.150445\n",
      "train Loss: 0.1154 batch_loss: 0.150248\n",
      "train Loss: 0.1156 batch_loss: 0.151194\n",
      "train Loss: 0.1158 batch_loss: 0.150133\n",
      "train Loss: 0.1159 batch_loss: 0.151663\n",
      "train Loss: 0.1161 batch_loss: 0.150097\n",
      "train Loss: 0.1163 batch_loss: 0.157020\n",
      "train Loss: 0.1165 batch_loss: 0.149930\n",
      "train Loss: 0.1167 batch_loss: 0.149954\n",
      "train Loss: 0.1169 batch_loss: 0.149974\n",
      "train Loss: 0.1171 batch_loss: 0.151321\n",
      "train Loss: 0.1173 batch_loss: 0.154043\n",
      "train Loss: 0.1175 batch_loss: 0.149994\n",
      "train Loss: 0.1177 batch_loss: 0.150144\n",
      "train Loss: 0.1179 batch_loss: 0.150030\n",
      "train Loss: 0.1181 batch_loss: 0.151170\n",
      "train Loss: 0.1183 batch_loss: 0.150664\n",
      "train Loss: 0.1185 batch_loss: 0.150893\n",
      "train Loss: 0.1187 batch_loss: 0.150004\n",
      "train Loss: 0.1189 batch_loss: 0.150084\n",
      "train Loss: 0.1191 batch_loss: 0.150098\n",
      "train Loss: 0.1193 batch_loss: 0.150880\n",
      "train Loss: 0.1195 batch_loss: 0.151005\n",
      "train Loss: 0.1197 batch_loss: 0.150116\n",
      "train Loss: 0.1199 batch_loss: 0.151388\n",
      "train Loss: 0.1201 batch_loss: 0.152273\n",
      "train Loss: 0.1203 batch_loss: 0.150166\n",
      "train Loss: 0.1205 batch_loss: 0.149996\n",
      "train Loss: 0.1207 batch_loss: 0.151139\n",
      "train Loss: 0.1209 batch_loss: 0.149927\n",
      "train Loss: 0.1211 batch_loss: 0.150177\n",
      "train Loss: 0.1213 batch_loss: 0.151730\n",
      "train Loss: 0.1215 batch_loss: 0.151158\n",
      "train Loss: 0.1216 batch_loss: 0.150004\n",
      "train Loss: 0.1218 batch_loss: 0.149936\n",
      "train Loss: 0.1220 batch_loss: 0.154440\n",
      "train Loss: 0.1222 batch_loss: 0.150894\n",
      "train Loss: 0.1224 batch_loss: 0.150083\n",
      "train Loss: 0.1226 batch_loss: 0.150013\n",
      "train Loss: 0.1228 batch_loss: 0.150080\n",
      "train Loss: 0.1230 batch_loss: 0.154281\n",
      "train Loss: 0.1232 batch_loss: 0.150966\n",
      "train Loss: 0.1234 batch_loss: 0.153358\n",
      "train Loss: 0.1236 batch_loss: 0.151582\n",
      "train Loss: 0.1238 batch_loss: 0.151511\n",
      "train Loss: 0.1240 batch_loss: 0.150328\n",
      "train Loss: 0.1242 batch_loss: 0.149930\n",
      "train Loss: 0.1244 batch_loss: 0.156088\n",
      "train Loss: 0.1246 batch_loss: 0.153550\n",
      "train Loss: 0.1248 batch_loss: 0.149959\n",
      "train Loss: 0.1250 batch_loss: 0.150006\n",
      "train Loss: 0.1252 batch_loss: 0.149980\n",
      "train Loss: 0.1254 batch_loss: 0.149902\n",
      "train Loss: 0.1256 batch_loss: 0.153675\n",
      "train Loss: 0.1258 batch_loss: 0.153027\n",
      "train Loss: 0.1260 batch_loss: 0.149982\n",
      "train Loss: 0.1262 batch_loss: 0.150919\n",
      "train Loss: 0.1264 batch_loss: 0.150068\n",
      "train Loss: 0.1266 batch_loss: 0.153029\n",
      "train Loss: 0.1268 batch_loss: 0.149935\n",
      "train Loss: 0.1270 batch_loss: 0.150167\n",
      "train Loss: 0.1272 batch_loss: 0.156775\n",
      "train Loss: 0.1274 batch_loss: 0.150141\n",
      "train Loss: 0.1276 batch_loss: 0.154879\n",
      "train Loss: 0.1278 batch_loss: 0.149548\n",
      "train Loss: 0.1280 batch_loss: 0.154577\n",
      "train Loss: 0.1282 batch_loss: 0.154768\n",
      "train Loss: 0.1284 batch_loss: 0.149745\n",
      "train Loss: 0.1286 batch_loss: 0.149761\n",
      "train Loss: 0.1288 batch_loss: 0.149716\n",
      "train Loss: 0.1289 batch_loss: 0.149716\n",
      "train Loss: 0.1291 batch_loss: 0.150070\n",
      "train Loss: 0.1293 batch_loss: 0.149910\n",
      "train Loss: 0.1295 batch_loss: 0.150704\n",
      "train Loss: 0.1297 batch_loss: 0.149992\n",
      "train Loss: 0.1299 batch_loss: 0.151163\n",
      "train Loss: 0.1301 batch_loss: 0.151249\n",
      "train Loss: 0.1303 batch_loss: 0.151137\n",
      "train Loss: 0.1305 batch_loss: 0.150375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1307 batch_loss: 0.150517\n",
      "train Loss: 0.1309 batch_loss: 0.149954\n",
      "train Loss: 0.1311 batch_loss: 0.150014\n",
      "train Loss: 0.1313 batch_loss: 0.153593\n",
      "train Loss: 0.1315 batch_loss: 0.149803\n",
      "train Loss: 0.1317 batch_loss: 0.150010\n",
      "train Loss: 0.1319 batch_loss: 0.150157\n",
      "train Loss: 0.1321 batch_loss: 0.156021\n",
      "train Loss: 0.1323 batch_loss: 0.150259\n",
      "train Loss: 0.1325 batch_loss: 0.150768\n",
      "train Loss: 0.1327 batch_loss: 0.151055\n",
      "train Loss: 0.1329 batch_loss: 0.151096\n",
      "train Loss: 0.1331 batch_loss: 0.149877\n",
      "train Loss: 0.1333 batch_loss: 0.154673\n",
      "train Loss: 0.1335 batch_loss: 0.151603\n",
      "train Loss: 0.1337 batch_loss: 0.149815\n",
      "train Loss: 0.1339 batch_loss: 0.150129\n",
      "train Loss: 0.1341 batch_loss: 0.150250\n",
      "train Loss: 0.1343 batch_loss: 0.150059\n",
      "train Loss: 0.1345 batch_loss: 0.152536\n",
      "train Loss: 0.1346 batch_loss: 0.150293\n",
      "train Loss: 0.1348 batch_loss: 0.150127\n",
      "train Loss: 0.1350 batch_loss: 0.150058\n",
      "train Loss: 0.1352 batch_loss: 0.150070\n",
      "train Loss: 0.1354 batch_loss: 0.150225\n",
      "train Loss: 0.1356 batch_loss: 0.153260\n",
      "train Loss: 0.1358 batch_loss: 0.151124\n",
      "train Loss: 0.1360 batch_loss: 0.150596\n",
      "train Loss: 0.1362 batch_loss: 0.150037\n",
      "train Loss: 0.1364 batch_loss: 0.150008\n",
      "train Loss: 0.1366 batch_loss: 0.150167\n",
      "train Loss: 0.1368 batch_loss: 0.151091\n",
      "train Loss: 0.1370 batch_loss: 0.150070\n",
      "train Loss: 0.1372 batch_loss: 0.150055\n",
      "train Loss: 0.1374 batch_loss: 0.151010\n",
      "train Loss: 0.1376 batch_loss: 0.151190\n",
      "train Loss: 0.1378 batch_loss: 0.150866\n",
      "train Loss: 0.1380 batch_loss: 0.150872\n",
      "train Loss: 0.1382 batch_loss: 0.149886\n",
      "train Loss: 0.1384 batch_loss: 0.150327\n",
      "train Loss: 0.1386 batch_loss: 0.150001\n",
      "train Loss: 0.1388 batch_loss: 0.150016\n",
      "train Loss: 0.1390 batch_loss: 0.150570\n",
      "train Loss: 0.1392 batch_loss: 0.150039\n",
      "train Loss: 0.1394 batch_loss: 0.155867\n",
      "train Loss: 0.1396 batch_loss: 0.149885\n",
      "train Loss: 0.1397 batch_loss: 0.151135\n",
      "train Loss: 0.1399 batch_loss: 0.149916\n",
      "train Loss: 0.1401 batch_loss: 0.150121\n",
      "train Loss: 0.1403 batch_loss: 0.151039\n",
      "train Loss: 0.1405 batch_loss: 0.150261\n",
      "train Loss: 0.1407 batch_loss: 0.149987\n",
      "train Loss: 0.1409 batch_loss: 0.153830\n",
      "train Loss: 0.1411 batch_loss: 0.150031\n",
      "train Loss: 0.1413 batch_loss: 0.151249\n",
      "train Loss: 0.1415 batch_loss: 0.154703\n",
      "train Loss: 0.1417 batch_loss: 0.150938\n",
      "train Loss: 0.1419 batch_loss: 0.150896\n",
      "train Loss: 0.1421 batch_loss: 0.150915\n",
      "train Loss: 0.1423 batch_loss: 0.150886\n",
      "train Loss: 0.1425 batch_loss: 0.151108\n",
      "train Loss: 0.1427 batch_loss: 0.150023\n",
      "train Loss: 0.1429 batch_loss: 0.150171\n",
      "train Loss: 0.1431 batch_loss: 0.150143\n",
      "train Loss: 0.1433 batch_loss: 0.150139\n",
      "train Loss: 0.1435 batch_loss: 0.150135\n",
      "train Loss: 0.1437 batch_loss: 0.150565\n",
      "train Loss: 0.1439 batch_loss: 0.150148\n",
      "train Loss: 0.1441 batch_loss: 0.149882\n",
      "train Loss: 0.1443 batch_loss: 0.150054\n",
      "train Loss: 0.1445 batch_loss: 0.150000\n",
      "train Loss: 0.1447 batch_loss: 0.152614\n",
      "train Loss: 0.1449 batch_loss: 0.150100\n",
      "train Loss: 0.1450 batch_loss: 0.150086\n",
      "train Loss: 0.1452 batch_loss: 0.151073\n",
      "train Loss: 0.1454 batch_loss: 0.150856\n",
      "train Loss: 0.1456 batch_loss: 0.150252\n",
      "train Loss: 0.1458 batch_loss: 0.151213\n",
      "train Loss: 0.1460 batch_loss: 0.151022\n",
      "train Loss: 0.1462 batch_loss: 0.151561\n",
      "train Loss: 0.1464 batch_loss: 0.151007\n",
      "train Loss: 0.1466 batch_loss: 0.153552\n",
      "train Loss: 0.1468 batch_loss: 0.154301\n",
      "train Loss: 0.1470 batch_loss: 0.149928\n",
      "train Loss: 0.1472 batch_loss: 0.150731\n",
      "train Loss: 0.1474 batch_loss: 0.150118\n",
      "train Loss: 0.1476 batch_loss: 0.149968\n",
      "train Loss: 0.1478 batch_loss: 0.150817\n",
      "train Loss: 0.1480 batch_loss: 0.150193\n",
      "train Loss: 0.1482 batch_loss: 0.154459\n",
      "train Loss: 0.1484 batch_loss: 0.151374\n",
      "train Loss: 0.1486 batch_loss: 0.151491\n",
      "train Loss: 0.1488 batch_loss: 0.150941\n",
      "train Loss: 0.1490 batch_loss: 0.149728\n",
      "train Loss: 0.1492 batch_loss: 0.149538\n",
      "train Loss: 0.1494 batch_loss: 0.149571\n",
      "train Loss: 0.1496 batch_loss: 0.149858\n",
      "train Loss: 0.1498 batch_loss: 0.149860\n",
      "train Loss: 0.1500 batch_loss: 0.149887\n",
      "train Loss: 0.1502 batch_loss: 0.150779\n",
      "train Loss: 0.1504 batch_loss: 0.149911\n",
      "train Loss: 0.1505 batch_loss: 0.149919\n",
      "train Loss: 0.1507 batch_loss: 0.150921\n",
      "train Loss: 0.1509 batch_loss: 0.150420\n",
      "Loss on the test images: 0.15229 \n",
      "Epoch 2/9\n",
      "----------\n",
      "trainloader ready!\n",
      "testloader ready!\n",
      "train Loss: 0.0002 batch_loss: 0.150682\n",
      "train Loss: 0.0004 batch_loss: 0.150223\n",
      "train Loss: 0.0006 batch_loss: 0.149836\n",
      "train Loss: 0.0008 batch_loss: 0.151660\n",
      "train Loss: 0.0010 batch_loss: 0.150173\n",
      "train Loss: 0.0012 batch_loss: 0.154056\n",
      "train Loss: 0.0014 batch_loss: 0.153376\n",
      "train Loss: 0.0016 batch_loss: 0.150906\n",
      "train Loss: 0.0018 batch_loss: 0.151040\n",
      "train Loss: 0.0020 batch_loss: 0.150100\n",
      "train Loss: 0.0022 batch_loss: 0.150716\n",
      "train Loss: 0.0024 batch_loss: 0.151898\n",
      "train Loss: 0.0026 batch_loss: 0.150035\n",
      "train Loss: 0.0028 batch_loss: 0.150809\n",
      "train Loss: 0.0030 batch_loss: 0.150973\n",
      "train Loss: 0.0031 batch_loss: 0.149931\n",
      "train Loss: 0.0033 batch_loss: 0.151168\n",
      "train Loss: 0.0035 batch_loss: 0.151024\n",
      "train Loss: 0.0037 batch_loss: 0.150105\n",
      "train Loss: 0.0039 batch_loss: 0.151003\n",
      "train Loss: 0.0041 batch_loss: 0.150016\n",
      "train Loss: 0.0043 batch_loss: 0.149950\n",
      "train Loss: 0.0045 batch_loss: 0.149933\n",
      "train Loss: 0.0047 batch_loss: 0.150023\n",
      "train Loss: 0.0049 batch_loss: 0.149979\n",
      "train Loss: 0.0051 batch_loss: 0.149889\n",
      "train Loss: 0.0053 batch_loss: 0.149865\n",
      "train Loss: 0.0055 batch_loss: 0.149915\n",
      "train Loss: 0.0057 batch_loss: 0.154313\n",
      "train Loss: 0.0059 batch_loss: 0.149959\n",
      "train Loss: 0.0061 batch_loss: 0.150321\n",
      "train Loss: 0.0063 batch_loss: 0.151186\n",
      "train Loss: 0.0065 batch_loss: 0.154432\n",
      "train Loss: 0.0067 batch_loss: 0.151452\n",
      "train Loss: 0.0069 batch_loss: 0.149961\n",
      "train Loss: 0.0071 batch_loss: 0.149982\n",
      "train Loss: 0.0073 batch_loss: 0.149961\n",
      "train Loss: 0.0075 batch_loss: 0.154789\n",
      "train Loss: 0.0077 batch_loss: 0.149929\n",
      "train Loss: 0.0079 batch_loss: 0.149902\n",
      "train Loss: 0.0081 batch_loss: 0.150035\n",
      "train Loss: 0.0082 batch_loss: 0.150088\n",
      "train Loss: 0.0084 batch_loss: 0.150205\n",
      "train Loss: 0.0086 batch_loss: 0.151874\n",
      "train Loss: 0.0088 batch_loss: 0.149969\n",
      "train Loss: 0.0090 batch_loss: 0.151108\n",
      "train Loss: 0.0092 batch_loss: 0.149912\n",
      "train Loss: 0.0094 batch_loss: 0.155216\n",
      "train Loss: 0.0096 batch_loss: 0.151010\n",
      "train Loss: 0.0098 batch_loss: 0.150025\n",
      "train Loss: 0.0100 batch_loss: 0.151048\n",
      "train Loss: 0.0102 batch_loss: 0.150166\n",
      "train Loss: 0.0104 batch_loss: 0.151162\n",
      "train Loss: 0.0106 batch_loss: 0.150726\n",
      "train Loss: 0.0108 batch_loss: 0.150068\n",
      "train Loss: 0.0110 batch_loss: 0.150054\n",
      "train Loss: 0.0112 batch_loss: 0.151032\n",
      "train Loss: 0.0114 batch_loss: 0.149887\n",
      "train Loss: 0.0116 batch_loss: 0.149888\n",
      "train Loss: 0.0118 batch_loss: 0.152800\n",
      "train Loss: 0.0120 batch_loss: 0.149918\n",
      "train Loss: 0.0122 batch_loss: 0.149988\n",
      "train Loss: 0.0124 batch_loss: 0.153313\n",
      "train Loss: 0.0126 batch_loss: 0.149947\n",
      "train Loss: 0.0128 batch_loss: 0.151206\n",
      "train Loss: 0.0130 batch_loss: 0.151806\n",
      "train Loss: 0.0132 batch_loss: 0.150964\n",
      "train Loss: 0.0134 batch_loss: 0.154418\n",
      "train Loss: 0.0136 batch_loss: 0.150022\n",
      "train Loss: 0.0138 batch_loss: 0.155841\n",
      "train Loss: 0.0140 batch_loss: 0.150289\n",
      "train Loss: 0.0142 batch_loss: 0.150108\n",
      "train Loss: 0.0143 batch_loss: 0.149656\n",
      "train Loss: 0.0145 batch_loss: 0.150384\n",
      "train Loss: 0.0147 batch_loss: 0.154161\n",
      "train Loss: 0.0149 batch_loss: 0.149562\n",
      "train Loss: 0.0151 batch_loss: 0.151042\n",
      "train Loss: 0.0153 batch_loss: 0.149976\n",
      "train Loss: 0.0155 batch_loss: 0.149929\n",
      "train Loss: 0.0157 batch_loss: 0.151129\n",
      "train Loss: 0.0159 batch_loss: 0.150257\n",
      "train Loss: 0.0161 batch_loss: 0.150297\n",
      "train Loss: 0.0163 batch_loss: 0.150592\n",
      "train Loss: 0.0165 batch_loss: 0.150092\n",
      "train Loss: 0.0167 batch_loss: 0.150051\n",
      "train Loss: 0.0169 batch_loss: 0.153610\n",
      "train Loss: 0.0171 batch_loss: 0.151029\n",
      "train Loss: 0.0173 batch_loss: 0.152636\n",
      "train Loss: 0.0175 batch_loss: 0.150047\n",
      "train Loss: 0.0177 batch_loss: 0.153202\n",
      "train Loss: 0.0179 batch_loss: 0.150056\n",
      "train Loss: 0.0181 batch_loss: 0.150167\n",
      "train Loss: 0.0183 batch_loss: 0.150635\n",
      "train Loss: 0.0185 batch_loss: 0.150398\n",
      "train Loss: 0.0187 batch_loss: 0.154978\n",
      "train Loss: 0.0189 batch_loss: 0.150125\n",
      "train Loss: 0.0191 batch_loss: 0.149897\n",
      "train Loss: 0.0193 batch_loss: 0.149970\n",
      "train Loss: 0.0195 batch_loss: 0.149889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0197 batch_loss: 0.150098\n",
      "train Loss: 0.0199 batch_loss: 0.151773\n",
      "train Loss: 0.0200 batch_loss: 0.150782\n",
      "train Loss: 0.0202 batch_loss: 0.149864\n",
      "train Loss: 0.0204 batch_loss: 0.149896\n",
      "train Loss: 0.0206 batch_loss: 0.150077\n",
      "train Loss: 0.0208 batch_loss: 0.150031\n",
      "train Loss: 0.0210 batch_loss: 0.151396\n",
      "train Loss: 0.0212 batch_loss: 0.152875\n",
      "train Loss: 0.0214 batch_loss: 0.150722\n",
      "train Loss: 0.0216 batch_loss: 0.150154\n",
      "train Loss: 0.0218 batch_loss: 0.153878\n",
      "train Loss: 0.0220 batch_loss: 0.151893\n",
      "train Loss: 0.0222 batch_loss: 0.150506\n",
      "train Loss: 0.0224 batch_loss: 0.149995\n",
      "train Loss: 0.0226 batch_loss: 0.149976\n",
      "train Loss: 0.0228 batch_loss: 0.150794\n",
      "train Loss: 0.0230 batch_loss: 0.152571\n",
      "train Loss: 0.0232 batch_loss: 0.149923\n",
      "train Loss: 0.0234 batch_loss: 0.149922\n",
      "train Loss: 0.0236 batch_loss: 0.150273\n",
      "train Loss: 0.0238 batch_loss: 0.149887\n",
      "train Loss: 0.0240 batch_loss: 0.150768\n",
      "train Loss: 0.0242 batch_loss: 0.151448\n",
      "train Loss: 0.0244 batch_loss: 0.150568\n",
      "train Loss: 0.0246 batch_loss: 0.150137\n",
      "train Loss: 0.0248 batch_loss: 0.150053\n",
      "train Loss: 0.0250 batch_loss: 0.150360\n",
      "train Loss: 0.0252 batch_loss: 0.152459\n",
      "train Loss: 0.0253 batch_loss: 0.149980\n",
      "train Loss: 0.0255 batch_loss: 0.149966\n",
      "train Loss: 0.0257 batch_loss: 0.152106\n",
      "train Loss: 0.0259 batch_loss: 0.153338\n",
      "train Loss: 0.0261 batch_loss: 0.150639\n",
      "train Loss: 0.0263 batch_loss: 0.151676\n",
      "train Loss: 0.0265 batch_loss: 0.150001\n",
      "train Loss: 0.0267 batch_loss: 0.149955\n",
      "train Loss: 0.0269 batch_loss: 0.150604\n",
      "train Loss: 0.0271 batch_loss: 0.149928\n",
      "train Loss: 0.0273 batch_loss: 0.149957\n",
      "train Loss: 0.0275 batch_loss: 0.154975\n",
      "train Loss: 0.0277 batch_loss: 0.149984\n",
      "train Loss: 0.0279 batch_loss: 0.150880\n",
      "train Loss: 0.0281 batch_loss: 0.151018\n",
      "train Loss: 0.0283 batch_loss: 0.155318\n",
      "train Loss: 0.0285 batch_loss: 0.153959\n",
      "train Loss: 0.0287 batch_loss: 0.151180\n",
      "train Loss: 0.0289 batch_loss: 0.150003\n",
      "train Loss: 0.0291 batch_loss: 0.150347\n",
      "train Loss: 0.0293 batch_loss: 0.149559\n",
      "train Loss: 0.0295 batch_loss: 0.149601\n",
      "train Loss: 0.0297 batch_loss: 0.149545\n",
      "train Loss: 0.0299 batch_loss: 0.149603\n",
      "train Loss: 0.0301 batch_loss: 0.150074\n",
      "train Loss: 0.0303 batch_loss: 0.150013\n",
      "train Loss: 0.0305 batch_loss: 0.149929\n",
      "train Loss: 0.0307 batch_loss: 0.149913\n",
      "train Loss: 0.0308 batch_loss: 0.150539\n",
      "train Loss: 0.0310 batch_loss: 0.150844\n",
      "train Loss: 0.0312 batch_loss: 0.153032\n",
      "train Loss: 0.0314 batch_loss: 0.150058\n",
      "train Loss: 0.0316 batch_loss: 0.149966\n",
      "train Loss: 0.0318 batch_loss: 0.149950\n",
      "train Loss: 0.0320 batch_loss: 0.150370\n",
      "train Loss: 0.0322 batch_loss: 0.151135\n",
      "train Loss: 0.0324 batch_loss: 0.150889\n",
      "train Loss: 0.0326 batch_loss: 0.151038\n",
      "train Loss: 0.0328 batch_loss: 0.149896\n",
      "train Loss: 0.0330 batch_loss: 0.154191\n",
      "train Loss: 0.0332 batch_loss: 0.156015\n",
      "train Loss: 0.0334 batch_loss: 0.149923\n",
      "train Loss: 0.0336 batch_loss: 0.150081\n",
      "train Loss: 0.0338 batch_loss: 0.151159\n",
      "train Loss: 0.0340 batch_loss: 0.151824\n",
      "train Loss: 0.0342 batch_loss: 0.149914\n",
      "train Loss: 0.0344 batch_loss: 0.149922\n",
      "train Loss: 0.0346 batch_loss: 0.150244\n",
      "train Loss: 0.0348 batch_loss: 0.149956\n",
      "train Loss: 0.0350 batch_loss: 0.149994\n",
      "train Loss: 0.0352 batch_loss: 0.149838\n",
      "train Loss: 0.0354 batch_loss: 0.149958\n",
      "train Loss: 0.0356 batch_loss: 0.149897\n",
      "train Loss: 0.0358 batch_loss: 0.151303\n",
      "train Loss: 0.0360 batch_loss: 0.149874\n",
      "train Loss: 0.0361 batch_loss: 0.149882\n",
      "train Loss: 0.0363 batch_loss: 0.150180\n",
      "train Loss: 0.0365 batch_loss: 0.151200\n",
      "train Loss: 0.0367 batch_loss: 0.150026\n",
      "train Loss: 0.0369 batch_loss: 0.153927\n",
      "train Loss: 0.0371 batch_loss: 0.150779\n",
      "train Loss: 0.0373 batch_loss: 0.151004\n",
      "train Loss: 0.0375 batch_loss: 0.150076\n",
      "train Loss: 0.0377 batch_loss: 0.149957\n",
      "train Loss: 0.0379 batch_loss: 0.155883\n",
      "train Loss: 0.0381 batch_loss: 0.150777\n",
      "train Loss: 0.0383 batch_loss: 0.149938\n",
      "train Loss: 0.0385 batch_loss: 0.150040\n",
      "train Loss: 0.0387 batch_loss: 0.149988\n",
      "train Loss: 0.0389 batch_loss: 0.150724\n",
      "train Loss: 0.0391 batch_loss: 0.156192\n",
      "train Loss: 0.0393 batch_loss: 0.149945\n",
      "train Loss: 0.0395 batch_loss: 0.156837\n",
      "train Loss: 0.0397 batch_loss: 0.152731\n",
      "train Loss: 0.0399 batch_loss: 0.150078\n",
      "train Loss: 0.0401 batch_loss: 0.150204\n",
      "train Loss: 0.0403 batch_loss: 0.150223\n",
      "train Loss: 0.0405 batch_loss: 0.150058\n",
      "train Loss: 0.0407 batch_loss: 0.150747\n",
      "train Loss: 0.0409 batch_loss: 0.150022\n",
      "train Loss: 0.0411 batch_loss: 0.150727\n",
      "train Loss: 0.0413 batch_loss: 0.151219\n",
      "train Loss: 0.0415 batch_loss: 0.150465\n",
      "train Loss: 0.0417 batch_loss: 0.150043\n",
      "train Loss: 0.0419 batch_loss: 0.150003\n",
      "train Loss: 0.0421 batch_loss: 0.150457\n",
      "train Loss: 0.0422 batch_loss: 0.149881\n",
      "train Loss: 0.0424 batch_loss: 0.149932\n",
      "train Loss: 0.0426 batch_loss: 0.150251\n",
      "train Loss: 0.0428 batch_loss: 0.150161\n",
      "train Loss: 0.0430 batch_loss: 0.149852\n",
      "train Loss: 0.0432 batch_loss: 0.155607\n",
      "train Loss: 0.0434 batch_loss: 0.149523\n",
      "train Loss: 0.0436 batch_loss: 0.149588\n",
      "train Loss: 0.0438 batch_loss: 0.149695\n",
      "train Loss: 0.0440 batch_loss: 0.150415\n",
      "train Loss: 0.0442 batch_loss: 0.150089\n",
      "train Loss: 0.0444 batch_loss: 0.149953\n",
      "train Loss: 0.0446 batch_loss: 0.151493\n",
      "train Loss: 0.0448 batch_loss: 0.151221\n",
      "train Loss: 0.0450 batch_loss: 0.151138\n",
      "train Loss: 0.0452 batch_loss: 0.149961\n",
      "train Loss: 0.0454 batch_loss: 0.150018\n",
      "train Loss: 0.0456 batch_loss: 0.149990\n",
      "train Loss: 0.0458 batch_loss: 0.149948\n",
      "train Loss: 0.0460 batch_loss: 0.150029\n",
      "train Loss: 0.0462 batch_loss: 0.149983\n",
      "train Loss: 0.0464 batch_loss: 0.149956\n",
      "train Loss: 0.0466 batch_loss: 0.155216\n",
      "train Loss: 0.0468 batch_loss: 0.149872\n",
      "train Loss: 0.0470 batch_loss: 0.150110\n",
      "train Loss: 0.0472 batch_loss: 0.150927\n",
      "train Loss: 0.0473 batch_loss: 0.150059\n",
      "train Loss: 0.0475 batch_loss: 0.149997\n",
      "train Loss: 0.0477 batch_loss: 0.151256\n",
      "train Loss: 0.0479 batch_loss: 0.151275\n",
      "train Loss: 0.0481 batch_loss: 0.149918\n",
      "train Loss: 0.0483 batch_loss: 0.149933\n",
      "train Loss: 0.0485 batch_loss: 0.149946\n",
      "train Loss: 0.0487 batch_loss: 0.150334\n",
      "train Loss: 0.0489 batch_loss: 0.151053\n",
      "train Loss: 0.0491 batch_loss: 0.152021\n",
      "train Loss: 0.0493 batch_loss: 0.150116\n",
      "train Loss: 0.0495 batch_loss: 0.150172\n",
      "train Loss: 0.0497 batch_loss: 0.150030\n",
      "train Loss: 0.0499 batch_loss: 0.150103\n",
      "train Loss: 0.0501 batch_loss: 0.155607\n",
      "train Loss: 0.0503 batch_loss: 0.150029\n",
      "train Loss: 0.0505 batch_loss: 0.156649\n",
      "train Loss: 0.0507 batch_loss: 0.150017\n",
      "train Loss: 0.0509 batch_loss: 0.151196\n",
      "train Loss: 0.0511 batch_loss: 0.150028\n",
      "train Loss: 0.0513 batch_loss: 0.150022\n",
      "train Loss: 0.0515 batch_loss: 0.153615\n",
      "train Loss: 0.0517 batch_loss: 0.149903\n",
      "train Loss: 0.0519 batch_loss: 0.149989\n",
      "train Loss: 0.0521 batch_loss: 0.153351\n",
      "train Loss: 0.0523 batch_loss: 0.153660\n",
      "train Loss: 0.0525 batch_loss: 0.149971\n",
      "train Loss: 0.0527 batch_loss: 0.154396\n",
      "train Loss: 0.0529 batch_loss: 0.150360\n",
      "train Loss: 0.0531 batch_loss: 0.156005\n",
      "train Loss: 0.0533 batch_loss: 0.149964\n",
      "train Loss: 0.0535 batch_loss: 0.151412\n",
      "train Loss: 0.0537 batch_loss: 0.149891\n",
      "train Loss: 0.0538 batch_loss: 0.149924\n",
      "train Loss: 0.0540 batch_loss: 0.149911\n",
      "train Loss: 0.0542 batch_loss: 0.150046\n",
      "train Loss: 0.0544 batch_loss: 0.149859\n",
      "train Loss: 0.0546 batch_loss: 0.149900\n",
      "train Loss: 0.0548 batch_loss: 0.149861\n",
      "train Loss: 0.0550 batch_loss: 0.149965\n",
      "train Loss: 0.0552 batch_loss: 0.150186\n",
      "train Loss: 0.0554 batch_loss: 0.153573\n",
      "train Loss: 0.0556 batch_loss: 0.149920\n",
      "train Loss: 0.0558 batch_loss: 0.150088\n",
      "train Loss: 0.0560 batch_loss: 0.149939\n",
      "train Loss: 0.0562 batch_loss: 0.152104\n",
      "train Loss: 0.0564 batch_loss: 0.149925\n",
      "train Loss: 0.0566 batch_loss: 0.150051\n",
      "train Loss: 0.0568 batch_loss: 0.152762\n",
      "train Loss: 0.0570 batch_loss: 0.150079\n",
      "train Loss: 0.0572 batch_loss: 0.150801\n",
      "train Loss: 0.0574 batch_loss: 0.154025\n",
      "train Loss: 0.0576 batch_loss: 0.150001\n",
      "train Loss: 0.0578 batch_loss: 0.150073\n",
      "train Loss: 0.0580 batch_loss: 0.151207\n",
      "train Loss: 0.0582 batch_loss: 0.150942\n",
      "train Loss: 0.0584 batch_loss: 0.149969\n",
      "train Loss: 0.0586 batch_loss: 0.150092\n",
      "train Loss: 0.0587 batch_loss: 0.149915\n",
      "train Loss: 0.0589 batch_loss: 0.149952\n",
      "train Loss: 0.0591 batch_loss: 0.151001\n",
      "train Loss: 0.0593 batch_loss: 0.150051\n",
      "train Loss: 0.0595 batch_loss: 0.151318\n",
      "train Loss: 0.0597 batch_loss: 0.150088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0599 batch_loss: 0.149976\n",
      "train Loss: 0.0601 batch_loss: 0.150095\n",
      "train Loss: 0.0603 batch_loss: 0.151993\n",
      "train Loss: 0.0605 batch_loss: 0.150635\n",
      "train Loss: 0.0607 batch_loss: 0.153561\n",
      "train Loss: 0.0609 batch_loss: 0.150160\n",
      "train Loss: 0.0611 batch_loss: 0.150203\n",
      "train Loss: 0.0613 batch_loss: 0.150001\n",
      "train Loss: 0.0615 batch_loss: 0.149978\n",
      "train Loss: 0.0617 batch_loss: 0.151709\n",
      "train Loss: 0.0619 batch_loss: 0.149886\n",
      "train Loss: 0.0621 batch_loss: 0.154005\n",
      "train Loss: 0.0623 batch_loss: 0.155077\n",
      "train Loss: 0.0625 batch_loss: 0.150785\n",
      "train Loss: 0.0627 batch_loss: 0.151157\n",
      "train Loss: 0.0629 batch_loss: 0.153665\n",
      "train Loss: 0.0631 batch_loss: 0.151116\n",
      "train Loss: 0.0633 batch_loss: 0.150326\n",
      "train Loss: 0.0635 batch_loss: 0.154835\n",
      "train Loss: 0.0637 batch_loss: 0.149979\n",
      "train Loss: 0.0639 batch_loss: 0.151641\n",
      "train Loss: 0.0641 batch_loss: 0.150905\n",
      "train Loss: 0.0643 batch_loss: 0.149988\n",
      "train Loss: 0.0645 batch_loss: 0.152228\n",
      "train Loss: 0.0647 batch_loss: 0.149946\n",
      "train Loss: 0.0649 batch_loss: 0.150346\n",
      "train Loss: 0.0650 batch_loss: 0.150130\n",
      "train Loss: 0.0652 batch_loss: 0.149917\n",
      "train Loss: 0.0654 batch_loss: 0.149889\n",
      "train Loss: 0.0656 batch_loss: 0.152388\n",
      "train Loss: 0.0658 batch_loss: 0.149895\n",
      "train Loss: 0.0660 batch_loss: 0.150771\n",
      "train Loss: 0.0662 batch_loss: 0.156428\n",
      "train Loss: 0.0664 batch_loss: 0.154157\n",
      "train Loss: 0.0666 batch_loss: 0.150141\n",
      "train Loss: 0.0668 batch_loss: 0.150052\n",
      "train Loss: 0.0670 batch_loss: 0.150216\n",
      "train Loss: 0.0672 batch_loss: 0.153544\n",
      "train Loss: 0.0674 batch_loss: 0.151039\n",
      "train Loss: 0.0676 batch_loss: 0.150098\n",
      "train Loss: 0.0678 batch_loss: 0.150107\n",
      "train Loss: 0.0680 batch_loss: 0.150072\n",
      "train Loss: 0.0682 batch_loss: 0.150174\n",
      "train Loss: 0.0684 batch_loss: 0.150147\n",
      "train Loss: 0.0686 batch_loss: 0.150186\n",
      "train Loss: 0.0688 batch_loss: 0.150001\n",
      "train Loss: 0.0690 batch_loss: 0.150668\n",
      "train Loss: 0.0692 batch_loss: 0.150773\n",
      "train Loss: 0.0694 batch_loss: 0.150111\n",
      "train Loss: 0.0696 batch_loss: 0.150127\n",
      "train Loss: 0.0698 batch_loss: 0.149970\n",
      "train Loss: 0.0700 batch_loss: 0.150240\n",
      "train Loss: 0.0702 batch_loss: 0.150763\n",
      "train Loss: 0.0703 batch_loss: 0.149924\n",
      "train Loss: 0.0705 batch_loss: 0.149911\n",
      "train Loss: 0.0707 batch_loss: 0.153537\n",
      "train Loss: 0.0709 batch_loss: 0.151509\n",
      "train Loss: 0.0711 batch_loss: 0.155063\n",
      "train Loss: 0.0713 batch_loss: 0.151022\n",
      "train Loss: 0.0715 batch_loss: 0.150013\n",
      "train Loss: 0.0717 batch_loss: 0.152484\n",
      "train Loss: 0.0719 batch_loss: 0.153113\n",
      "train Loss: 0.0721 batch_loss: 0.150139\n",
      "train Loss: 0.0723 batch_loss: 0.149957\n",
      "train Loss: 0.0725 batch_loss: 0.150131\n",
      "train Loss: 0.0727 batch_loss: 0.149917\n",
      "train Loss: 0.0729 batch_loss: 0.150535\n",
      "train Loss: 0.0731 batch_loss: 0.150696\n",
      "train Loss: 0.0733 batch_loss: 0.151216\n",
      "train Loss: 0.0735 batch_loss: 0.154337\n",
      "train Loss: 0.0737 batch_loss: 0.149945\n",
      "train Loss: 0.0739 batch_loss: 0.149989\n",
      "train Loss: 0.0741 batch_loss: 0.151266\n"
     ]
    }
   ],
   "source": [
    "# transfer learning resnet18\n",
    "step_size = 20\n",
    "model = ConvLSTM(input_size=(128,128),\n",
    "                 input_dim=1,\n",
    "                 hidden_dim=[64, 64, 128],\n",
    "                 kernel_size=(3, 3),\n",
    "                 num_layers=3,\n",
    "                 predict_steps=int(step_size/2),\n",
    "                 batch_first=True,\n",
    "                 bias=True,\n",
    "                 return_all_layers=True).cuda()\n",
    "\n",
    "if use_gpu:\n",
    "#     encoder = torch.nn.DataParallel(encoder)\n",
    "#     decoder = torch.nn.DataParallel(decoder)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=1, gamma=0.1)\n",
    "\n",
    "# train model\n",
    "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, batch_size = 1, step_size = 20, num_epochs=10)\n",
    "torch.save(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset = ansimDataset(img_list_csv = img_list_csv, seq_csv = test_csv, root_dir = img_path, step=step_size, random_rotate = True, mask = mask, transform=None)\n",
    "# testloader = torch.utils.data.DataLoader(testset,\n",
    "#                                              batch_size=1, shuffle=True,\n",
    "#                                              num_workers=1)\n",
    "# dataiter = iter(testloader)\n",
    "# data = dataiter.next()\n",
    "# data_split = torch.split(data, int(data.shape[1]/2), dim=1)\n",
    "# inputs = data_split[0]\n",
    "# target = data_split[1]\n",
    "# outputs = model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "input = Variable(torch.randn(3, 1, 512, 64, 32)).cuda()\n",
    "a = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 512, 64, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
