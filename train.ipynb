{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import PIL\n",
    "import torch, torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from ansim_dataset import ansimDataset, create_circular_mask\n",
    "# from convolution_lstm import encoderConvLSTM, decoderConvLSTM\n",
    "from ConvLSTM import ConvLSTM\n",
    "import random\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1, 2,3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/rliu/ansim/data/data/JPEGImages/'\n",
    "img_list_csv = '/home/rliu/github/ansim/img_list.csv'\n",
    "train_csv = '/home/rliu/github/ansim/train.csv'\n",
    "test_csv = '/home/rliu/github/ansim/test.csv'\n",
    "output_path = '/home/rliu/ansim/models/very_first.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = create_circular_mask(128,128)\n",
    "trainset = ansimDataset(img_list_csv = img_list_csv, seq_csv = train_csv, root_dir = img_path, step=20, random_rotate = True, mask = mask, transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                             batch_size=8, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "\n",
    "testset = ansimDataset(img_list_csv = img_list_csv, seq_csv = test_csv, root_dir = img_path, step=20, random_rotate = True, mask = mask, transform=None)\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                             batch_size=8, shuffle=True,\n",
    "                                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU in use\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"GPU in use\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(encoder, decoder, criterion, optimizer, scheduler, num_epochs=25):\n",
    "#     since = time.time()\n",
    "\n",
    "#     best_encoder_wts = encoder.state_dict()\n",
    "#     best_decoder_wts = decoder.state_dict()\n",
    "#     best_acc = 0.0\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "#         print('-' * 10)\n",
    "\n",
    "#         # Each epoch has a training phase\n",
    "#         scheduler.step()\n",
    "#         encoder.train(True)  # Set model to training mode\n",
    "#         decoder.train(True)  # Set model to training mode\n",
    "#         running_loss = 0.0\n",
    "#         running_corrects = 0\n",
    "\n",
    "#         # Iterate over data.\n",
    "#         trainset = ansimDataset(img_list_csv = img_list_csv, seq_csv = train_csv, root_dir = img_path, step=20, random_rotate = True, mask = mask, transform=None)\n",
    "#         trainloader = torch.utils.data.DataLoader(trainset,\n",
    "#                                                      batch_size=1, shuffle=True,\n",
    "#                                                      num_workers=4)\n",
    "\n",
    "#         print(\"trainloader ready!\")\n",
    "#         testset = ansimDataset(img_list_csv = img_list_csv, seq_csv = test_csv, root_dir = img_path, step=20, random_rotate = True, mask = mask, transform=None)\n",
    "#         testloader = torch.utils.data.DataLoader(testset,\n",
    "#                                                      batch_size=1, shuffle=True,\n",
    "#                                                      num_workers=4)\n",
    "#         print(\"testloader ready!\")\n",
    "        \n",
    "#         for data in trainloader:\n",
    "#             # get the inputs\n",
    "#             data_split = torch.split(data, 10, dim=1)\n",
    "#             inputs = data_split[0]\n",
    "#             target = data_split[1]\n",
    "# #            print(inputs)\n",
    "#             # wrap them in Variable\n",
    "#             if use_gpu:\n",
    "# #                inputs = Variable(inputs.cuda())\n",
    "# #                labels = Variable(labels.cuda())\n",
    "# #                inputs = torch.nn.DataParallel(inputs, device_ids=[0, 1]).cuda()\n",
    "# #                labels = torch.nn.DataParallel(labels, device_ids=[0, 1]).cuda()\n",
    "#                 inputs, target = inputs.to(device), target.to(device)\n",
    "# #                print(inputs)\n",
    "#             else:\n",
    "#                 inputs, target = Variable(inputs), Variable(target)\n",
    "\n",
    "#             # zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             output, h, c, states = encoder(inputs)\n",
    "#             output_last = output[0][0].double()\n",
    "#             h1,h2,h3,h4,h5 = states[0][0],states[1][0],states[2][0],states[3][0],states[4][0]\n",
    "#             c1,c2,c3,c4,c5 = states[0][1],states[1][1],states[2][1],states[3][1],states[4][1]\n",
    "#             states_cat = torch.cat((h1,h2,h3,h4,h5,c1,c2,c3,c4,c5), dim=1, out=None)\n",
    "#             x = decoder.activateConv(states_cat)\n",
    "#             input_d = [x, states]\n",
    "#             output_d, h_d, c_d, states_d = decoder(input_d)\n",
    "#             predicted = torch.cat(output_d, dim=0, out=None).double()\n",
    "            \n",
    "            \n",
    "#             loss = criterion(predicted, target)\n",
    "#             # forward\n",
    "# #            outputs = model(inputs)\n",
    "# #            _, preds = torch.max(outputs.data, 1)\n",
    "# #            loss = criterion(outputs, labels)\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # statistics\n",
    "#             iter_loss = loss.item()\n",
    "#             running_loss += loss.item()    \n",
    "#             epoch_loss = running_loss / len(trainset)\n",
    "            \n",
    "#             print('{} Loss: {:.4f} batch_loss: {:d}'.format(\n",
    "#                 \"train\", epoch_loss, iter_loss))\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for data in testloader:\n",
    "#                 data_split = torch.split(data, 10, dim=1)\n",
    "#                 inputs = data_split[0]\n",
    "#                 target = data_split[1]\n",
    "                \n",
    "#                 if use_gpu:\n",
    "#                     inputs, target = inputs.to(device), target.to(device)\n",
    "#                 else:\n",
    "#                     inputs, target = Variable(inputs), Variable(target)\n",
    "\n",
    "                    \n",
    "#                 output, h, c, states = encoder(inputs)\n",
    "#                 output_last = output[0][0].double()\n",
    "#                 h1,h2,h3,h4,h5 = states[0][0],states[1][0],states[2][0],states[3][0],states[4][0]\n",
    "#                 c1,c2,c3,c4,c5 = states[0][1],states[1][1],states[2][1],states[3][1],states[4][1]\n",
    "#                 states_cat = torch.cat((h1,h2,h3,h4,h5,c1,c2,c3,c4,c5), dim=1, out=None)\n",
    "#                 x = decoder.activateConv(states_cat)\n",
    "#                 input_d = [x, states]\n",
    "#                 output_d, h_d, c_d, states_d = decoder(input_d)\n",
    "#                 predicted = torch.cat(output_d, dim=0, out=None).double()\n",
    "            \n",
    "#                 loss_test = criterion(predicted, target)\n",
    "#                 iter_loss_test = loss_test.item()\n",
    "#                 running_loss_test += loss_test.item()    \n",
    "#                 epoch_loss_test = running_loss_test / len(testset)\n",
    "\n",
    "#         print('Loss on the test images: %.5f %%' % (\n",
    "#             epoch_loss_test))\n",
    "        \n",
    "\n",
    "#     time_elapsed = time.time() - since\n",
    "#     print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "#         time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "#     # load best model weights\n",
    "#     encoder.load_state_dict(best_encoder_wts)\n",
    "#     decoder.load_state_dict(best_decoder_wts)\n",
    "#     return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, batch_size = 4, step_size = 20):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training phase\n",
    "        scheduler.step()\n",
    "        model.train(True)  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "\n",
    "        # Iterate over data.\n",
    "        trainset = ansimDataset(img_list_csv = img_list_csv, seq_csv = train_csv, root_dir = img_path, step=step_size, random_rotate = True, mask = mask, transform=None)\n",
    "        trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                                     batch_size=batch_size, shuffle=True,\n",
    "                                                     num_workers=4)\n",
    "\n",
    "        print(\"trainloader ready!\")\n",
    "        testset = ansimDataset(img_list_csv = img_list_csv, seq_csv = test_csv, root_dir = img_path, step=step_size, random_rotate = True, mask = mask, transform=None)\n",
    "        testloader = torch.utils.data.DataLoader(testset,\n",
    "                                                     batch_size=batch_size, shuffle=True,\n",
    "                                                     num_workers=4)\n",
    "        print(\"testloader ready!\")\n",
    "        \n",
    "        for data in trainloader:\n",
    "            # get the inputs\n",
    "            data_split = torch.split(data, int(data.shape[1]/2), dim=1)\n",
    "            inputs = data_split[0]\n",
    "            target = data_split[1]\n",
    "#            print(inputs)\n",
    "            # wrap them in Variable\n",
    "            if use_gpu:\n",
    "                inputs, target = inputs.to(device), target.to(device)\n",
    "            else:\n",
    "                inputs, target = Variable(inputs), Variable(target)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "#             output, h, c, states = encoder(inputs)\n",
    "#             output_last = output[0][0].double()\n",
    "#             h1,h2,h3,h4,h5 = states[0][0],states[1][0],states[2][0],states[3][0],states[4][0]\n",
    "#             c1,c2,c3,c4,c5 = states[0][1],states[1][1],states[2][1],states[3][1],states[4][1]\n",
    "#             states_cat = torch.cat((h1,h2,h3,h4,h5,c1,c2,c3,c4,c5), dim=1, out=None)\n",
    "#             x = decoder.activateConv(states_cat)\n",
    "#             input_d = [x, states]\n",
    "#             output_d, h_d, c_d, states_d = decoder(input_d)\n",
    "#             predicted = torch.cat(output_d, dim=0, out=None).double()\n",
    "            \n",
    "            _, _, predicted = model(inputs)\n",
    "            \n",
    "            loss = criterion(predicted, target)\n",
    "            # forward\n",
    "#            outputs = model(inputs)\n",
    "#            _, preds = torch.max(outputs.data, 1)\n",
    "#            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            iter_loss = loss.item()\n",
    "            running_loss += loss.item()    \n",
    "            epoch_loss = running_loss / len(trainset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} batch_loss: {:f}'.format(\n",
    "                \"train\", epoch_loss, iter_loss))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_loss_test = 0.0\n",
    "            for data in testloader:\n",
    "                data_split = torch.split(data, int(data.shape[1]/2), dim=1)\n",
    "                inputs = data_split[0]\n",
    "                target = data_split[1]\n",
    "                \n",
    "                \n",
    "                if use_gpu:\n",
    "                    inputs, target = inputs.to(device), target.to(device)\n",
    "                else:\n",
    "                    inputs, target = Variable(inputs), Variable(target)\n",
    "\n",
    "                    \n",
    "#                 output, h, c, states = encoder(inputs)\n",
    "#                 output_last = output[0][0].double()\n",
    "#                 h1,h2,h3,h4,h5 = states[0][0],states[1][0],states[2][0],states[3][0],states[4][0]\n",
    "#                 c1,c2,c3,c4,c5 = states[0][1],states[1][1],states[2][1],states[3][1],states[4][1]\n",
    "#                 states_cat = torch.cat((h1,h2,h3,h4,h5,c1,c2,c3,c4,c5), dim=1, out=None)\n",
    "#                 x = decoder.activateConv(states_cat)\n",
    "#                 input_d = [x, states]\n",
    "#                 output_d, h_d, c_d, states_d = decoder(input_d)\n",
    "#                 predicted = torch.cat(output_d, dim=0, out=None).double()\n",
    "\n",
    "                _, _, predicted = model(inputs)\n",
    "                \n",
    "                loss_test = criterion(predicted, target)\n",
    "                iter_loss_test = loss_test.item()\n",
    "                running_loss_test += loss_test.item()    \n",
    "                epoch_loss_test = running_loss_test / len(testset)\n",
    "\n",
    "        print('Loss on the test images: %.5f ' % (\n",
    "            epoch_loss_test))\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "trainloader ready!\n",
      "testloader ready!\n",
      "train Loss: 17.5539 batch_loss: 13481.406250\n",
      "train Loss: 30.6795 batch_loss: 10080.424805\n",
      "train Loss: 47.1266 batch_loss: 12631.360352\n",
      "train Loss: 60.4075 batch_loss: 10199.760742\n",
      "train Loss: 70.3738 batch_loss: 7654.120117\n",
      "train Loss: 81.0404 batch_loss: 8191.975098\n",
      "train Loss: 91.6079 batch_loss: 8115.784180\n",
      "train Loss: 102.4019 batch_loss: 8289.830078\n",
      "train Loss: 112.6105 batch_loss: 7840.217773\n",
      "train Loss: 126.1679 batch_loss: 10412.104492\n",
      "train Loss: 136.3194 batch_loss: 7796.322754\n",
      "train Loss: 146.7731 batch_loss: 8028.424805\n",
      "train Loss: 157.1093 batch_loss: 7938.237305\n",
      "train Loss: 171.1848 batch_loss: 10809.921875\n",
      "train Loss: 182.9845 batch_loss: 9062.194336\n",
      "train Loss: 193.0223 batch_loss: 7709.076660\n",
      "train Loss: 202.4273 batch_loss: 7223.038086\n",
      "train Loss: 209.7072 batch_loss: 5590.966797\n",
      "train Loss: 223.6229 batch_loss: 10687.207031\n",
      "train Loss: 234.2375 batch_loss: 8152.027344\n",
      "train Loss: 244.0485 batch_loss: 7534.834473\n",
      "train Loss: 258.8138 batch_loss: 11339.800781\n",
      "train Loss: 269.1012 batch_loss: 7900.648438\n",
      "train Loss: 278.7733 batch_loss: 7428.177246\n",
      "train Loss: 287.3605 batch_loss: 6595.024414\n",
      "train Loss: 296.7048 batch_loss: 7176.365723\n",
      "train Loss: 308.3134 batch_loss: 8915.426758\n",
      "train Loss: 320.9964 batch_loss: 9740.591797\n",
      "train Loss: 331.4469 batch_loss: 8025.948242\n",
      "train Loss: 343.8508 batch_loss: 9526.221680\n",
      "train Loss: 353.1950 batch_loss: 7176.345215\n",
      "train Loss: 362.7354 batch_loss: 7327.024902\n",
      "train Loss: 373.9695 batch_loss: 8627.771484\n",
      "train Loss: 386.6562 batch_loss: 9743.357422\n",
      "train Loss: 398.1168 batch_loss: 8801.762695\n",
      "train Loss: 410.0303 batch_loss: 9149.588867\n",
      "train Loss: 419.4106 batch_loss: 7204.039062\n",
      "train Loss: 430.7644 batch_loss: 8719.705078\n",
      "train Loss: 443.6186 batch_loss: 9872.029297\n",
      "train Loss: 454.8807 batch_loss: 8649.306641\n",
      "train Loss: 465.3748 batch_loss: 8059.498535\n",
      "train Loss: 479.2924 batch_loss: 10688.687500\n",
      "train Loss: 489.8465 batch_loss: 8105.567383\n",
      "train Loss: 500.9971 batch_loss: 8563.619141\n",
      "train Loss: 508.6070 batch_loss: 5844.399414\n",
      "train Loss: 518.3642 batch_loss: 7493.567383\n",
      "train Loss: 527.0260 batch_loss: 6652.247559\n",
      "train Loss: 538.3293 batch_loss: 8680.913086\n",
      "train Loss: 546.7425 batch_loss: 6461.331055\n",
      "train Loss: 556.5425 batch_loss: 7526.406250\n",
      "train Loss: 565.5962 batch_loss: 6953.275879\n",
      "train Loss: 575.9443 batch_loss: 7947.364258\n",
      "train Loss: 585.3460 batch_loss: 7220.506836\n",
      "train Loss: 597.8147 batch_loss: 9575.923828\n",
      "train Loss: 609.7932 batch_loss: 9199.520508\n",
      "train Loss: 618.7513 batch_loss: 6879.820312\n",
      "train Loss: 629.3477 batch_loss: 8138.035156\n",
      "train Loss: 639.8709 batch_loss: 8081.764160\n",
      "train Loss: 654.9661 batch_loss: 11593.122070\n",
      "train Loss: 668.1390 batch_loss: 10116.841797\n",
      "train Loss: 675.4832 batch_loss: 5640.333984\n",
      "train Loss: 681.9052 batch_loss: 4932.047852\n",
      "train Loss: 692.0445 batch_loss: 7786.979004\n",
      "train Loss: 700.2536 batch_loss: 6304.600586\n",
      "train Loss: 710.2962 batch_loss: 7712.770508\n",
      "train Loss: 717.7822 batch_loss: 5749.198242\n",
      "train Loss: 725.8610 batch_loss: 6204.533691\n",
      "train Loss: 738.3845 batch_loss: 9618.071289\n",
      "train Loss: 750.3609 batch_loss: 9197.851562\n",
      "train Loss: 760.8911 batch_loss: 8087.162598\n",
      "train Loss: 770.2609 batch_loss: 7196.069336\n",
      "train Loss: 782.1960 batch_loss: 9166.107422\n",
      "train Loss: 788.9566 batch_loss: 5192.145508\n",
      "train Loss: 798.0033 batch_loss: 6947.883789\n",
      "train Loss: 808.9029 batch_loss: 8370.881836\n",
      "train Loss: 816.3409 batch_loss: 5712.400879\n",
      "train Loss: 824.3117 batch_loss: 6121.575195\n",
      "train Loss: 833.0275 batch_loss: 6693.733398\n",
      "train Loss: 840.6012 batch_loss: 5816.585938\n",
      "train Loss: 849.2441 batch_loss: 6637.747070\n",
      "train Loss: 855.4462 batch_loss: 4763.211914\n",
      "train Loss: 867.1216 batch_loss: 8966.681641\n",
      "train Loss: 877.8024 batch_loss: 8202.916016\n",
      "train Loss: 885.1531 batch_loss: 5645.298340\n",
      "train Loss: 892.7021 batch_loss: 5797.666504\n",
      "train Loss: 902.5302 batch_loss: 7547.937500\n",
      "train Loss: 910.6789 batch_loss: 6258.249512\n",
      "train Loss: 921.5522 batch_loss: 8350.679688\n",
      "train Loss: 927.3539 batch_loss: 4455.723633\n",
      "train Loss: 933.2109 batch_loss: 4498.132812\n",
      "train Loss: 941.3305 batch_loss: 6235.834473\n",
      "train Loss: 948.8442 batch_loss: 5770.574219\n",
      "train Loss: 956.9475 batch_loss: 6223.332520\n",
      "train Loss: 966.6443 batch_loss: 7447.104004\n",
      "train Loss: 974.5821 batch_loss: 6096.274902\n",
      "train Loss: 983.1011 batch_loss: 6542.602539\n",
      "train Loss: 991.1464 batch_loss: 6178.754883\n",
      "train Loss: 998.6043 batch_loss: 5727.637695\n",
      "train Loss: 1007.2571 batch_loss: 6645.411621\n",
      "train Loss: 1014.0377 batch_loss: 5207.461914\n",
      "train Loss: 1019.7523 batch_loss: 4388.807617\n",
      "train Loss: 1026.3049 batch_loss: 5032.369141\n",
      "train Loss: 1035.0763 batch_loss: 6736.458008\n",
      "train Loss: 1047.2997 batch_loss: 9387.616211\n",
      "train Loss: 1055.5089 batch_loss: 6304.622070\n",
      "train Loss: 1062.8320 batch_loss: 5624.186523\n",
      "train Loss: 1073.0644 batch_loss: 7858.458008\n",
      "train Loss: 1082.1833 batch_loss: 7003.317383\n",
      "train Loss: 1093.0353 batch_loss: 8334.296875\n",
      "train Loss: 1099.8422 batch_loss: 5227.743164\n",
      "train Loss: 1107.1032 batch_loss: 5576.458496\n",
      "train Loss: 1113.6541 batch_loss: 5031.088379\n",
      "train Loss: 1118.5812 batch_loss: 3783.964355\n",
      "train Loss: 1127.7572 batch_loss: 7047.228027\n",
      "train Loss: 1132.2175 batch_loss: 3425.468018\n",
      "train Loss: 1141.8553 batch_loss: 7401.815430\n",
      "train Loss: 1149.8666 batch_loss: 6152.727539\n",
      "train Loss: 1157.4202 batch_loss: 5801.132812\n",
      "train Loss: 1164.1811 batch_loss: 5192.373535\n",
      "train Loss: 1170.7549 batch_loss: 5048.682617\n",
      "train Loss: 1177.5897 batch_loss: 5249.138184\n",
      "train Loss: 1189.4739 batch_loss: 9127.052734\n",
      "train Loss: 1201.5914 batch_loss: 9306.224609\n",
      "train Loss: 1210.3334 batch_loss: 6713.875000\n",
      "train Loss: 1216.0979 batch_loss: 4427.140625\n",
      "train Loss: 1222.6792 batch_loss: 5054.457520\n",
      "train Loss: 1228.7489 batch_loss: 4661.531250\n",
      "train Loss: 1237.4473 batch_loss: 6680.366211\n",
      "train Loss: 1243.4992 batch_loss: 4647.844727\n",
      "train Loss: 1249.1297 batch_loss: 4324.224121\n",
      "train Loss: 1253.5877 batch_loss: 3423.748535\n",
      "train Loss: 1261.8759 batch_loss: 6365.287598\n",
      "train Loss: 1269.7816 batch_loss: 6071.592285\n",
      "train Loss: 1276.6315 batch_loss: 5260.718750\n",
      "train Loss: 1284.0048 batch_loss: 5662.737305\n",
      "train Loss: 1291.9668 batch_loss: 6114.780273\n",
      "train Loss: 1298.6103 batch_loss: 5102.248535\n",
      "train Loss: 1304.5468 batch_loss: 4559.217285\n",
      "train Loss: 1308.9284 batch_loss: 3365.045410\n",
      "train Loss: 1314.8098 batch_loss: 4516.920898\n",
      "train Loss: 1321.9210 batch_loss: 5461.446289\n",
      "train Loss: 1328.0028 batch_loss: 4670.778320\n",
      "train Loss: 1333.0124 batch_loss: 3847.356201\n",
      "train Loss: 1339.2379 batch_loss: 4781.233398\n",
      "train Loss: 1346.0637 batch_loss: 5242.177246\n",
      "train Loss: 1350.5003 batch_loss: 3407.314453\n",
      "train Loss: 1355.1788 batch_loss: 3593.094482\n",
      "train Loss: 1359.0164 batch_loss: 2947.296143\n",
      "train Loss: 1363.0765 batch_loss: 3118.142090\n",
      "train Loss: 1372.2746 batch_loss: 7064.131348\n",
      "train Loss: 1379.1588 batch_loss: 5287.053711\n",
      "train Loss: 1385.7883 batch_loss: 5091.478027\n",
      "train Loss: 1392.9110 batch_loss: 5470.204590\n",
      "train Loss: 1400.4645 batch_loss: 5801.145508\n",
      "train Loss: 1408.3008 batch_loss: 6018.238281\n",
      "train Loss: 1415.0743 batch_loss: 5202.064941\n",
      "train Loss: 1422.3297 batch_loss: 5572.167969\n",
      "train Loss: 1428.5034 batch_loss: 4741.382812\n",
      "train Loss: 1436.2172 batch_loss: 5924.214355\n",
      "train Loss: 1441.4098 batch_loss: 3987.884033\n",
      "train Loss: 1446.3973 batch_loss: 3830.393066\n",
      "train Loss: 1451.9473 batch_loss: 4262.438965\n",
      "train Loss: 1458.5119 batch_loss: 5041.585938\n",
      "train Loss: 1464.7056 batch_loss: 4756.733398\n",
      "train Loss: 1470.5642 batch_loss: 4499.439941\n",
      "train Loss: 1476.5709 batch_loss: 4613.170410\n",
      "train Loss: 1485.1133 batch_loss: 6560.520508\n",
      "train Loss: 1490.1174 batch_loss: 3843.167236\n",
      "train Loss: 1496.9076 batch_loss: 5214.879883\n",
      "train Loss: 1503.3918 batch_loss: 4979.817871\n",
      "train Loss: 1508.0614 batch_loss: 3586.290527\n",
      "train Loss: 1512.4497 batch_loss: 3370.217285\n",
      "train Loss: 1518.4248 batch_loss: 4588.889648\n",
      "train Loss: 1523.9374 batch_loss: 4233.637695\n",
      "train Loss: 1530.7056 batch_loss: 5198.023438\n",
      "train Loss: 1535.8944 batch_loss: 3985.007812\n",
      "train Loss: 1541.5255 batch_loss: 4324.617188\n",
      "train Loss: 1545.5542 batch_loss: 3094.106445\n",
      "train Loss: 1552.3346 batch_loss: 5207.321777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1557.4322 batch_loss: 3914.953857\n",
      "train Loss: 1562.4869 batch_loss: 3882.002441\n",
      "train Loss: 1567.7064 batch_loss: 4008.604004\n",
      "train Loss: 1571.6798 batch_loss: 3051.551270\n",
      "train Loss: 1578.8838 batch_loss: 5532.671875\n",
      "train Loss: 1584.6499 batch_loss: 4428.396973\n",
      "train Loss: 1590.3943 batch_loss: 4411.672363\n",
      "train Loss: 1597.4452 batch_loss: 5415.057129\n",
      "train Loss: 1600.9028 batch_loss: 2655.446533\n",
      "train Loss: 1605.9899 batch_loss: 3906.911621\n",
      "train Loss: 1611.5016 batch_loss: 4232.958984\n",
      "train Loss: 1616.5785 batch_loss: 3899.086426\n",
      "train Loss: 1621.4469 batch_loss: 3738.912842\n",
      "Loss on the test images: 1047.46658 \n",
      "Epoch 1/9\n",
      "----------\n",
      "trainloader ready!\n",
      "testloader ready!\n",
      "train Loss: 5.6746 batch_loss: 4358.100586\n",
      "train Loss: 10.8672 batch_loss: 3987.910889\n",
      "train Loss: 13.8290 batch_loss: 2274.644531\n",
      "train Loss: 19.0196 batch_loss: 3986.364014\n",
      "train Loss: 25.7190 batch_loss: 5145.146484\n",
      "train Loss: 31.3468 batch_loss: 4322.210938\n",
      "train Loss: 35.5620 batch_loss: 3237.244141\n",
      "train Loss: 39.6757 batch_loss: 3159.348389\n",
      "train Loss: 43.1944 batch_loss: 2702.303711\n",
      "train Loss: 49.7645 batch_loss: 5045.824707\n",
      "train Loss: 55.9848 batch_loss: 4777.260742\n",
      "train Loss: 60.4838 batch_loss: 3455.205566\n",
      "train Loss: 66.9006 batch_loss: 4928.081055\n",
      "train Loss: 76.2610 batch_loss: 7188.807617\n",
      "train Loss: 81.8156 batch_loss: 4265.898926\n",
      "train Loss: 86.3188 batch_loss: 3458.487549\n",
      "train Loss: 90.3563 batch_loss: 3100.833496\n",
      "train Loss: 96.3657 batch_loss: 4615.166016\n",
      "train Loss: 101.7519 batch_loss: 4136.618164\n",
      "train Loss: 109.4436 batch_loss: 5907.197266\n",
      "train Loss: 113.1374 batch_loss: 2836.846436\n",
      "train Loss: 116.7060 batch_loss: 2740.680908\n",
      "train Loss: 123.4202 batch_loss: 5156.524902\n",
      "train Loss: 131.2677 batch_loss: 6026.861328\n",
      "train Loss: 136.5041 batch_loss: 4021.587891\n",
      "train Loss: 142.1100 batch_loss: 4305.343750\n",
      "train Loss: 145.7751 batch_loss: 2814.739258\n",
      "train Loss: 150.4772 batch_loss: 3611.262207\n",
      "train Loss: 156.2085 batch_loss: 4401.608887\n",
      "train Loss: 161.8371 batch_loss: 4322.812012\n",
      "train Loss: 168.8215 batch_loss: 5364.002930\n",
      "train Loss: 172.9998 batch_loss: 3208.914062\n",
      "train Loss: 179.4024 batch_loss: 4917.209961\n",
      "train Loss: 184.2111 batch_loss: 3693.101562\n",
      "train Loss: 191.1533 batch_loss: 5331.554688\n",
      "train Loss: 197.1549 batch_loss: 4609.237793\n",
      "train Loss: 201.7784 batch_loss: 3550.875732\n",
      "train Loss: 206.8331 batch_loss: 3882.020752\n",
      "train Loss: 210.8464 batch_loss: 3082.207031\n",
      "train Loss: 217.3572 batch_loss: 5000.310547\n",
      "train Loss: 221.5315 batch_loss: 3205.833496\n",
      "train Loss: 226.8625 batch_loss: 4094.210938\n",
      "train Loss: 235.3161 batch_loss: 6492.372559\n",
      "train Loss: 241.0150 batch_loss: 4376.763184\n",
      "train Loss: 247.7998 batch_loss: 5210.690918\n",
      "train Loss: 252.1770 batch_loss: 3361.732910\n",
      "train Loss: 257.2184 batch_loss: 3871.758301\n",
      "train Loss: 261.0311 batch_loss: 2928.144531\n",
      "train Loss: 266.8418 batch_loss: 4462.625977\n",
      "train Loss: 271.3950 batch_loss: 3496.870361\n",
      "train Loss: 275.1766 batch_loss: 2904.245117\n",
      "train Loss: 279.1527 batch_loss: 3053.649170\n",
      "train Loss: 285.2271 batch_loss: 4665.165039\n",
      "train Loss: 292.6069 batch_loss: 5667.701660\n",
      "train Loss: 295.5072 batch_loss: 2227.376953\n",
      "train Loss: 300.2244 batch_loss: 3622.808105\n",
      "train Loss: 304.5627 batch_loss: 3331.810547\n",
      "train Loss: 308.7662 batch_loss: 3228.317871\n",
      "train Loss: 316.5169 batch_loss: 5952.551758\n",
      "train Loss: 322.2672 batch_loss: 4416.218262\n",
      "train Loss: 327.5636 batch_loss: 4067.602295\n",
      "train Loss: 332.0154 batch_loss: 3419.030029\n",
      "train Loss: 336.1642 batch_loss: 3186.237793\n",
      "train Loss: 340.7355 batch_loss: 3510.758301\n",
      "train Loss: 347.0978 batch_loss: 4886.278809\n",
      "train Loss: 352.5407 batch_loss: 4180.178711\n",
      "train Loss: 356.3676 batch_loss: 2939.002441\n",
      "train Loss: 362.5675 batch_loss: 4761.567383\n",
      "train Loss: 366.3093 batch_loss: 2873.689697\n",
      "train Loss: 370.0462 batch_loss: 2869.916504\n",
      "train Loss: 375.6470 batch_loss: 4301.452148\n",
      "train Loss: 379.8722 batch_loss: 3244.912842\n",
      "train Loss: 385.1382 batch_loss: 4044.270996\n",
      "train Loss: 389.1880 batch_loss: 3110.298096\n",
      "train Loss: 393.6443 batch_loss: 3422.455078\n",
      "train Loss: 399.3681 batch_loss: 4395.867676\n",
      "train Loss: 403.9885 batch_loss: 3548.439453\n",
      "train Loss: 408.2815 batch_loss: 3297.026855\n",
      "train Loss: 412.6076 batch_loss: 3322.444580\n",
      "train Loss: 417.5143 batch_loss: 3768.369141\n",
      "train Loss: 422.7358 batch_loss: 4010.073486\n",
      "train Loss: 428.1195 batch_loss: 4134.727539\n",
      "train Loss: 434.8386 batch_loss: 5160.214844\n",
      "train Loss: 440.0496 batch_loss: 4002.059082\n",
      "train Loss: 446.1998 batch_loss: 4723.390137\n",
      "train Loss: 451.2664 batch_loss: 3891.118408\n",
      "train Loss: 456.8884 batch_loss: 4317.667969\n",
      "train Loss: 460.7131 batch_loss: 2937.391113\n",
      "train Loss: 468.1896 batch_loss: 5741.950195\n",
      "train Loss: 471.2737 batch_loss: 2368.573486\n",
      "train Loss: 475.5037 batch_loss: 3248.709229\n",
      "train Loss: 477.9973 batch_loss: 1915.067139\n",
      "train Loss: 484.4311 batch_loss: 4941.135254\n",
      "train Loss: 488.7930 batch_loss: 3349.953613\n",
      "train Loss: 491.4125 batch_loss: 2011.758545\n",
      "train Loss: 496.9728 batch_loss: 4270.307129\n",
      "train Loss: 502.9090 batch_loss: 4558.976562\n",
      "train Loss: 508.6455 batch_loss: 4405.672852\n",
      "train Loss: 515.4465 batch_loss: 5223.129883\n",
      "train Loss: 522.1743 batch_loss: 5166.988281\n",
      "train Loss: 524.7538 batch_loss: 1981.070679\n",
      "train Loss: 529.4243 batch_loss: 3586.892090\n",
      "train Loss: 533.9602 batch_loss: 3483.608887\n",
      "train Loss: 540.7595 batch_loss: 5221.822754\n",
      "train Loss: 545.1810 batch_loss: 3395.755859\n",
      "train Loss: 548.5922 batch_loss: 2619.810059\n",
      "train Loss: 554.7366 batch_loss: 4718.893066\n",
      "train Loss: 559.3793 batch_loss: 3565.604248\n",
      "train Loss: 563.0841 batch_loss: 2845.290527\n",
      "train Loss: 568.4666 batch_loss: 4133.725586\n",
      "train Loss: 575.9518 batch_loss: 5748.651367\n",
      "train Loss: 581.8310 batch_loss: 4515.202637\n",
      "train Loss: 588.0378 batch_loss: 4766.802734\n",
      "train Loss: 593.1688 batch_loss: 3940.667969\n",
      "train Loss: 599.6213 batch_loss: 4955.527832\n",
      "train Loss: 605.0194 batch_loss: 4145.675781\n",
      "train Loss: 610.2611 batch_loss: 4025.679688\n",
      "train Loss: 615.8637 batch_loss: 4302.787109\n",
      "train Loss: 621.1871 batch_loss: 4088.335938\n",
      "train Loss: 625.6698 batch_loss: 3442.713623\n",
      "train Loss: 630.2092 batch_loss: 3486.240723\n",
      "train Loss: 635.1197 batch_loss: 3771.289062\n",
      "train Loss: 640.6966 batch_loss: 4283.070801\n",
      "train Loss: 645.9224 batch_loss: 4013.391357\n",
      "train Loss: 650.6546 batch_loss: 3634.356201\n",
      "train Loss: 655.9375 batch_loss: 4057.273438\n",
      "train Loss: 660.7217 batch_loss: 3674.281250\n",
      "train Loss: 664.4734 batch_loss: 2881.274902\n",
      "train Loss: 667.5454 batch_loss: 2359.295410\n",
      "train Loss: 673.3454 batch_loss: 4454.432617\n",
      "train Loss: 677.7554 batch_loss: 3386.871094\n",
      "train Loss: 682.2810 batch_loss: 3475.645264\n",
      "train Loss: 686.2263 batch_loss: 3030.009766\n",
      "train Loss: 690.6271 batch_loss: 3379.771973\n",
      "train Loss: 696.8703 batch_loss: 4794.833008\n",
      "train Loss: 702.9223 batch_loss: 4647.932129\n",
      "train Loss: 710.2304 batch_loss: 5612.571289\n",
      "train Loss: 713.4921 batch_loss: 2504.979980\n",
      "train Loss: 716.7484 batch_loss: 2500.883545\n",
      "train Loss: 720.9837 batch_loss: 3252.705811\n",
      "train Loss: 728.2854 batch_loss: 5607.701660\n",
      "train Loss: 733.3211 batch_loss: 3867.407471\n",
      "train Loss: 737.8249 batch_loss: 3458.895020\n",
      "train Loss: 743.2693 batch_loss: 4181.337402\n",
      "train Loss: 747.0134 batch_loss: 2875.470459\n",
      "train Loss: 751.2147 batch_loss: 3226.593262\n",
      "train Loss: 759.4022 batch_loss: 6288.016602\n",
      "train Loss: 765.2028 batch_loss: 4454.851562\n",
      "train Loss: 772.0837 batch_loss: 5284.501465\n",
      "train Loss: 775.9592 batch_loss: 2976.395264\n",
      "train Loss: 778.2489 batch_loss: 1758.459229\n",
      "train Loss: 785.3077 batch_loss: 5421.159180\n",
      "train Loss: 790.3070 batch_loss: 3839.460938\n",
      "train Loss: 794.5484 batch_loss: 3257.409424\n",
      "train Loss: 800.2328 batch_loss: 4365.625977\n",
      "train Loss: 806.0844 batch_loss: 4494.000977\n",
      "train Loss: 810.8421 batch_loss: 3653.966309\n",
      "train Loss: 816.2015 batch_loss: 4116.024902\n",
      "train Loss: 822.0070 batch_loss: 4458.571777\n",
      "train Loss: 825.7523 batch_loss: 2876.437256\n",
      "train Loss: 830.8545 batch_loss: 3918.459717\n",
      "train Loss: 833.7859 batch_loss: 2251.350830\n",
      "train Loss: 837.9373 batch_loss: 3188.224121\n",
      "train Loss: 841.6566 batch_loss: 2856.455811\n",
      "train Loss: 848.4648 batch_loss: 5228.673828\n",
      "train Loss: 853.8302 batch_loss: 4120.625977\n",
      "train Loss: 858.2544 batch_loss: 3397.804199\n",
      "train Loss: 861.7238 batch_loss: 2664.484131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 866.7783 batch_loss: 3881.882324\n",
      "train Loss: 873.1881 batch_loss: 4922.686523\n",
      "train Loss: 880.0480 batch_loss: 5268.427734\n",
      "train Loss: 885.0228 batch_loss: 3820.655518\n",
      "train Loss: 889.3510 batch_loss: 3324.055420\n",
      "train Loss: 894.5655 batch_loss: 4004.703613\n",
      "train Loss: 900.7096 batch_loss: 4718.737793\n",
      "train Loss: 906.3181 batch_loss: 4307.293457\n",
      "train Loss: 910.5583 batch_loss: 3256.445068\n",
      "train Loss: 915.1309 batch_loss: 3511.782715\n",
      "train Loss: 919.8786 batch_loss: 3646.262207\n",
      "train Loss: 925.6721 batch_loss: 4449.414062\n",
      "train Loss: 929.7783 batch_loss: 3153.495361\n",
      "train Loss: 932.9692 batch_loss: 2450.631348\n",
      "train Loss: 936.0737 batch_loss: 2384.270264\n",
      "train Loss: 940.5995 batch_loss: 3475.802246\n",
      "train Loss: 945.3723 batch_loss: 3665.537598\n",
      "train Loss: 948.5976 batch_loss: 2476.994629\n",
      "train Loss: 952.3643 batch_loss: 2892.873779\n",
      "train Loss: 954.6811 batch_loss: 1779.275024\n",
      "train Loss: 960.5604 batch_loss: 4515.336426\n",
      "train Loss: 963.5410 batch_loss: 2289.068359\n",
      "train Loss: 967.8201 batch_loss: 3286.339111\n",
      "train Loss: 971.9383 batch_loss: 3162.776855\n",
      "Loss on the test images: 955.89593 \n",
      "Epoch 2/9\n",
      "----------\n",
      "trainloader ready!\n",
      "testloader ready!\n",
      "train Loss: 5.8944 batch_loss: 4526.872070\n",
      "train Loss: 10.8009 batch_loss: 3768.220215\n",
      "train Loss: 16.1417 batch_loss: 4101.706055\n",
      "train Loss: 19.7682 batch_loss: 2785.153076\n",
      "train Loss: 25.7026 batch_loss: 4557.627441\n",
      "train Loss: 29.8336 batch_loss: 3172.630615\n",
      "train Loss: 33.2503 batch_loss: 2624.052002\n",
      "train Loss: 40.5020 batch_loss: 5569.292480\n",
      "train Loss: 43.9144 batch_loss: 2620.693359\n",
      "train Loss: 46.4865 batch_loss: 1975.346313\n",
      "train Loss: 49.7729 batch_loss: 2524.005371\n",
      "train Loss: 52.8427 batch_loss: 2357.578369\n",
      "train Loss: 57.0643 batch_loss: 3242.229980\n",
      "train Loss: 60.2719 batch_loss: 2463.409668\n",
      "train Loss: 66.3447 batch_loss: 4663.893555\n",
      "train Loss: 69.9190 batch_loss: 2745.092041\n",
      "train Loss: 73.6251 batch_loss: 2846.245605\n",
      "train Loss: 79.5323 batch_loss: 4536.735352\n",
      "train Loss: 82.4597 batch_loss: 2248.245117\n",
      "train Loss: 87.8741 batch_loss: 4158.270020\n",
      "train Loss: 92.2818 batch_loss: 3385.134033\n",
      "train Loss: 96.3543 batch_loss: 3127.703613\n",
      "train Loss: 100.8751 batch_loss: 3471.923828\n",
      "train Loss: 106.9759 batch_loss: 4685.424805\n",
      "train Loss: 112.4984 batch_loss: 4241.283691\n",
      "train Loss: 115.5606 batch_loss: 2351.799316\n",
      "train Loss: 122.3093 batch_loss: 5182.936523\n",
      "train Loss: 127.9601 batch_loss: 4339.879883\n",
      "train Loss: 133.6498 batch_loss: 4369.625000\n",
      "train Loss: 139.0667 batch_loss: 4160.223145\n",
      "train Loss: 144.0968 batch_loss: 3863.079590\n",
      "train Loss: 149.6841 batch_loss: 4291.108398\n",
      "train Loss: 156.2336 batch_loss: 5029.965820\n",
      "train Loss: 163.2804 batch_loss: 5411.968750\n",
      "train Loss: 167.5770 batch_loss: 3299.757812\n",
      "train Loss: 171.7121 batch_loss: 3175.795654\n",
      "train Loss: 178.0154 batch_loss: 4840.916016\n",
      "train Loss: 181.9780 batch_loss: 3043.299072\n",
      "train Loss: 187.1354 batch_loss: 3960.901611\n",
      "train Loss: 192.5080 batch_loss: 4126.116699\n",
      "train Loss: 198.3137 batch_loss: 4458.800293\n",
      "train Loss: 200.8794 batch_loss: 1970.440063\n",
      "train Loss: 205.2163 batch_loss: 3330.723389\n",
      "train Loss: 210.9279 batch_loss: 4386.550781\n",
      "train Loss: 216.0337 batch_loss: 3921.219482\n",
      "train Loss: 220.4634 batch_loss: 3401.987549\n",
      "train Loss: 227.2377 batch_loss: 5202.690430\n",
      "train Loss: 233.7680 batch_loss: 5015.299316\n",
      "train Loss: 237.1537 batch_loss: 2600.204102\n",
      "train Loss: 242.0031 batch_loss: 3724.351562\n",
      "train Loss: 246.3163 batch_loss: 3312.494873\n",
      "train Loss: 252.2149 batch_loss: 4530.175781\n",
      "train Loss: 257.0118 batch_loss: 3684.007812\n",
      "train Loss: 262.0897 batch_loss: 3899.825439\n",
      "train Loss: 265.3214 batch_loss: 2481.941895\n",
      "train Loss: 269.3716 batch_loss: 3110.559326\n",
      "train Loss: 273.6401 batch_loss: 3278.177734\n",
      "train Loss: 279.4644 batch_loss: 4473.092285\n",
      "train Loss: 285.8952 batch_loss: 4938.859375\n",
      "train Loss: 290.0834 batch_loss: 3216.495361\n",
      "train Loss: 293.1894 batch_loss: 2385.431152\n",
      "train Loss: 297.4596 batch_loss: 3279.508545\n",
      "train Loss: 303.3438 batch_loss: 4519.034180\n",
      "train Loss: 307.8725 batch_loss: 3478.037842\n",
      "train Loss: 311.2703 batch_loss: 2609.569336\n",
      "train Loss: 316.0596 batch_loss: 3678.180420\n",
      "train Loss: 319.3408 batch_loss: 2519.920166\n",
      "train Loss: 325.7001 batch_loss: 4883.981934\n",
      "train Loss: 329.1811 batch_loss: 2673.357910\n",
      "train Loss: 336.4001 batch_loss: 5544.225098\n",
      "train Loss: 341.6975 batch_loss: 4068.364502\n",
      "train Loss: 345.0148 batch_loss: 2547.690186\n",
      "train Loss: 350.2679 batch_loss: 4034.375488\n",
      "train Loss: 353.4204 batch_loss: 2421.172363\n",
      "train Loss: 358.4856 batch_loss: 3890.032715\n",
      "train Loss: 363.1393 batch_loss: 3574.043457\n",
      "train Loss: 367.2575 batch_loss: 3162.777344\n",
      "train Loss: 371.7122 batch_loss: 3421.219971\n",
      "train Loss: 376.6723 batch_loss: 3809.335938\n",
      "train Loss: 381.2569 batch_loss: 3520.967285\n",
      "train Loss: 383.8979 batch_loss: 2028.356445\n",
      "train Loss: 389.0412 batch_loss: 3950.045654\n",
      "train Loss: 396.0203 batch_loss: 5359.959961\n",
      "train Loss: 400.3655 batch_loss: 3337.042236\n",
      "train Loss: 405.2099 batch_loss: 3720.516357\n",
      "train Loss: 409.7704 batch_loss: 3502.500488\n",
      "train Loss: 413.6759 batch_loss: 2999.389160\n",
      "train Loss: 421.2573 batch_loss: 5822.559570\n",
      "train Loss: 425.0919 batch_loss: 2944.977051\n",
      "train Loss: 432.0869 batch_loss: 5372.127930\n",
      "train Loss: 435.9617 batch_loss: 2975.849609\n",
      "train Loss: 438.7455 batch_loss: 2137.985840\n",
      "train Loss: 441.7315 batch_loss: 2293.203613\n",
      "train Loss: 446.3318 batch_loss: 3533.061768\n",
      "train Loss: 451.0231 batch_loss: 3602.921875\n",
      "train Loss: 454.7926 batch_loss: 2894.964600\n",
      "train Loss: 458.6745 batch_loss: 2981.295166\n",
      "train Loss: 464.7925 batch_loss: 4698.612305\n",
      "train Loss: 468.8986 batch_loss: 3153.516846\n",
      "train Loss: 473.2270 batch_loss: 3324.159668\n",
      "train Loss: 478.2876 batch_loss: 3886.567139\n",
      "train Loss: 481.0762 batch_loss: 2141.609863\n",
      "train Loss: 486.3855 batch_loss: 4077.550049\n",
      "train Loss: 491.1258 batch_loss: 3640.608643\n",
      "train Loss: 496.5119 batch_loss: 4136.528320\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f4f1f02857e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ff316c7ac0a4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs, batch_size, step_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0miter_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# transfer learning resnet18\n",
    "step_size = 20\n",
    "model = ConvLSTM(input_size=(128,128),\n",
    "                 input_dim=1,\n",
    "                 hidden_dim=[64, 64, 128],\n",
    "                 kernel_size=(3, 3),\n",
    "                 num_layers=3,\n",
    "                 predict_steps=int(step_size/2),\n",
    "                 batch_first=True,\n",
    "                 bias=True,\n",
    "                 return_all_layers=True).cuda()\n",
    "\n",
    "if use_gpu:\n",
    "#     encoder = torch.nn.DataParallel(encoder)\n",
    "#     decoder = torch.nn.DataParallel(decoder)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=1, gamma=0.1)\n",
    "\n",
    "# train model\n",
    "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, batch_size = 4, step_size = 20, num_epochs=10)\n",
    "torch.save(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset = ansimDataset(img_list_csv = img_list_csv, seq_csv = test_csv, root_dir = img_path, step=step_size, random_rotate = True, mask = mask, transform=None)\n",
    "# testloader = torch.utils.data.DataLoader(testset,\n",
    "#                                              batch_size=1, shuffle=True,\n",
    "#                                              num_workers=1)\n",
    "# dataiter = iter(testloader)\n",
    "# data = dataiter.next()\n",
    "# data_split = torch.split(data, int(data.shape[1]/2), dim=1)\n",
    "# inputs = data_split[0]\n",
    "# target = data_split[1]\n",
    "# outputs = model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
